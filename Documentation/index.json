{
  "_site/documentation/ALM/ALM.html": {
    "href": "_site/documentation/ALM/ALM.html",
    "title": "Power Platform - Release Pipeline | GOC Theme Documentation",
    "keywords": "Power Platform - Release Pipeline Download PDF GOC – DYNAMICS 365/PORTALS ALM SUMMARY The purpose of this document is to describe and illustrate the application lifecycle management (ALM) strategy at GAC. The Dynamics 365 implementation at GAC is quickly growing and new features are being added every quarter. In addition to new features, more and more line of businesses are onboarding as well. To accommodate frequent changes and optimizations to the platform to meet evolving business requirements, GAC is embarking on an exercise to optimize their ALM process to achieve a greater level of configuration management integrity. ALM CHECKLIST CONVENTIONS Solutions are unpacked and version controlled in GIT hosted repo in Azure DevOps Solutions are repacked through an automated process and stored as artefacts for releasees Packages are built through an automated process that combines the required solutions and configuration data into a PackageDeployer.zip file Solutions are exported and imported as managed There is at least one discrete development instance per solution being developed (by line of business). A base solution includes common components is imported into LOB environments Where multiple solutions are developed for deployment to a single production environment, the same solution publisher and prefix are used Pipeline can support both standard solutions and solution patches Solution.zip files and their contents are never manually edited Where a solution being developed has dependencies on one or more solutions, the dependencies are satisfied through always importing those solutions as managed solutions into the discrete development instance for the solution being developed (e.g. Base solution) Only managed solutions are deployed to environments downstream of development All solutions are deployed via the release pipeline No unmanaged changes are made directly to environments downstream of development Each test case has traceability back to requirement Test cases are automated The pipeline supports configuration data transfers Exported configuration data is saved as artefacts The pipeline's runtime variables map to their own stage and run in the sequence they are provided by the user If any stage fails, the pipeline exits (so will not execute subsequent stages to avoid faulty deployments) Each stage generates a log artefact (e.g. Solution, Portal checker, diagnostics) Release managers can create a release from a build which will leverage the artefacts from the build (instead of exporting these from an environment) Any issues that are deemed critical by the solution checker and the portal checker will exit the pipeline A default variable group is available to the build team. Each developer can clone the variable group for specific connection parameters. Alternatively, the organization can opt to leverage a single variable group to govern the connection parameters to each environment. Developers will always have access to a runtime variable and successful builds will store the resulting artefacts from these variables for future release. The primary development stream solution set is static. Developers can create as many solutions as they need however, their components must be migrated to the primary solutions that make up the system. The same applies for the schema files. The pipeline allows a developer to deploy their solutions to the primary dev environment where the pipeline will merge its components to the primary solution (Optional) In a scenario where GAC opts for developer environments or developer solutions, the pipeline will support solution component mover API calls to merge developer solution components to the BASE, CanExport, and Sonar 360 solutions (and others for future LOB implementations) IMPLEMENTATION Overview The PowerPlatform CI/CD implementation is comprised of a scripts that provides the PowerPlatform build teams with the ability to check-in their Dataverse solutions, configuration data schema files, and portal code to the organizations primary repository(ies) hosting the source code and deployment artefacts that make up the Department's grant funding system. This pipeline extension is built to be generic to any PowerPlatform implementation that is comprised of both Model-Driven-Apps and PowerApps Portals. Canvas Apps are not yet fully supported in this implementation, however support for these types of applications will be released in version 1.3. That being said, since Canvas apps are typically stored in a solution, this pipeline automation extension can include Canvas Apps but will not include certain features like automated tests designed for these types of applications. Version 1.3 is scheduled to be released on June 28, 2022 along with an updated version of this document. Continuous Integration For the organization to achieve stability, predictability and quality of their Dataverse application deployments, the build team needs to be equipped with tools that will automate the export for solutions, configuration data, and portal data and apply version control. In addition, members of the build team need the capability to deploy new features and or bug fixes frequently to minimize the complexities associated with less frequent and very large deployments. Having the ability to continuously release changes to the Dataverse applications will allow the developer to address issues such as missing dependencies, critical bugs, and potential misconfigurations or mis-interpreted requirements which can cause issues downstream. By catching these issues early, the team can resolve these issues and re-deploy. The deployment processes in the PowerPlatform can take a significant amount of time if executed manually which can cause overall productivity issues. Therefore, by implementing a robust continuous integration framework, developers can instead rely on the DevOps Pipelines feature to automate deployments which will not only provide overall productivity gains, but will also standardize the deployment processes, provide a facility for automated testing to ensure best practices are followed, validate that the solution, data and portal artefacts can be successfully deployed based on their latest changes etc. In this section, the extension for CI is described. Option 1: Developer invoked integration Developers have the option of manually invoking a pipeline for which the developer will be supply a series of variable values in a specific sequence. Once the variables are defined, and the pipeline invoked, the extension will interpret the supplied variables and will deploy the developer's solution(s), configuration data (from the schema), and (or) the portal configuration changes automatically. As part of this process, the extension will run the solution and portal checker API's provided by the Dataverse build tools and will export the Dataverse resources informed by the variable(s) provided by the developer. These artefacts are stored in the DevOps project storage for future usage such as release to a downstream environment such as UAT and PROD. Option 2: Integration Automation The organization can configure the pipeline to trigger automatically via Pull-Requests. In this scenario, pull-requests are invoked to a branch such as TEST which will automatically deploy the solutions, data files, and (or) portals using pre-defined variable values in both the variable group and run time variables. In this scenario developers will be responsible to move their solution components to the primary solutions set in the variable artefacts. The release manager will then issue a PR to the relevant branch will execute the pipeline, save the artefacts in preparation for a release to a downstream environment. The benefits of this approach include a reduction of effort associated with developers having to manually invoke pipelines. The downside will be less traceability of who made the changes and when as with this approach, changes are published by a single (or multiple) release managers for the sole purpose of deployment and generation of artefacts to deploy to downstream environments. Summary GAC can opt to use a combination of the two options instead of picking one. In this scenario, developers will be responsible to commit their work, indirectly through the pipeline script in DevOps, and the cadence for option 2 would be that the release manager would run the same pipeline at specific time intervals during a release cycle to prepare the artefacts for a full release to UAT and Production. If GAC opts to leverage only a single option, Option 1 would be more beneficial as it would align better with modern ALM standards whereby developers can test their own releases and receive feedback on any potential defects associated with their development artefacts through the automated test outputs provided by the pipeline so that when its time a full release, issues like missing dependencies, bugs and mis-configurations are minimized or eliminated resulting in a more reliable deployment process. Continuous Deployments This pipeline extension includes a release pipeline targeted to UAT and PRODUCTION environments. Releases are invoked by release managers who will have the ability to select any successful and issue a release to UAT and PROD using that build's artefacts. The script is identical to the build integration script and will use the same variable group. The only delta is that the release pipeline script will loop through its source build integration artifacts and set the runtime variables automatically so that the release manager does not need to re-set these variables at run time. Option 1: Release manager intervention The release pipeline can be invoked manually by a release manager from any build or from the release's menu in DevOps. Option 2: Deployment Automation Like the automated build integration, a release manager can choose to have the release pipeline execute as part of a PR and thus tied to a branch. The processes demonstrated below are the same, with t DESIGN Security Only build administrators can modify the pipeline and execute the full release pipeline. Contributors (developers) can execute the build integration pipeline. This baseline implementation also restricts any modifications to the variable group that is linked to the pipelines. Only build administrators can modify these groups. However if GAC employs a developer environment strategy, it is recommended that the variable groups are cloned and that each developer will have access to the build integration pipeline within their own branch and these would be tied to a variable group whose source and target would reflect their developer environment and point to the consolidated dev environment. This current version of the scripts supports OOB variable groups and variable secrets but assumes that the DevOps projects is not linked to a KeyVaults. If GAC links the project to a KeyVaults, the variable group values will be updated to include the KeyVaults name and secret IDs instead of using \"masked\" secrets. Service Connections The table below includes the service connections required for this extension. In addition to service connections, since this extension is leveraging the community version of the build tools, connecting strings must also be configured. However, the connection strings are automatically generated by other variables (e.g. the SPN -- App Registrations/App User's TenantID, URL, ClientID and Secret all of which are concatenated within the source and target connection strings for each environment. These variables, including the service connections (Target and SourceSPN-{ENV} variables are available in the Variable Group (Connection-Parameters). A service connection should be created for each environment that will service as a source and target environment for the integration and deployment pipelines. All connection parameters should be pre-defined in the connection parameters variable group. Variables & Variable Groups Variable Name Description ClientID App Registration’s ClientID (ApplicationID – SPN for Dataverse Environments’ App User (assumes 1 SPN for multiple environments) DeploymentProfile-PROD Portals -> PROD Global settings (located in Deployment-Profiles folder in repository – prod.deployment.yml DeploymentProfile-TEST Portals -> TEST Global settings (located in Deployment-Profiles folder in repository – test.deployment.yml DeploymentProfile-UAT Portals -> UAT Global settings (located in Deployment-Profiles folder in repository – uat.deployment.yml Secret App Registration’s Client Secret (SPN for Dataverse Environments’ App User (assumes 1 SPN for multiple environments) SourceConnection-DEV Value is populated by ClientID, Secret, Source and Target URLs, and TenantID – this is for data transfers SourceConnection-UAT Value is populated by ClientID, Secret, Source and Target URLs, and TenantID – this is for data transfers SourceSPN-DEV Service Connection name for DEV (Source environment) SourceSPN-UAT Service Connection name for UAT (Source environment) SourceURL-DEV Source URL for DEV (feeds SourceConnection variable) SourceUAT-DEV Source URL for UAT (feeds SourceConnection variable) TargetConnection-DEV Value is populated by ClientID, Secret, Source and Target URLs, and TenantID – this is for data transfers (source environment connection string) TargetConnection-UAT Value is populated by ClientID, Secret, Source and Target URLs, and TenantID – this is for data transfers (target environment connection string) TargetSPN-DEV Service Connection name for DEV (Target environment) TargetSPN-UAT Service Connection name for UAT (Target environment) TargetURL-PROD Target URL for PROD (feeds TargetConnection variable) TargetURL-UAT Target URL for UAT (feeds TargetConnection variable) TargetURL-DEV Target URL for DEV (feeds TargetConnection variable) TenantID Azure tenant ID hosting App registration record Runtime Variables Variable Name Description Artefact1 Provide the name of a solution (doesn’t need to be in the repository, the pipeline will perform the export and commit etc.) or configuration migration xml file (must be in the data directory in the repo) Artefact1-Target Solution If Artefact1 is a solution, and you need to copy its component to a primary solution, provide the name of the primary solution here Artefact2 Provide the name of a solution (doesn’t need to be in the repository, the pipeline will perform the export and commit etc.) or configuration migration xml file (must be in the data directory in the repo) Artefact2-Target-Solution If Artefact2 is a solution, and you need to copy its component to a primary solution, provide the name of the primary solution here Artefact3 Provide the name of a solution (doesn’t need to be in the repository, the pipeline will perform the export and commit etc.) or configuration migration xml file (must be in the data directory in the repo) Artefact3-Target-Solution If Artefact3 is a solution, and you need to copy its component to a primary solution, provide the name of the primary solution here Artefact4 Provide the name of a solution (doesn’t need to be in the repository, the pipeline will perform the export and commit etc.) or configuration migration xml file (must be in the data directory in the repo) Artefact4-Target-Solution If Artefact4 is a solution, and you need to copy its component to a primary solution, provide the name of the primary solution here Deploy Portal? If set to yes, the pipeline will export the portal via CLI and commit to source. It will also deploy the portal to the target environment (using SourceURL and TargetURL) Comments Used to issue the git comments when the pipelines automatically commit your artefacts Project Name Optional, provides project context Variable-Group Name of the variable group that is linked to the pipeline to leverage. The group must have the same variable names (and types) as the “Connection-Parameters” variable group. This is useful to isolated dev environments with specific Dataverse environments that fall outside the main dev stream. .NET/NuGET Libraries / Marketplace (Dependencies) The following marketplace plugins and nuget packages are leveraged in this extension. The goal is to reduce the number of marketplace plugins and slowly transition towards using the CLI exclusively coupled with Dataverse API calls to reduce dependencies. Name Type Purpose PowerPlatform Build Tools DevOps Marketplace Plugin Helper YAML extensions for pipelines Power DevOps Tools DevOps Marketplace Plugin Helper YAML extensions for pipelines PowerPlatforms CLI Nuget Package Office Microsoft CLI for PowerPlatform Developer initiated multi-stage pipeline The process below illustrates the typical process for which a developer will execute the build integration pipeline. In this example, the developer is transferring Organizations and Contacts from developed to a staging environment before transferring the CanExport solution as this solution has workflow dependencies relying on Organization and Contact lookup values being populated. At the same time, the user wants to deploy his/her latest portal changes to the staging. *NOTE* to enable CI, you can opt to have this pipeline trigger when a PR issued to a specific branch. However, the process below is recommended as it provides the build team with the ability to check-in their work and automatically deploy and commit their changes and generate the artefacts and test logs. If you decide to employ full automation, simply update the trigger of this pipeline to a specific branch and ensure that the runtime variables configured are pre-defined (see variables section) -- the variable group is already pre-defined so not additional configuration required there. If successful, each stage (1 per artefact specified in the variables), will store its artefact in the Agent's Artefact Staging Directory for a release to a UAT and eventually PROD environment. The pipeline execution's success means that the source developer environment artefacts provided in the variables will be deployed on the target staging environment (or test environment, depending on your deployment strategy for build integration). The solutions are deployed as managed, and the portal is deployed using the CLI and only delta portal changes are also deployed to the target staging environment (or build environment). To view the artefacts, click on the artefact anchor in one of the stages. You can then view the download the artefacts and the solution and portal checker log files for review. Note that this is the happy path. If any of the stages fail, the pipeline will exit to ensure that no subsequent solutions, data files or portal is deployed. In this case, you can click on the failed stage(s) to view the detailed log of the failure. Below demonstrates the artefact storage structure. Notice that the folder names will include an integer as a suffix which informs the eventual release of the sequence for which to deploy these artefacts to downstream environments. *Note that as you add additional tests to this CI/CD extension, you should consider adding your test report(s) to the artefacts instead of just in the GIT repository so that in the future, you have the ability to leverage other pipeline tasks to send these report artifacts to a SIEM to archiving system* In the event the pipeline execution fails, you can view the logs by clicking on any failed stage to view the details. In the example below, the TargetSPN was misspelled in the variable group resulting in a connection issue to the target environment for the data import. Release Manager multi-stage release The process below illustrates the typical process for which a release manager will execute a full release to UAT and invoke the PROD release manually only once UAT is completed. This is useful because, once the artefacts are deployed to UAT, the client can test against the acceptance criteria and if everything is ok, the release manager can return to this release and execute the production stage knowing that the same artefacts deployed to UAT successfully will be used to deploy to production. Using the example build from the previous section, the release manager can navigate to the successful build, select the ellipsis and press \"Release\" When creating the new release, ensure to select the \"Production Deployment Stage\" as a stage to run \"Manually\". This ensures that the artefacts will only be deployed to your UAT environment to allow for client UAT. You will execute the production deployment stage once UAT is completed in a subsequent step. Once created, the release anchor will be displayed just below the breadcrumb, but you can also view the progress of the release under the releases menu. In the release details, you will notice that the UAT deployment is being exectuted whereas Production is not being deployed. In addition, the artefacts from the release are those that were generated by the successful build. Once succeeded you should see the following. *Note that if you would like to re-deploy the same artefacts but to a different environment, the UAT stage below relies on the same variable group used in the build integration pipeline. To do so you could update the TargetURL-UAT, TargetSPN-UAT, ClientID and Secret (if the App User is different in the new target) OR simply clone the Connection-Parameters variable group, re-run the same build (in this case _20220616.3) and edit the Variable-Group variable to your new group name and re-run the build integration pipeline and issue a new release. The latter would be recommended to avoid any disruption from updating connection parameters of your main development stream. IN summary, if you would like to employ the same release pipeline deployment strategy to another set of downstream environment(s), simply clone the Connection-Parameters variable group, everything else an remain the same as this CI/CD extension allows you to specify a variable group at runtime to give you control over which environments you would like to apply CI/CD to* Once UAT has been successfully completed, and you are ready to deploy to production. Navigate to the release that deployed the UAT artefacts that resulting in the successful UAT testing cycle and execute the production deployment CONFIGURING THE CI/CD PIPELINE This section describes how to deploy and configure the CI/CD artefacts to your DevOps project. Pre-Requisites Commit the following Pipeline Automation YAML File to your repository's root directory Commit the following PowerShell script to a folder named \"Pipeline-Scripts\" in your repository's root directory Download the release-pipeline JSON file Create a Solutions folder in your repository's root directory Create a Data folder in your repository's root directory Create a ExportedData folder in your repository's Data folder (created in the previous step) Create a Portals folder in your repository's root directory Create a Logs folder in your repository's root directory Create a Deployment-Profiles folder in your repository's Profile directory Create a folder that matches the name of your PowerApps Portals website record in your repository's Portals folder. In that folder, create a deployment-profiles folder and include your dev, test, uat, and prod YAML profile files (e.g. test.deployment.yml <- where test is the CLI parameter flag that will inform the CLI which deployment profile to use for your portal deployments. The example below demonstrates the above folder structure implemented in a DevOps repository to leverages this extension Install the following marketplace plugins Create Pipeline This section demonstrates the steps to create the build integration pipeline. Create the Pipeline Select the CI/CD YML File provided for this extension, which needs to hosted in your repository's root directory. You can also choose the branch for which the YML file exists. Next create all the following variables and make sure to set the variable group name to \"Connection-Parameters\" or whichever name you will use for your variable group you will create in the next section. Once completed, press the drop down next to the blue run button and press Save [(Do not attempt to run the pipeline at this point)]{.underline} Create Variable Group & Link to Pipeline When creating the group ensure that the variable names match those listed in the [Variable Group section of this document] The pipeline is not fully configured and can be executed at anytime Create The Release Pipeline Even though the pipelines can be used to also release to UAT and PROD environments, the recommended method to release to these environments is to use the releases feature of DevOps. The steps below demonstrate how to configure the release pipeline so that release managers can issue releases based on builds that were ran by the build integration pipeline (CI). Select the PowerPlatform-Deployment JSON file provided with this extension and press \"OK\" Once imported you will be asked to specify your agent pool. You can select \"Azure Pipelines\" or your own custom pool and \"windows-latest\" as the agent specifications (for both UAT and PROD) Once specified, return to the release pipeline and the errors should be gone. You're release pipeline is now ready for use."
  },
  "_site/documentation/Azure-Storage/Storage-Diagnostics.html": {
    "href": "_site/documentation/Azure-Storage/Storage-Diagnostics.html",
    "title": "Azure Storage - Diagnostics & Technical Documentation & User Guides | GOC Theme Documentation",
    "keywords": "Azure Storage - Diagnostics & Technical Documentation & User Guides Download PDF Pre-Requisites Dataverse environment administrator including user with PowerPlatform Administrator rights and System Administrator security role assigned / access to the environment for which diagnostics is being configured. Global Administrator, Owner of an Azure subscription hosting the Azure Storage resource Purpose PowerApps Portals has a feature that will send runtime exception details and other errors to a series of log files that can be hosted in an Azure Storage Blob. This is useful to avoid displaying detailed trace logs to portal users and provides portal developers / administrators with the ability to review a detailed and verbose log of any exception / error thrown by the portal. Azure Storage & Blob Address Navigate to the subscription that will host the Azure Storage resource and add a new resource. Search for Azure Storage (or Storage Account) Set the appropriate resource group and set the following parameters and go through the entire wizard and leave the defaults. These diagnostics are for development only. Once created, go to the newly created Azure Storage resource, click on the Access Keys menu and provide both the Key and Connection string to the PowerApps Portal developer / admin who is configuring the Portal settings to send the diagnostics log to the newly created blob. Next, provide Reader access to the PowerApps Portal developer/admin(s) who will need to inspect the logs PowerApps Portal – Configure Diagnostics To complete the steps below you will need a connection string and key from the Azure Storage Resource created in the previous section. Navigate to https://make.powerapps.com and select the environment where diagnostics will be configured. Once selected, navigate to apps, click on the ellipsis next to the portal app, press “settings” and then “administration” In the portal administration console, click on Portal Actions and select “Enable Diagnostic Logging” Paste the Connection String from the Storage Account and press configure Once completed, all portal diagnostics will be stored in the Azure Storage in a blob container and logs will be stored in folders by Date and the Portal’s App Registration ID. To view logs, go the Azure Storage resource, click on Containers and “telemetry-logs”. Each portal configured to send its diagnostics to this container will have a dedicated folder with there respective App Registration Record ID’s Logs within the portal folder have a folder for each day and a file for each diagnostic sent to the container"
  },
  "_site/documentation/AzureB2C/Combined.html": {
    "href": "_site/documentation/AzureB2C/Combined.html",
    "title": "AZURE B2C | GOC Theme Documentation",
    "keywords": "AZURE B2C Download PDF THIS DOCUMENT DESCRIBES THE AZURE B2C AND THE PROCESS FOR IMPLEMENTING AND OPERATIONALIZING AZURE B2C FOR SECURE SINGLE SIGN ON FOR PORTALS AND API AUTHORIZATION. THE GUIDE DESCRIBES LEVERAGING B2C FOR MULTIPLE PLATFORMS INCLUDING POWERAPPS PORTALS, .NET CORE, .NET FRAMEWORK, AND NODE.JS USING OPENID CONNECT (WEB APPS SSO) AND OAUTH 2.0 (API’S). IT WILL ALSO DEMONSTRATE HOW B2C CAN BE LEVERAGED TO INTEGRATE WITH THE GOVERNMENT OF CANADA’S SSO PLATFORMS (ENTERPRISE AUTHORIZATION BROKER (EAB), AND SIGNIN CANADA (SIC)) AND PLUS IMPLEMENTING A STANDALONE SSO & ENTERPRISE PROFILE SO THAT THE DEPARTMENT CAN CENTRALIZE EXTERNAL USER DATA VIA AZURE AD CLAIMS FOR INTEGRATED APPS TO CONSUME. BY LEVERAGING AN ENTERPRISE SSO, CLIENTS CAN LEVERAGE A SINGLE SET OF CREDENTIALS TO ACCESS ANY EXTERNAL FACING APPLICATION AND SINCE B2C PROVIDES AN OIDC LAYER WITH AZURE AD – THE DEPARTMENT CAN IMPLEMENT CONDITIONAL ACCESS POLICIES (CAP), MFA AND OTHER SECURITY FEATURES WITH THE GOAL TO ADHERE TO ITSG 33/ISO STANDARDS. FINALLY, THIS DOCUMENT WILL OUTLINE THE RECOMMENDED GUARDRAILS TO ACHIEVE A PROTECTED B POSTURE AND DESCRIBE THE ALM PROCESS TO GOVERN DEPLOYMENTS AND CHANGES TO AZURE B2C VIA AZURE PIPELINES (AND WILL ALSO DEMONSTRATE MANUAL DEPLOYMENTS). PREFACE {ORGANIZATION} is undertaking an initiative to provide a secure single sign on service (SSO) by implementing Azure B2C to its subscription services and configure this technology up to a Protected B posture. Azure Active Directory B2C provides business-to-customer identity as a service and is targeted external users and {ORGANIZATION} offers several external facing web applications and API's that require a secure authorization layer for user and machine authentication. Azure B2C has been chosen as the right tool as it provides secure local account identities to get single sign-on access to FINTRACs web applications and APIs but also provides the necessary support to integrate as a relying party to SAML 2.0 and OpenID Connect (OIDC) identity providers such as the Enterprise Access Broker (GCCF CONSOLIDATOR) and SignIn Canada (SIC) both of which are OIDC brokers to the GCCF SAML 2.0 Identity Providers (2keys & Verify.Me). By using this technology, {ORGANIZATION} benefits by centralizing its authentication services into a platform that specializes in this domain and benefits from the leveraging the robust Active Directory toolset including conditional access policies, MFA, groups, monitoring for risky users, automated release pipeline integration, App Registration records (SPN's) to integrate applications. This document will detail the architecture of the Azure B2C implementation and provide the detailed steps to configure and maintain the service. The illustration below depicts the high-level architecture of Azure B2C and its integration with PowerApps Portals, Web Applications and APIs. OPTION 1 b2c with custom domain using afd SECURITY GOVERNED BY B2C -- SAAS, FRONT DOOR LEVERAGED AS CDN AND PROXY FOR CUSTOM DOMAIN OPTION 2 (TBD-FUTURE PHASE - SECURITY PERIMITER GOVERNED BY SCED) OPTION 3 -- b2c without custom domain (OPTIONAL ONLY for instances with sign in canada integration or without a third party identity provider integration) ALIGNMENT TO TBS CLOUD USAGE PROFILES AND CONNECTION PATTERS This section describes AzureB2C's adherence and mapping to the TBS cloud usage profiles and connection patterns. The Government of Canada has a suite of recommendations and hard requirements for implementing cloud systems that host Protected B data or perhaps has inter-connectivity with the data centres hosting Protected B data. Azure B2C is considered a SAAS based technology and therefore, only a subset of the usage profiles and connection patterns will apply to this technology. The entire list of scenarios is outlined below, and the ones that apply to Azure B2C are highlighted below and further described in context of B2C in the following section. Reference Scenario (connection pattern) Description A GC user access to cloud-based service from GC network A GC worker accessing a cloud-based GC service on the GC network. B GC user access to cloud-based service from Internet A GC worker accessing a cloud-based GC service from outside the GC network over the public Internet. C External user access to cloud-based service A non-GC external user accessing a cloud-based GC service from outside the GC network. D Service/Application Interoperability Service and application communications with cloud-based GC services. E Cloud Administration and Management Traffic Management of cloud-based components and support for Network Operations Center (NOC) and Security Operations Center (SOC) activities. Scenario C: External User Access to Cloud-Based GC Service The diagram below depicts an external user accessing Azure B2C indirectly via the internet. The user will access Azure B2C indirectly by first accessing a Web Application that leverages Azure B2C for authentication. This application will redirect the user to Azure B2C for authentication. In the officially approved cloud flow below, this is depicted in use case \"C2\": Non-GC user access to cloud-based GC service hosted in Dept. SaaS Application (in this case Azure B2C). Scenario D: SERVICE/APPLICATION INTEROPERABILITY The diagram below depicts an external service (e.g. API) accessing Azure B2C indirectly via the internet. The service will access Azure B2C indirectly by first accessing a {ORGANIZATION}'s service (e.g. API) that leverages Azure B2C for authentication. This application will request a token via OAUTH 2.0 using a Client ID and Secret (or certificate) to perform subsequent http requests over the internet or via trusted channel such as Express Route or Azure AD Application Proxy. In the officially approved cloud flow below, this is depicted in use case \"D1\", \"D2\", \"D4\", \"D6\". Scenario E: CLOUD ADMINISTRATION AND MANAGEMENT TRAFFIC The diagram below depicts how a GC Administrator would access the Azure B2C service's administrative console. Since Azure B2C is a SAAS application, use case E2 applies to this implementation. The table below describes the official GoC cloud usage profiles. These are meant to characterize the different types of use cases for cloud and whether they apply to SCED. The profiles 1 and 2 apply to Azure B2C's development environment whereas profile 4 is applicable to the production Azure B2C implementation. Even though, Azure B2C serves as an authentication mechanism for both cloud and on-premises applications, since the connection to this services leverages OpenID Connect, the entire flow happens over HTTP/TLS from a client's browser to the B2C and back. In terms of machine-to-machine flows (APIs), Azure B2C serves an OAUTH 2.0 endpoint to obtain tokens for API calls (bearer tokens). Tokens can only be generated by trusted applications who are awarded an App Registration with a secret or certificate. The API must perform HTTP requests once they've received a token from the OAUTH endpoint by passing a ClientID, Secret (or Certificate), TenantID, and Reply URI (unique identifier) in its payload. AZURE B2C CONNECTION PATTERNS -- MICROSOFT This section describes and illustrates the relevant connection patterns that Azure B2C supports out of the box. These patterns are officially supported by Microsoft; however, {ORGANIZATION} does have additional flexibility to extend and go beyond the boundaries of these patterns. However, it is recommended that {ORGANIZATION} does not deviate or attempt to significantly customize and extend Azure B2C's capabilities and instead leverage its OOB features and where limitations are found, attempt to refactor the connecting application to integrate with B2C in a native fashion. This is important because Azure B2C has implemented the OpenID Connect and OAUTH 2.0 specifications and, especially, for web portal authentication, following the OIDC specifications is key to ensuring that security standards are met to avoid any potential pitfalls associated with non-standard ways to integrate with the platform. Web Applications For web applications (including .NET, PHP, Java, Ruby, Python, and Node.js) that are hosted on a server and accessed through a browser, Azure AD B2C supports OpenID Connect for all user experiences. In the Azure AD B2C implementation of OpenID Connect, the web application initiates user experiences by issuing authentication requests to Azure AD. The result of the request is an id_token. This security token represents the user's identity. It also provides information about the user in the form of claims. This also applies to SAAS portal applications such as PowerApps Protals. This technology will abstract the B2C configurations into a user interface to facilitate the integration rather than configuring OIDC parameters and implementing library interfaces and functions in a custom application via code. Once a JWT is issued to a trusted application, the decrypted version may look like the following: { \\\"name\\\": \\\"John Smith\\\", \\\"email\\\": \\\"john.smith@gmail.com\\\", \\\"oid\\\": \\\"d9674823-dffc-4e3f-a6eb-62fe4bd48a58\\\" \\ } The above is applicable only to a user flow (described in a later section), that is configured to capture claims beyond the baseline ones like OID and IDP. For applications that leverage GCCF CONSOLIDATOR or SIC (GCCF services), and where Azure B2C is acting as a pass through service to broker the authentication request between the web application and the third party IDPs, the id_token will simply include the GCCF persistent anonymous identifier (PAI). Therefore the object may look like the following: // Partial content of a decoded id_token { \\\"sub\\\": \\\"3fffreefde54554efdfdfdfdf32113434232\\\", //PAI \\\"oid\\\": \\\"d9674823-dffc-4e3f-a6eb-62fe4bd48a58\\\" \\ } Finally, in a hybrid scenario, whereby the user flow leverages both the third party IDP and a B2C profile, the object will include both the PAI (sub claims) returned by the third party IDP along with the additional claims provided by the user in B2C once redirected to B2C to complete their profile before being redirected to the web application as an authorized user: // Partial content of a decoded id_token { \\\"sub\\\": \\\"3fffreefde54554efdfdfdfdf32113434232\\\", //PAI \\\"name\\\": \\\"John Smith\\\", \\\"email\\\": \\\"john.smith@gmail.com\\\", \\\"oid\\\": \\\"d9674823-dffc-4e3f-a6eb-62fe4bd48a58\\\" \\ } In a typical web application that is using Azure B2C takes these high-level steps: The user browses to the web application. The web application redirects the user to Azure AD B2C indicating the policy to execute. The user completes policy. Azure AD B2C returns an id_token to the browser. The id_token is posted to the redirect URI. The id_token is validated, and a session cookie is set. A secure page is returned to the user. Validation of the id_token by using a public signing key that is received from Azure AD is sufficient to verify the identity of the user. This process also sets a session cookie that can be used to identify the user on subsequent page requests. The sequence diagram below depicts SINGLE PAGE WEB APPS (SPA) In the event the organization integrates an SPA with Azure B2C, a token needs to be generated and returned to the client to perform API calls using JavaScript. SPA's (without a server side renderer like NextJS) will need to perform API calls to the web API that performs CRUD operations to the server application that governs requests to some persistent store like a database or no SQL database or example, because an SPA architecture means that the entire application is rendered to the client browser and therefore all application interactions happen in the browser using the DOM or Shadow DOM (e.g. React JS). Unlike client-server applications, an SPA wont perform server requests each time a user clicks on a link or navigates to another route, therefore, in the event that the SPA also implements authentication, the SPA must use JavaScript to call Azure B2C (via the Graph API). This means that in the JavaScript request, the SPA will need to pass a bearer token which it receives via the oauth2 endpoint and then uses that token to make authorized requests to the server without having to reload the page. These tokens are short lived to minimize the risk of someone hijacking the token to make malicious requests. This journey is demonstrated in the sequence diagram below: API's ~~Applications that contain long-running processes or that operate without the presence of a user also need a way to access secured resources such as web APIs. These applications can authenticate and get tokens by using their identities (rather than a user's delegated identity) and by using the OAuth 2.0 client credentials flow. In order to configure this, the administrator will set up a credential flow using the AAD and MIP token endpoint: https://login.microsoftonline.com/TENANT.onmicrosoft.com/oauth2/v2.0/token Upon receiving a token, the application (Dameon Apps) can perform subsequent HTTP requests using OAUTH 2.0 with a bearer token in the header of the request. This means that there is no user interaction and OIDC is therefore not being leveraged in this flow. Instead, the API that connects to B2C to interface with another API is responsible in obtaining tokens automatically. This API will be issued a Client ID, Secret (or perhaps a certificate) that will be used to obtain short lived tokens. It is recommended that the secret or certificate are rolled over (or changed) at regular intervals of 6 months. The diagram below depicts this scenario. IMPLEMENTATION ARCHITECTURE COMPONENTS This section describes {ORGANIZATION}'s specific implementation of Azure B2C. More specifically, {ORGANIZATION} will be integrating with the official Government of Canada's GCCF service which is SAML 2.0 based with a OAUTH Broker to support more modern applications and libraries. Furthermore, {ORGANIZATION} will be leveraging PowerApps Portals for its external facing portal services and is leveraging Azure B2C as its authentication mechanism. {ORGANIZATION} wishes to use Azure B2C for not only PowerApps but potentially other web applications that require authentication to take advantage of creating a single profile for its external user base and facilitate monitoring, administration, reporting and enhance security by standardizing to one authorization platform rather than having multiple portals implement their own authentication systems or patterns which can be prone to security vulnerabilities and costly to maintain/upgrade. {ORGANIZATION} is also hoping to continue its effort to minimize IAAS in favor for SAAS applications that can operate at Protected B and therefore minimize the complexities associated with hosting complex infrastructures and maintain servers and or containers to ensure that the latest security patches and other OS artefacts are up to date. By leveraging SAAS, {ORGANIZATION} can focus on the application layer, encryption, and enforcing OIDC / OAUTH norms across the organization while providing a better user experience to its clients who will no longer need to maintain multiple sets of credentials to interact with {ORGANIZATION} services. The diagram below depicts, at a high level, how one or more web applications would interface with Azure B2C and in turn, how Azure B2C would then broker the authentication requests (login and logoff) to the Government of Canada's Identity Provider services (2Keys-GCKey & Verify.Me -> partner credentials (banks)). AZURE FRONT DOOR (CDN) Because {ORGANIZATION} is integrating with the GOC Identity Providers via the Broker, to avoid issues such as CORS or third-party cookie / session restrictions in modern browsers, it is recommended that a custom domain on Canada.ca and gc.ca is configured for Azure B2C rather than leveraging the OOB issued domain. Furthermore, TBS and SSC recommend that FINTRACs force the use of TLS 1.2x and even though Azure B2C does support TLS 1.2x, it still supports earlier versions. Both factors warrant the need to implement Azure Front Door as a Proxy/CDN in front of Azure B2C. AFD has native support for B2C and has a configuring to force TLS 1.2x flows to the service and a custom domain to it as well and block the OOB domain. For this implementation, these are the only two requirements to implement in AFD however AFD has additional features that could be leveraged in the future such as WAF (additional custom firewall policies), custom headers, and health probes. AFD is recommended as it's a PAAS and therefore will simply compliment the use of B2C while living within the same cloud usage profile and connection patterns as B2C (e.g. unlike Azure Gateway, AFD is not assigned an IP which would otherwise deem it as IAAS). KEYVAULTS & ENTRUST CERTIFICATES B2C provides full support for the usage of certificates to encrypt and decrypt tokens that are sent and received by client applications. Furthermore, since a custom domain is leveraged with TLS, a certificate is required for the AFD CDN in front of AB2C. These certificates are directly installed in both AB2C and AFD however it is recommended that the CSR's are generated from Azure KeyVaults and once created by the Entrust CA, KeyVaults should be leveraged to generate the PFX files that are installed in both platforms. This ensures that administrators are aware of where the certificates are and when they expire. At the same time, for apps that are leveraging \"secrets\" (keys) instead of certificates to authenticate using the App Registration, these secrets can be referenced in Azure KeyVault's via their Secret ID. By using the KeyVaults to centralize all of the certs and keys for B2C and AFD, {ORGANIZATION} has control over who is accessing these certs, how, and when they need to be renewed. Furthermore, the release pipeline automation for deploying AB2C artefacts (polices) leverages Variable Groups that are linked to the Azure KeyVaults hosting the certs and secrets to avoid referencing secrets or certificates directly in configuration files in DevOps / service connections. APP REGISTRATIONS App Registrations are fundamental in AB2C as they are the primary mechanism used to connect to the service. An App Registration is generated for each client application that needs authentication to the GCCF services. The same applies to Apps that require access to API's that are protected by AB2C. The App Registration is synonymous with an \"SPN\" or \"Service Account\" which is like a machine user that is leveraged by an application to perform authorization requests to the AB2C service. When configuring App Registrations, {ORGANIZATION} will not allow \"Implicit Flows\" and therefore should always avoid selecting \"ID Tokens\" under the authorization flow. Applications must provide a Reply URI as a unique identifier and a \"front channel logout\" URL which B2C will invoke if it receives a logout request in a different browser tab to ensrue that all sessions within the users' browser is purged. Once this information is received by the B2C administrator, the requester will receive a ClientID (unique ID that identifies the application integrating with the service -- which is the app registration assigned to it), a Secret (key) or Certificate (for apps that can support this), and the tenant ID (if needed) to configure within their application's code based (or configurations -- e.g. PowerApps Portals). OPENID CONNECT & SAML 2.0 The GOC Identity Providers GCKey and Verify.Me (formerly SecureKey) that are ran by the Canadian company \"Interac\" provide a SAML 2.0 authentication pattern. SAML 2.0. Security Assertion Markup Language (SAML) is a login standard that helps users access applications based on sessions in another context, in other words, allowing applications to offload their authentication to another service rather than implementing their own user name and password (or similar) pattern directly within their own application. This pattern allows for single sign on whereby users can reuse the same set of credentials across multiple applications in a secure way using XML and RSA certificates issued by a trusted CA on both sides (application and identity provider). SAML authenticates users by redirecting the user's browser to either the GCKey or Verify.Me (or chosen bank) login page, then after successful authentication on that login page, redirecting the user's browser back to the integrated web app where they are granted access. The key to SAML is browser redirects. The diagram below depicts the typical user authorization flow that SAML 2.0 provides to applications: There are three key artefacts that make up the SAML architecture: Identity Provider (IdP) - The software tool or service (often visualized by a login page and/or dashboard) that performs the authentication, checking usernames and passwords, verifying account FINTRACus, invoking two-factor, etc. This is GCKey and Verify.Me. Service Provider (SP) - The web application where user is trying to gain access. This is {ORGANIZATION}'s web application (e.g. PowerApps Portals, or Broker service such as GCCF CONSOLIDATOR or SIC. SAML Assertion - A message asserting a user's identity and often other attributes, sent over HTTP via browser redirects. This is the primary artefact that generates / persists the session. The issue with SAML is that it is less commonly used in modern applications today as its more complex to implement, and a newer and more streamlined and simplified pattern OAUTH 2.0 has become the norm and standard in the industry. However, both the IDPs still only support SAML 2.0. As a result, SSC and TBS have developed OIDC Broker services that abstract the need to implement a SAML Service Provider for web applications. Although Azure B2C does support SAML, it requires a non-trivial customization using custom policies. Furthermore, both IDPs only support the SOAP binding for logout in SAML which is no longer being used by not only B2C but many (most) other SAML supported platforms. As a result, if {ORGANIZATION} were to connect to the IDPs using AB2C directly, they would not be able to implement the back channel single logout pattern implemented by the IDPs as B2C will only support single logout via the redirect binding (so browser based only, not SOAP). Therefore, {ORGANIZATION} would not be fully compliant with the GOC's CATS 2.0 standards and would need to seek an exemption to leverage AB2C. There are ways to implement the SOAP binding and integrate this custom service with AB2C but this would add additional complexity to the architecture and require additional maintenance and niche knowledge. SSC and TBS is aware of the emerging industry standards and therefore have developed an OIDC broker service that handles the connection to the IDPs via SAML and provides an OIDC interface to {ORGANIZATION} and therefore AB2C will interface indirectly with the IDPs using OIDC rather than SAML and therefore is only responsible to meet the CATS 3.0 requirements which map to the official OIDC specifications which AB2C is fully compliant with. This broker service / SAML abstracted is illustrated below: ############################### Login Flow- GCCF Consolidator The illustration below depicts the login flow. ############################### RP-Initiated Logout Flow - GCCF Consolidator The illustration below depicts the relying party-initiated logout flow. ################################ ############################### Single Logout Flow - GCCF Consolidator The illustration below depicts the single logout flow. The OP must invoke the [end session endpoint (e.g. https://<subdomain>.canada.ca/<b2csubdomain>.onmicrosoft.com/b2c_1_<policyname>/oauth2/v2.0/logout)] indicated in the B2C metadata document. This will trigger the single logout / global logout mechanism in B2C LOCAL ACCOUNTS (CLAIMS) Passthrough IDP via oidc broker The GOC IDPs will only provide a PAI token to the B2C. This means that B2C will be informed that the user has successfully authenticated to one of the IDPs and the only information returned to B2C from the IDP is a token that uniquely identifies the user. Once a token arrives to B2C, B2C will first try and match this value in its storage and if not found will treat this user as a new user and prompt them to provide more information such as their email, first name, last name. If the returning token is found, B2C will simply redirect the user back to the application along with the associated email, first name and last name so that the integrated application knows who the person is and can therefore assign them a session under their user context. For the initial implementation, B2C will not be responsible to gather any claims once it receives a token, instead it will simply store the token and redirect the user to the integrated application where this application will be responsible to gather additional claims such as email, first name and last name plus any additional information that is needed for the application itself. The next time this user signs in, B2C redirects the user to the integration web application and if this application finds this user by its token in its own storage, then the user will not be directed to fill in profile information (unless the application flow forces a user to confirm their profile information before accessing the application). Both options will be implemented in {ORGANIZATION} -- the benefit of requiring the user to provide their profile information in B2C is that {ORGANIZATION} can now share additional claims beyond just the PAI between multiple applications and therefore streamline the user experience by eliminating the need for that same user to enter profile information multiple times in different applications despite using applications within the same {ORGANIZATION}. For the enterprise profile, only a limited set of fields (claims) will be populated by a user and integrated apps will receive this information and can opt to ask the user for more profile information that is specific to their app's domain (e.g. a permit application might be interested in knowing which company the user works for, whereas a travel permit or visa application might be interested in obtaining the user's DOB -- these claims would not be captured in B2C but instead directly in the app). MONITORING AB2C provides extensions for both Azure Monitor to route sign in and auditing logs to the SIEM for longer retention or integrate with security information and event management (SIEM) tool (e.g. Sentinel) to gain more granular insights into the B2C implementation. To route log events, Azure Storage, a Log Analytics Workspace and Event Hub services are used. Most of these services get auto generated by the ARM template (provided in references). Once the monitoring is configured, the user insights and authentication visual will demonstrate which usage by country, browser, app (portals), by identity provider, by policy (e.g GCCF CONSOLIDATOR, SIC, Local), and Failures with Reason codes. This provides {ORGANIZATION} with a generalized overview of the overall behavior and key metrics for AB2C. In addition to the common metrics for the overall health and access telemetry of AB2C, the Risk Detection is another report to be implemented that uses Azure AD B2C Sign-in logs to capture risk detections. The dashboards / reports will provide the following data and visualizations Aggregated Risk Levels Risks Levels during SignIn Risk Levels by Region Risk Events by IP Address (Filterable) Risk Events by Type (Filterable) Risk Events Details (based on selected Risk Type) Risk Events by Geo Coordinates (Filterable) Risks Events Over Time By default, the risk detection criteria's include the following: Anonymous IP address use Atypical travel Malware linked IP address Unfamiliar sign-in properties Leaked credentials Password spray AUDITING The information in B2C is limited to the user's object ID (Azure generated ID) and details about the GCCF Identity Provider coupled with any additional claim information configured for the Enterprise Profile (first name, last name, and email). Audit logs are retained for 7 days, however this can be extended by leveraging Azure Monitor and Application Insights to extend beyond this period. Figure : B2C User Tracking - User Object As illustrated in the capture below, GCCF only provides the SAML 2.0 issued Token that uniquely identifies the user which is stored in B2C's Issuer ID field (Claim). B2C will also capture the source of the identity provider being either 2keys (te.clegc-gckey.gc.ca [TEST], clegc-gckey.gc.ca [PROD]) or Verified.Me (cbs-uat-cbs.securekey.com [TEST], cbs.securekey.com [PROD]) Figure : User Object Login Identity Issuer (GCCF Identity Provider chosen) Subsequent logins from the SAML 2.0 federated user is tracked in the B2C audit logs. Figure : B2C User Login Audit Log There are two key events stored, one for successful SAML 2.0 Federation with the Identity provider which provides useful metrics such as the user's location, browser, his/her SPN, which IDP they authenticated to as well as the date and time for the authentication. See below capture of this log in {ORGANIZATION}'s B2C TE environment. Figure : B2C User Login - Audit Detailed View *Modified Properties always blank as GCCF does not map to any property. Figure : Target IDP Detail The second log is the information around the issuance of a Token to the client Application, in this case PowerApps Portals. The log provides the same metrics as the previous log captured above however outlines the target application (AAD App Registration Record): Figure : B2C Audit Log Detail - PowerApps Registration Record GUARDRAILS Since AB2C creates a new Azure AD Domain that is targeted for external user authentication. A subset of the guardrails {ORGANIZATION} would implement for access management by B2C administrators will be implemented. In addition to the access controls, there are a series of settings described in the tables below that need to be configured or activated to ensure that {ORGANIZATION} achieves a Protected B posture in its B2C implementation. For the most part, since AB2C is a service that falls within FINTRACs tenant subscription, the majority of guardrails will be inherited. However, at the AB2C application layer, additional guardrails should be implemented to ensure that the external user and API authentication platform is fully secure and well managed. Guardrails Have a minimum of 2 Global Admins with MFA enabled. The Global Admins should be AD federated accounts and not cloud only accounts. Optional: All other tenant administrators responsible for managing app registrations, connectivity to the OIDC brokers and certificate/secret maintenance are assigned to a newly created B2C Administrators with the following roles: Application Administrator, External Identity Provider Administrator, External ID User Flow Administrator, External ID User Flow Attribute Administrator, User Administrator, B2C IEF Policy Administrator, B2C IEF Keyset Administrator, Domain Name Administrator. A group of type security and dynamic assignment can be created with these roles but requires P2 licensing Create Security Group for each web application or API integrating with B2C (segment and separate) Optional: Enforce TLS 1.2 by deploying AFD -- required only if your organization will have applications connecting to B2C that don't support TLS 1.2 and higher Use a Canada.ca or GC.ca domain by deploying AFD (block the Microsoft domain) Use Entrust TLS Certificate for custom domain (For GCCF Consolidator and SAML implementations) Use Entrust PKI -- Encryption and Decryption Certifications Use RSA 2056 or higher for the Tenant Encryption Key (or Use CSE-approved cryptographic algorithms and protocols) Optional: Use JWT signatures for login and logout requests if using IDPs (signature should be done by trusted CA cert -- entrust, and not a self-signed certificate) -> this only applies to applications that support this feature. For most applications within the PB context, a secret is sufficient. Prohibit the use of implicit flows for applications except for SPA's & certain SAAS portals (consider leveraging server side for authorization rather than client side) Optional: Do not share Secrets directly with Apps. Instead provide 'SecretID' from KeyVaults -- this only applies to applications than can support this feature Enforce Front Channel Logout implementation to client applications by ensuring that the Front Channel Logout URL of an application is always configured in the App Registrations Use Risky Users and Risk Detection monitoring services for significantly greater control over risky authentications and access policies. Azure AD B2C Premium P2 is required Ensure one of the Canada regions are set when installing B2C Create multiple environments (Production and non-production) -- 2 is sufficient Optional: Audit log events are only retained for seven days. Integrate with Azure Monitor to retain the logs for long-term use, or integrate with existing (SIEM) tool -- extend the retention period to 1 year (instead of 30 days) Optional: Setup active alerting and monitoring \"Track user behavior\" feature in Azure AD B2C using Application Insights. CONFIGURING AZURE B2C This section describes the process of coniguring the B2C tenants (P1 & P2). PRE-REQUISITES Global Administrator rights to the {ENVIRONMENT} subscription that will host the Azure B2C Instance Access to a \"Dev\" or \"Sandbox\" Azure subscription used for creating the AzureB2C Development resource Access to a \"Production\" Azure subscription used for creating the AzureB2C Production resource Access to a \"Dev\" or \"Sandbox\" Azure subscription used for creating the Azure Front Door (CDN) Development resource (can be provisioned within the same subscription as b2c) Access to a \"Production\" Azure subscription used for creating the Azure Front Door (CDN) Production resource (can be provisioned within the same subscription as b2c) Access to an Azure KeyVaults resource to store TLS certificates, Secrets and Keys *Note -> since Azure B2C is a PAAS, the 2 tenants can be associated to the same Azure subscription. Secondly, only 1 Azure Front Door instance can be deployed as you can create multiple proxies to each B2C tenant. Another consideration is that since Azure B2C is a PAAS, 1 tenant is enough to support multiple \"user flows\" (environments) to delineate development and production flows. PROVISIONING AZURE B2C Sign into the Azure portal as GA. Switch to the directory that contains the primary subscription (or a Dev subscription preferably). The directory should be on the domain that is connected to the on premise active directory so that B2C configurators leverage their federated accounts instead of cloud only domain accounts (optional-this can be done using your .onmicrosoft.com identity). Once in signed into the Azure subscription as Global Administrator or Subscription Owner, select \"Create Resource\" Search for Azure Active Directory B2C, press Enter and select \"Create\" In the Create blade, click on \"Create a new Azure AD B2C Tenant\" In the following step, enter {ENVIRONMENT}b2cdev for both the organization name and initial domain name (which will be the subdomain) and fill in the country, subscription, resource group and resource group region Click on Review and Create and if Validation has passed click on Create. In the event you encounter the following error when provisioning the tenant: \"The subscription is not registered to use namespace 'Microsoft.AzureActiveDirectory' \" follow the steps below This error is created because on the subscription you want to configure this B2C tenant on creating extra Azure Active Directory's is not enabled (registered). To fix this go to the subscription page Click on the subscription and navigate to the resource providers pane. In the filter by name type in Active Directory. You will see that the Microsoft.AzureActiveDirectory FINTRACus is not registered (enabled). So click on register. The FINTRACus will change from NotRegistered to Registering and after a while it will change to Registered After that redo the steps from the beginning of the provisioning section and you will see that the B2C tenant is created. Inviting B2C Configurators The steps below outline the steps required to provide B2C configurators with the necessary roles to configure identity providers (GCkey), app registrations (integrated portals leveraging GCkey), user flows (the OIDC providers), and maintenance of secrets and certificates once the B2C tenant is created. Click on your profile in the top right, and select switch directory, and switch to the new Azure B2C directory just created Once you are in the new directory search for Azure AD B2C Select Users from the Menu Blade Select New User Select \"Invite User\", enter the name and email, click on Role \"user\" and select \"Global Administrator\" [(note this only applies Global Admin rights to the newly created Dev B2C tenant)] and press select. Once completed, press \"Invite\". [Alternatively, the roles listed under the following screen capture can be assigned to the B2C configurator.] If Global Administrator is not associated to the B2C Configurators, the following roles need to be assigned for the B2C Configurator to configure the tenant: Optional: Creating a group (applicable only with AAD Premium P2 licensing) To streamline the process of identity governance in the B2C domain, AAD Premium P2 Licensing is required to create groups with Dynamic role assignment. That being said, this is optional -- and can be implemented in a later phase. Return to the azure portal home page and search for or click on Azure Active Directory. Note this is the AAD associated with the B2C domain and not the AAD associated with your primary Azure subscription. Click on Groups and press \"New Group\" Fill in the group details as per below and press create *Ensure to select \"Dynamic for Membership Type\"*. Note the group description is: \"Group whose members have the privileges to configure identity providers, app registrations, manage user flows, and administer keys, secrets, and certificates\" CONFIGURING THE AZURE B2C THEME (TENANT WIDE) Select \"Company Branding\" and choose the Default brand. In the modal, upload the Government of Canada JPEG to the Sign-in page background image and the Canada logo under the Banner logo file upload. CONFIGURING A STANDALONE USER FLOW FOR LOCAL AUTHENTICATION (WITHOUT LEVERAGING A THIRD PARTY IDP LIKE GCCF CONSOLIDATOR OR SIC) Select User Flows, and click on New user flow Next, select Sing up and sign in and the recommended tile under \"Version\" Select Email Signup, TOTP, and Conditional for MFA enforcement (policies to be defined in a later section). For user attributes, select Email Address, Given Name and Surname, press Ok and Create Once created, navigate to the newly created user flow and click on properties. Ensure to check \"Require ID Token in Logout Requests\". Next, create a profile and password reset policy. *Apply the same settings from the Sign up and sign in user flow for both policies. For the Password reset policy ensure that the Reset password using email address option is checked. CONFIGURING SIGN IN CANADA SIC (TBS) Azure B2C supports integrating with both OpenID Connect and SAML 2.0 Identity Providers. It can act not only as a \"service provider\" to Applications inside and outside our organization but also as an Identity Provider for your apps. In this section, the guide demonstrates how to integrate Azure B2C with the Government of Canada's OpenID Connect provider - SignIn Canada. The initial set up connects to SignIn Canada's test environment (CATE). The same steps are required for configuring production. We recommend creating a separate Azure B2C environment for Production. CREATE A NEW IDENTITY PROVIDER Navigate to the Identity Provider menu blade, and select \"New Open ID Connect Provider\" and enter the following metadata: Name: Sign In Canada CATE Metadata URL: https://te-auth.id.tbs-sct.gc.ca/oxauth/.well-known/openid-configuration ClientID: {CLIENTID} - ENTER ANY STRING FOR NOW. THIS VALUE WILL BE UPDATED LATER ONCE YOU RECEIVE FROM SIC Client secret: {CLIENTSECRET} - - ENTER ANY STRING FOR NOW. THIS VALUE WILL BE UPDATED LATER ONCE YOU RECEIVE FROM SIC Scope: openid Response Type: code Response Mode: query User ID: sub Display Name: sub CREATING A USER FLOW Select User Flows, and Create a new user flow with the following configurations In the Name field enter SignInCanada-CATE (or something that clearly identifies the service), select \"None\" for the Local Accounts, and Check \"SignIn Canada CATE\" (the identity provider created in the previous step) and leave everything else as default and press \"Create\" Once created click on \"Run User Flow\" and copy the Metadata URL as you will need to send this to the SignIn Canada mailbox in your request to onboard to their test service. Next, locate your Azure B2C TenantID and copy this value in preparation for your request. Once the user flow created and metadata ready you can send the request to SignIn Canada by sending an encrypted email to Signin-AuthentiCanada@tbs-sct.gc.ca. Example email (must be sent by an official Government email that can accept encrypted email -- Entrust CA) We would like to onboard to the SignIn Canada's CATE environment, the information you require is provided below: Our tenant ID is {GUID of Azure B2C's tenant ID} Our Metadata URLs: -> All frameworks support https://gckeyep.b2clogin.com/tfp/<tenantID->GUID>/<nameofuserflow>/v2.0/ -> .NET 4.7.2 support -> optional Our Redirect URLs: -> .NET 4.7.2 support --> optional Please provide the information required for configuration of your service in Azure B2C. SIC will respond by providing you with the information you need to configure the service which will include their metadata information, configuration options and a client ID and Secret. At which point you can proceed to the next steps. Once you've received the response, navigate to the Identity Provider menu blade, and select the SignIn Canada identity provider and update the ClientID and Secret values you've received from SIC: Once the identity provider is configured, edit the user flow and check the new Identity Provider instead of email sign up The same steps above apply for GCCF Consolidator integration (SSC) which is an alternative OIDC broker service managed by 2Keys and governed by SSC. The only difference is instead of the email request, you will need to request for a \"OIDC workbook\" and instruction from SSC by contacting: Kurt.Magalhaes@ssc-spc.gc.ca or louis.leduc@ssc-spc.gc.ca Configure custom domain and force tls 1.2 using azure front door -> [tls 1.2 rule required only if your organization hosts applications that use b2c only support tls 1.1 -- further, tls 1.1 support in b2c to be deprecated (same timeline as azure ad support for tls 1.1)] The following section describes how to configure a custom domain in Azure B2C using AFD. AFD is also required to force TLS 1.2 flows to Azure B2C to meet the GOC guardrail. The illustration below depicts the integration between B2C and AFD Add custom domain to Azure Active Directory from within the Azure B2C tenant Once added, send the TXT record to the Canada.ca DNS. The request should read: Type: TXT, Host: mydomain.canada.ca, TXT: MS-ms658, TTL 3600. Once actioned, you can press verify and proceed to next steps. NOTE, once verified, SSC needs to delete the TXT entry as this domain will be configured as CNAME in a subsequent request. Once SSC has created the DNS record, click on the Verify button and if successful, the FINTRACus will show \"Verified\" Azure Front Door resource for custom domain & rule to force TLS 1.2 In an Azure Subscription, as subscription owner or global administrator, create a new AFD resource. To choose the directory that contains the Azure subscription that you'd like to use for Azure Front Door and not the directory containing your Azure AD B2C tenant a. Select the Directories + subscriptions icon in the portal toolbar. b. On the Portal settings | Directories + subscriptions page, find your Azure AD directory in the Directory name list, and then select Switch button next to the directory (the primary directory): On the Create a Front Door profile page, enter, or select the following settings: Select Review + Create and then select Create to deploy your Azure Front Door profile. Next, send the CNAME request to SSC: Source: yourfrontdoorsubdomain-randomstring.azurefd.net -> Destination: yourcustomdomain.canada.ca ` Using the AFD designer, create a new Frontend for the newly registered domain and create a backend pool that points to the tenant.b2clogin.com OOB domain Create a routing rule that accepts HTTPS protocol and select both the AFD and custom domains under the frontend/domains drop down. Ensure that the backend pool is chosen and that HTTPS only is chosen under Forwarding protocol. Once completed, the configuration can take up to 10 minutes to take effect. To test that your new domain is configured correctly, select an existing user flow, and then select \"run user flow\" Copy the Run user flow endpoint, and replace the sub-domain with the new domain and copy it to your browser to validate that it renders For example, instead of: https://contoso.b2clogin.com/contoso.onmicrosoft.com/oauth2/v2.0/authorize?p=B2C_1\\_susi&client_id=63ba0d17-c4ba-47fd-89e9-31b3c2734339&nonce=defaultNonce&redirect_uri=https%3A%2F%2Fjwt.ms&scope=openid&response_type=id_token&prompt=login Use: https://{customsubdomain}.canada.ca/contoso.onmicrosoft.com/oauth2/v2.0/authorize?p=B2C_1_susi&client_id=63ba0d17-c4ba-47fd-89e9-31b3c2734339&nonce=defaultNonce&redirect_uri=https%3A%2F%2Fjwt.ms&scope=openid&response_type=id_token&prompt=login INTEGRATING web portals TO AZURE B2C To onboard web applications that require SSO, you will need to provide them with the metadata URL, and a ClientID and Secret. When a client creates a request, they will need to provide you with their \"Redirect URL\" and their \"Front Channel Logout URL\". The latter is required for B2C to know which URL to invoke when it receives a logout request from another integrated web application to ensure that the other apps that have an active session are also logged out. Once you have this information, you can proceed with the steps below. Onboarding a PowerPage Site Step 1 (Client): The PowerApps developer will need to provide you with a \"Redirect URL\". To do so they must go to the https://make.powerapps.com, select the environment for which they would like to leverage SignIn Canada, select the portal authentication settings and press \"Add Provider\", select Other, and choose \"OIDC Provider\". Once the modal is displayed, PowerApps generates a \"Redirect URL\" and before they can configure your service, they will need to email you this URL. In addition to this URL, the client should also provide you with the logout URL for their portal. By default, this should always be (for portals without a custom domain). Alternatively, if you already know the portal URL, you can create the app registration record in advance and enter any redirect URI using the following convention and provide the details to the developer to configure in PowerApps. Otherwise, the developer will be responsible to provide you with the Redirect URI. To obtain the Redirect URI directly from PowerApps, the developer can follow these steps: Select \"Add Provider\" and enter the following details in the Wizard Copy the \"Reply URL\" and send to the AzureB2C Administrator to obtain the OIDC data you need to finish the configuration The PowerApps developer will resume the configuration once you've provided them with the App Registration information. Step 2 (App Registration) -- completed by AzureB2C administrator: Once you receive the Reply URL, navigation to the App Registrations Blade and create a new App Registration. The naming convention is at your organization's discretion, but in this example the convention is simply the sub-domain of the PowerApps Portal being on boarded. Please note that the PowerAppsPortals.com domain is typically not used in Production. Therefore, the same client may request you to add additional redirect URLs in the Authentication blade within the App Registration record in the future once SSC enters the DNS entry for their Portal in the Canada.ca DNS. Press \"Register\" Next, in the \"Authentication\" Blade, enter the front channel logout URL which is always the portal URL with /Account/Login/LogOff (for PowerApps). Make sure to select \"Access Tokens\" and press save. Next click on \"Certificates and Secrets\" and generate a secret and copy the secret to your clipboard (or somewhere as you will need to send this via encrypted email to the PowerApps Portals developer) Next click on overview and copy the ClientID. Send the following information back to the PowerApps Portals developer: ClientID Secret Metadata:Error! Hyperlink reference not valid. Step 3 (Finish configuration -- PowerApps): In your email that includes the ClientID, Secret and Metadata URL to the client, include the following example configuration for the developer to finish the configuration. (Optional) -> If the PowerApps Portal wants users to automatically be redirected to the SSO service upon navigating to the \"SignIn\" page, the developer can set the new provider as default Once completed, you can test by navigating to the integrated portal and invoking the sign in button (or anchor) which will automatically send an authorization request to B2C which in turn will send the request to SIC. You can then sign up to a GCKey or Sign-In Partner (banks) account, and once successfully signed in, you should be redirected to your portal's home page if you are an existing portal user. GCKEY Example: Canadian Banks Example (recommended during development for ease of use) Enter test with an integer suffix (e.g. test12345) and scroll to the bottom of the page and press Login. For a returning user, use the same username (e.g. test12345 in this example) you've used as part of the full registration to PowerApps. When simulating a new PowerApps user registration use a new integer suffix. For a new user, you should be prompted to create a profile in the Application as B2C will only send the \"Subject-ID\" claim to the integrated application which can be leveraged as a unique identifier in the portal application. If a subject-id is not yet associated to a profile in the integrated application, the application should invoke a user journey to create a profile in the application (provided the application requires the use of a profile). Below is what a new user (or new subject-id sent to the app) would invoke in PowerApps Portals (GOC PowerApps Theme) Once the profile is created OR if it's a returning user, the authenticated home page (or post login redirect) would render. The example below demonstrates the authenticated home page for the GOC PowerApps Theme. GENERAL SUPPORT AND OPERATIONS This chapter describes the on-going maintenance and governance activities associated with the Azure B2C tenants implemented at {ORGANIZATION} ACCESS CONTROL This section describes the roles and responsibilities surrounding the governance and operational activities surrounding Azure B2C. All Azure B2C administrators including Cloud Ops and SIAZ, except for Global admins on 005gc will inherit the department's policy on PAW devices and use admin accounts to configure and maintain the b2c tenants. Cloud Operations responsible for provisioning the Azure B2C tenants and inviting Azure B2C administrators from SIAZ to administer the B2C tenant. Cloud Operations is also responsible in creating the B2C Administrators AD Group in the new tenants and assign the roles listed below to that group. When inviting administrators from SIAZ, the G.A from Cloud Ops will assign the B2C administrators' group to the administrator who will automatically inherit these roles and can therefore administer the tenant. SIAZ responsible for integrating the OIDC broker service provider (or relying party) by creating user flows and external identity provider configurations in Azure B2C. They will also be responsible to rotate certificates and secrets and onboard Web Apps, SPA's and API's who will integrate with the service by creating and configuring App Registration records. SIAZ will also be responsible to proactively monitor for anomalies, health issues, and threats (or attempts) by observing the logs. Cloud Ops and SIAZ administrators who are guest users within the new B2C tenants, will automatically receive alerts for mis configurations, expiring secrets, keys, and certificates as well as general maintenance/MS platform announcements. Roles The primary roles used to maintain B2C are listed below. A B2C Administrator AD group should be created to maintain the IDP configurations and relying party applications which needs to be comprised of all roles below except for Global Administrator which is reserved to tenant provisioning and initial set up (can be assigned to any admin invited to the tenant to set it up, and role removed once baseline configuration completed and tested). Eventually, {ORGANIZATION} can opt to create 2 Groups: 1 dedicated for integration of portals, another for integration of Identity Providers. For the latter, the roles 2,3,4, 6, 7 are required, and the former would require 1, 4, 6, and 7. # Description Role 1 Register web apps (Generate App Registrations) Application Administrator 2 Configure the OIDC enterprise identity providers. External Identity Provider Administrator 3 Integrate user flows with web APIs and integrate with external systems. (OPTIONAL- not phase 1) External ID User Flow Administrator 4 Manage User Administrator accounts and administrative accounts as described in this article. 5 Manage role assignments in Azure AD B2C directory. Create and manage groups that can be assigned to Azure Global Administrator, Privileged Role AD B2C roles. Administrator 6 Create, read, update, and delete all custom policies in Azure AD B2C. B2C IEF Policy Administrator 7 Add and manage encryption keys for signing and validating tokens, client secrets, certificates, and B2C IEF Keyset Administrator passwords used in custom policies. 8 Access tenant logs Global Reader 9 Domain administration -- for custom domain configuration Domain Administrator 10 Can create and manage the attribute schema available to all user flows. External ID user flow attribute administrator Contingency & Incidents In event of an outage or user flow issue pertaining to B2C or in scenarios where the identity providers are \"down\", a 404/maintenance page is designed to instruct external users that the System is undergoing maintenance. In a scenario where B2C is inoperable, {ORGANIZATION} will raise a ticket with MS to resolve the issue. If the IDPs are down (or the OIDC broker is down), the same maintenance page will be shown to the user however the B2C administrator will need to temporarily disable the user flow. For mission critical portals, {ORGANIZATION} has the option of activating a temporary local account user flow which relying parties can invoke to allow users to continue to access the critical application. In this scenario, relying party applications must have the capability to sync existing users in their system with this new temporary identity until the IDP(s) (or OIDC broker) is operational. For portals that cannot support this, users will simply be shown the maintenance page in B2C. Regardless of application support for a maintenance page for the portal's local login / registration landing page, a B2C maintenance page will be activated when the services are down, including when the IDP, OIDC Broker, and or B2C itself is inoperable. User flows are version controlled in Azure DevOps (link to be provided). In the event of an administrator accidently deleting or deliberately deleting the user flow which would render authentication impossible to the IDPs, an authorized B2C administrator or Global admin will be able to restore to the tenant's user flow by running the B2C pipeline from DevOps or importing the user flow manually from source control. If the user flow is not available in source control due to extraneous circumstances, the B2C tenant can be restored in a known healthy FINTRACe via issuing a ticket to Microsoft. B2C maintenance page (EN) Integration of web apps When a client requests to register a web application to leverage the Azure B2C SSO integration with the OIDC Broker, the following information must be provided by the Client. Name Description Redirect URI URL that the relying party application will provide that uniquely identifies their application and is used both in Azure B2C and in the Relying Party's OIDC configuration as a trust attribute. Names and contact information of List of at least 1 or more Maintainers / Admins / Devs individuals who are responsible to maintain the application and in particular are responsible for the SSO capabilities within the web application Requires MFA? Conditional based on the web apps requirement. Once provided, the Azure B2C administrator will need to respond with the following information Name Description Client ID Unique identifier (GUID) generated from App Registration record associated with this new web application integration Tenant ID Unique identifier (GUID) of the Azure B2C's tenant properties (Tenant ID) OIDC Metadata Document Metadata URL for the OIDC User Flow Client Secret Secret generated from the App Registration record associated with this new web application integration Client Secret Expiry Date Secret expiry date for the client record. Reminders will be automatically sent, but the client may want to store this date for reference / planning purposes Integration of API's When a client requests to register a WebAPI to leverage the Azure B2C tenant as an API authorization layer, the following information must be provided by the Client. Name Description RedirectURI URL that the relying party application will provide that uniquely identifies their application and is used both in Azure B2C and in the Relying Party's OIDC configuration as a trust attribute. Names and contact information of List of at least 1 or more Maintainers / Admins / Devs individuals who are responsible to maintain the application and in particular are responsible for the SSO capabilities within the web application Requires MFA? Conditional based on the web apps requirement. Once provided, the Azure B2C administrator will need to respond with the following information Name Description Client ID Unique identifier (GUID) generated from App Registration record associated with this new web application integration Tenant ID Unique identifier (GUID) of the Azure B2C's tenant properties (Tenant ID) OIDC Metadata Document Metadata URL for the OIDC User Flow Client Secret Secret generated from the App Registration record associated with this new web application integration Client Secret Expiry Date Secret expiry date for the client record. Reminders will be automatically sent, but the client may want to store this date for reference / planning purposes SECURING ON-PREMISEs APPS WITH F5 BIG-IP configuration A BIG-IP offers several methods for configuring Azure AD secure hybrid access, including a wizard based Guided Configuration, minimizing time, and effort to implement several common scenarios. Its workflow-driven framework provides an intuitive experience tailored to specific access topologies and is used for rapid publishing of web services requiring minimal configuration to publish. Version check This tutorial is based on Guided Configuration v.7/8 but may also apply to previous versions. To check your version, login to the BIG-IP web config with an admin account and go to Access > Guided Configuration. The version should be displayed in the top right-hand corner. To upgrade your BIG-IP's Guided Configuration, follow these instructions. SSL profiles Configuring your BIG-IP with a client SSL profile will allow you to secure the client-side traffic over TLS. To do this you'll need to import a certificate matching the domain name used by the public facing URL for your application. Where possible we recommend using a public certificate authority, but the built-in BIG-IP self-signed certificates can also be used while testing. Add and manage certificates in the BIG-IP VE. Guided configuration In the web config, go to Access > Guided Configuration to launch the deployment wizard. Select the Federation > F5 as OAuth Client and Resource Server. Observe the summary of the flow for this scenario, then select Next to start the wizard. OAuth properties This section defines the properties enabling federation between the BIG-IP APM and the OAuth authorization server, your Azure AD B2C tenant. OAuth will be referenced throughout the BIG-IP configuration, but the solution will actually use OIDC, a simple identity layer on top of the OAuth 2.0 protocol allowing OIDC clients to verify the identity of users and obtaining other profile information. Pay close attention to detail, as any mistakes will impact authentication and access. Configuration name Providing a display name for the configuration will help you distinguish between the many deployment configs that could eventually exist in the guided configuration. Once set, the name cannot be changed, and is only visible in the Guided Configuration view. Mode The BIG-IP APM will act as an OIDC client, so select the Client option only. DNS resolver The specified target must be able to resolve the public IP addresses of your Azure AD B2C endpoints. Choose an existing public DNS resolver or create a new one. Provider settings Here, we'll configure Azure AD B2C as the OAuth2 IdP. You'll notice that the Guided Configuration v8 offers Azure AD B2C templates, but as it's missing several scopes, we'll use a custom type for now. F5 is looking to include the missing scopes in a future Guided Configuration update. Add a new provider and configure it as follows: OAuth general properties Properties Description OAuth provider Custom type Choose OAuth Create new (or use an existing OAuth provider if it provider exists) Name A unique display name for the B2C IdP. This name will be displayed to users as a provider option to sign-in against. Token type JSON web token OAuth policy settings Properties Description Scope Leave blank, the OpenID scope to sign users in will be added automatically Grant type Authorization code Enable OpenID Check to put the APM OAuth client in OIDC mode Connect Flow type Authorization code OAuth provider settings The below OpenID URI refers to the metadata endpoint used by OIDC clients to autodiscover critical IdP information such as the rollover of signing certificates. Locate the metadata endpoint for your Azure AD B2C tenant by navigating to App registrations > Endpoints and copying the Azure AD B2C OpenID Connect metadata document URI. For example, https://wacketywackb2c .b2clogin.com/<tenantname>.onmicrosoft.com/<policyname>/v2.0/.well-known/openid-configuration. Then update the URI with your own properties, https://<tenantname>.b2clogin.com/WacketywackB2C.onmicrosoft.com/B2C_1_SignUpIn/v2.0/.well-known/openid-configuration. Paste this URI into the browser to view the OIDC metadata for your Azure AD B2C tenant. Properties Description Audience The client ID of the application representing the BIG-IP in your Azure AD B2C tenant Authentication The authorization endpoint in your B2C OIDC metadata URI Token URI The token endpoint in your Azure AD B2C metadata Userinfo request Leave empty. Azure AD B2C does not currently support URI this feature OpenID URI The OpenID URI metadata endpoint you crafted above Ignore expired Leave unchecked certificate validation Allow Check self-signed JWK config certificate Trusted CA Select ca-bundle.crt to use the default F5 trusted bundle authorities Discovery Provide a suitable interval for the BIG-IP to query interval your Azure AD B2C tenant for updates. The minimum interval time offered by AGC version 16.1 0.0.19 final, is 5 minutes. OAuth server settings This section refers to the OIDC authorization server, being your Azure AD B2C tenant. Properties Descriptions Client ID The client ID of the application representing the BIG-IP in your Azure AD B2C tenant. Client secret The application's corresponding client secret. Client-server Setting an SSL profile will ensure the APM communicates SSL profile with the Azure AD B2C IdP over TLS. Select the default serverssl option. OAuth request settings The BIG-IP interestingly has all the required Azure AD B2C requests in its pre-configured request set. However, it was observed that for the build we were implementing on, these requests were malformed, and missing important parameters. So, we opted to create them manually. Token request - Enabled Properties Description Choose OAuth request Create new HTTP method POST Enable headers Unchecked Enable parameters Checked Parameter type Parameter name Parameter value client-id client-id nonce nonce redirect-uri redirect-uri scope scope response-type response-type client-secret client-secret custom grant_type authorization_code Auth redirect request - Enabled Properties Description Choose OAuth request Create new HTTP method GET Prompt type None Enable headers Unchecked Enable parameters Checked Parameter type Parameter name Parameter value client-id client-id redirect-uri redirect-uri response-type response-type scope scope nonce nonce Token refresh request - Disabled - Can be enabled and configured if necessary. OpenID UserInfo request - Disabled - Not currently supported in global Azure AD B2C tenants. Virtual server properties A BIG-IP virtual server must be created to intercept external client requests for the backend service being protected via secure hybrid access. The virtual server must be assigned an IP that is mapped to the public DNS record for the BIG-IP service endpoint representing the application. Go ahead and use an existing Virtual Server if available, otherwise provide the following: Properties Description Destination Private or Public IP that will become the BIG-IP service address endpoint for the backend application Service port HTTPS Enable redirect Check to have users auto redirected from http to https port Redirect port HTTP Client SSL Swap the predefined clientssl profile with the one profile containing your SSL certificate. Testing with the default profile is also ok but will likely cause a browser alert. Pool properties Backend services are represented in the BIG-IP as a pool, containing one or more application servers that virtual server's direct inbound traffic to. Select an existing pool, otherwise create a new one. Properties Description Load-balancing method Leave as Round Robin Pool server Internal IP of backend application Port Service port of backend application Note The BIG-IP must have line of sight to the pool server address specified. Single sign-on settings A BIG-IP supports many SSO options, but in OAuth client mode the Guided Config is limited to Kerberos or HTTP Headers. Enable SSO and use the following information to have the APM map inbound attributes you defined earlier, to outbound headers. Properties Description Header Operation Insert Header Name 'name' Header Value %{session.oauth.client.last.id_token.name} Header Operation Insert Header Name agentid Header Value %{session.oauth.client.last.id_token.extension_AgentGeo} Note APM session variables defined within curly brackets are CASE sensitive. So, entering agentid when the Azure AD B2C attribute name is being sent as AgentID will cause an attribute mapping failure. Unless necessary, we recommend defining all attributes in lowercase. In an Azure AD B2C case, the user flow prompts the user for the additional attributes using the name of the attribute as displayed in the portal, so using normal sentence case instead of lowercase might be preferable. Customization properties These settings allow you to customize the language and the look and feel of the screens that your users encounter when they interact with the APM access policy flow. You can personalize the screen messages and prompts, change screen layouts, colors, images, and localize captions, descriptions, and messages that are normally customizable in the access policy items. Replace the \"F5 Networks\" string in the Form Header text field with the name of your own organization. For example, \"Wacketywack Inc. Secure hybrid access\". Session management properties A BIG-IPs session management setting is used to define the conditions under which user sessions are terminated or allowed to continue, limits for users and IP addresses, and error pages. These are optional, but we highly recommend implementing single log out (SLO) functionality, which ensures sessions are securely terminated when no longer required, reducing the risk of someone inadvertently gaining unauthorized access to published applications. App Registration management Secret rollover App Registration secrets have a lifetime of between 6 and 24 months. The administrator mailbox account and App registration secret creator and App registration owner(s) will receive a notification 60 days from the expiry date. Upon receipt, the responsible individual will send an email to the web application owner(s) instructing them to plan for an app secret update to their configuration for both non productionized environments and for production (depending on the expiring secret). Certificate rollover Certificates have a lifetime of between 6 and 72 months. The administrator mailbox account and App registration creator and App registration owner(s) will receive a notification 60 days from the expiry date. This in additional to the CA reminders on the certificate itself (e.g. Entrust, KeyVaults). Upon receipt, the responsible individual will send an email to the web application owner(s) instructing them to plan for a certificate update to their configuration for both non productionized environments and for production (depending on the expiring certificate). Updating Identity Provider secrets The OIDC Identity Providers will occasionally require the Azure B2C to update the shared secret for both the CATE and PROD environments. To do so, simply edit the identity provider, and update the secret value. Once the update is successful, you can test the JWT.ms relying party to ensure that SSO is still operational. If the updated secret did not work, you can paste the original secret (copied from Azure KeyVaults) to rollback. Updating Token Encryption and Signing Keys (Tenant wide) N/A (only applies for custom policies -> not in phase 1). Tenant health audit (annual) Investigating Risks -- Identity Protection in Azure B2C Identity Protection provides ongoing risk detection for the Azure AD B2C tenant. It will help discover, investigate, and remediate identity-based risks. Identity Protection comes with risk reports that can be used to investigate identity risks in Azure AD B2C tenants. This feature is available via the Risky User's menu blade in the Azure B2C Tenant and further described in this section. Note that the majority of risk detection, monitoring and security features for SSO is managed by the identity providers however by activating this feature, {ORGANIZATION} protects the tenant from unauthorized access and other potential threats for not only SSO targeted to external users but also internal users trying access the tenant maliciously or not abiding by the organization's access policies. Overview Azure AD B2C Identity Protection provides two reports. The Risky users report is where administrators can find which users are at risk and details about detections. The risk detections report provides information about each risk detection, including type, other risks triggered at the same time, sign-in attempt location, and more. Each report launches with a list of all detections for the period shown at the top of the report. Reports can be filtered using the filters across the top of the report. Administrators can choose to download the data, or use MS Graph API and Microsoft Graph PowerShell SDK to continuously export the data. Service limitations and considerations When using Identity Protection, consider the following: Identity Protection is on by default. Identity Protection is available for both local and social identities, such as Google or Facebook. For social identities, Conditional Access must be activated. Detection is limited because the social account credentials are managed by the external identity provider. In Azure AD B2C tenants, only a subset of the Azure AD Identity Protection risk detections is available. The following risk detections are supported by Azure AD B2C: Risk detectionDescription** type** Atypical travel Sign-in from an atypical location based on the user's recent sign-ins. Anonymous IP Sign-in from an anonymous IP address (for example: address Tor browser, anonymizer VPNs). Malware linked IP Sign-in from a malware linked IP address. address Unfamiliar sign-in Sign-in with properties we've not seen recently properties for the given user. Admin confirmed An admin has indicated that a user was compromised. user compromised Password spray Sign-in through a password spray attack. Azure AD threat Microsoft's internal and external threat intelligence intelligence sources have identified a known attack pattern. Pricing tier Azure AD B2C Premium P2 is required for some Identity Protection features. If necessary, change your Azure AD B2C pricing tier to Premium P2. The following table summarizes Identity Protection features and the required pricing tier. Feature P1 P2 Investigate risky users With the information provided by the risky users report, administrators can find: The Risk FINTRACe, showing which users are At risk, have had risk Remediated, or have had risk Dismissed Details about detections History of all risky sign-ins Risk history Administrators can then choose to act on these events. Administrators can choose to: Reset the user password Confirm user compromise Dismiss user risk Block user from signing in Investigate further using Azure ATP An administrator can choose to dismiss a user's risk in the Azure portal or programmatically through the Microsoft Graph API Dismiss User Risk. Administrator privileges are required to dismiss a user's risk. Remediating a risk can be performed by the risky user or by an administrator on the user's behalf, for example through a password reset. Navigating the risky users report Sign in to the Azure portal. Make sure you're using the directory that contains your Azure AD B2C tenant. Select the Directories + subscriptions icon in the portal toolbar. On the Portal settings | Directories + subscriptions page, find your Azure AD B2C directory in the Directory name list, and then select Switch. Under Azure services, select Azure AD B2C. Or use the search box to find and select Azure AD B2C. Under Security, select Risky users. Selecting individual entries expands a details window below the detections. The details' view allows administrators to investigate and perform actions on each detection. Risk detections report The risk detections report contains filterable data for up to the past 90 days (three months). With the information provided by the risk detections report, administrators can find: Information about each risk detection including type. Other risks triggered at the same time. Sign-in attempt location. Administrators can then choose to return to the user's risk or sign-ins report to take actions based on information gathered. Navigating the risk detections report In the Azure portal, search for and select Azure AD B2C. Under Security, select Risk detections"
  },
  "_site/documentation/Digital Signature Module/index.html": {
    "href": "_site/documentation/Digital Signature Module/index.html",
    "title": "Digital Signatures - Introduction (PHASE 2) | GOC Theme Documentation",
    "keywords": "Digital Signatures - Introduction (PHASE 2) Download PDF Digital Signatures - A systematic approach of ensuring data integrity using public key encryption techniques. Design Principles Immutability Auditabilty / monitoring Strong cryptography Platform agnostic (API Driven) Integration with Active Directory & Entrust (GoC/{ORGANIZATION} standards) MFA as a pre-cursor to signature Multi layered access (access tiers) / administration (separate RBAC model/separation of concern) Traceability Data residency & ownership Abstractions / transparent to user (e.g., low complexity UX and configuration) Architecture & Software (diagram to-do) Azure Immutable Storage Persistent layer storing JSON payload re Azure KeyVaults Leveraged for signing, encryption, de-cryption for digital signature payloads stored in both Dataverse & Immutable storage MS Authenticator Entrust PKI Private/Pub key pairs (or keys) – integration with AKV? D365 SDK/API Class libraries (plugins) + odata endpoint to interface with Immutable Storage & AKV. Graph API Azure API platform to interface with azure services (leverage App Registrations for authorization to AKV, Immutable storage) Log Analytics Log platform that will ingest / aggregate all logs for the digital signatures module Immutable Record Policies & Applicability of these policies Time-based retention policies With a time-based retention policy, users can set policies to store data for a specified interval. When a time-based retention policy is set, objects can be created and read, but not modified or deleted. After the retention period has expired, objects can be deleted but not overwritten. Legal hold policies: A legal hold stores immutable data until the legal hold is explicitly cleared. When a legal hold is set, objects can be created and read, but not modified or deleted. Regulatory compliance: Immutable storage for Azure Blob Storage will address SEC 17a-4(f), CFTC 1.31(d), FINRA, and other regulations. (equivalent GoC specific policies to be added here + algorithms) Secure document retention: Immutable storage for blobs ensures that data can't be modified or deleted by any user, not even by users with account administrative privileges. Application Architecture / Key Features Decision module includes flag for digital signing. If true, administrator will select a configured “digital signature configuration” Digital signature configuration administration module includes: Payload configuration: Tables/Columns -> relationships to include in a generated JSON object Security Role(s) / Team(s) permitted to generate payload Signature: When a user creates the decision, they are challenged for MFA (authenticator/TOTP) and once successful, are prompted to provide their private key which is verified against the corresponding public key in Azure KeyVaults. In addition. A successfully signed decision, will construct a JSON object of the associated record by referencing the payload schema of the “digital signature configuration” and cryptographically sign it using the users’ private key and store the record in Azure Immutable storage. The address of the Azure Immutable storage record that holds the record will be stored in the CRM decision and on the record in question (e.g. a security clearance). In addition, once the user signs the record, the same encrypted / hashed version of the JSON payload is stored in the “core_hash” multiline text field in CRM, and a CRM plugin that runs on retrieve, update, delete, append, and append to will reference this value and compare it against the immutable value for any discrepancies and will prevent any further updates on the CRM side. In addition, Azure Sentinel and the Compliance Centre will monitor that the public facing (or mutable) version of the official record matches the the immutable value (querying both the Dataverse API & Storage API). Dashboard of the aggregation of results to be designed, events configured (alerts), and process for actioning discrepancies. Ability to generate a physical copy of the official record is an RFID (or similar) representation of the payload (TBD/Optional) Network Security Traffic must originate from a VNet. A VNet enables clients to securely connect to your storage account. The only way to secure the data in your account is by using a VNet and other network security settings. Any other tool used to secure data including account key authorization, Azure Active Directory (AD) security, and access control lists (ACLs) are not yet supported in accounts that have the NFS 3.0 protocol support enabled on them. Setting Comments Configure the minimum required version of Transport Layer Security (TLS) for a storage account. Require that clients use a more secure version of TLS to make requests against an Azure Storage account by configuring the minimum version of TLS for that account. For more information, see Configure minimum required version of Transport Layer Security (TLS) for a storage account Enable the Secure transfer required option on all of your storage accounts When you enable the Secure transfer required option, all requests made against the storage account must take place over secure connections. Any requests made over HTTP will fail. For more information, see Require secure transfer in Azure Storage. Enable firewall rules Configure firewall rules to limit access to your storage account to requests that originate from specified IP addresses or ranges, or from a list of subnets in an Azure Virtual Network (VNet). For more information about configuring firewall rules, see Configure Azure Storage firewalls and virtual networks. Allow trusted Microsoft services to access the storage account Turning on firewall rules for your storage account blocks incoming requests for data by default, unless the requests originate from a service operating within an Azure Virtual Network (VNet) or from allowed public IP addresses. Requests that are blocked include those from other Azure services, from the Azure portal, from logging and metrics services, and so on. You can permit requests from other Azure services by adding an exception to allow trusted Microsoft services to access the storage account. For more information about adding an exception for trusted Microsoft services, see Configure Azure Storage firewalls and virtual networks. Use private endpoints A private endpoint assigns a private IP address from your Azure Virtual Network (VNet) to the storage account. It secures all traffic between your VNet and the storage account over a private link. For more information about private endpoints, see Connect privately to a storage account using Azure Private Endpoint. Use VNet service tags A service tag represents a group of IP address prefixes from a given Azure service. Microsoft manages the address prefixes encompassed by the service tag and automatically updates the service tag as addresses change. For more information about service tags supported by Azure Storage, see Azure service tags overview. For a tutorial that shows how to use service tags to create outbound network rules, see Restrict access to PaaS resources. Limit network access to specific networks Limiting network access to networks hosting clients requiring access reduces the exposure of your resources to network attacks. Configure network routing preference You can configure network routing preference for your Azure storage account to specify how network traffic is routed to your account from clients over the Internet using the Microsoft global network or Internet routing. For more information, see Configure network routing preference for Azure Storage. Acceptance Criteria Administrator Global configuration Can configure Azure immutable storage container address (CRM Settings) for environment (1:N) If 1 or more blobs are linked, update not permitted – can create new container address record. Can set the active address set to Immutable storage address lookup N:1 Digital Signature Type Configuration Can create digital signature type record Can associate decision type(s) to digital record type record By associating the decision type, the table that is associated in the decision configuration will automatically be chosen for the “digital signature record schema” multi line text field which holds the JSON representation of the table holding records that can be signed/encrypted with this module. Can create/update schema to include association(s) to primary ”signable” record Can activate digital signature record Co-administrator approves configuration (decision) Can archive digital signature type record Co-administrator approves configuration (decision), transitions state to “archived” Can view / export report of digital signature type record usage Can view / export report of permitted users Acceptance Criteria: CRM Users (non-admin) Certificate configuration Can configure download encryption/signing key and install to Entrust CA store on organization machine (create w/ password - export w/ private key and import) – canvas (or desktop -> offline only) Digitally signing a record (via decision) Can provide approve MFA challenge upon creation of decision record Can provide private key pwd upon successful MFA response/approval Decision record state transitions to “Completed” and “Signed by” set to user. Cannot delete or update signed record (and decision record that invoked the signature request). Update prevention includes associations configured in the digital signature type configuration record (schema for payload) Instead of edit, must create a new decision record to revert state (optional) – email confirmation of digitally signed record includes link to signed record Data Architecture Digital Signature Type Configuration Example (in Dataverse) Table: FINTRAC_riskassessment Columns: FINTRAC_contactid, fristname, lastname, email, effectivedate, … {all} Relationships: FINTRAC_contact, FINTRAC_organization Related Decision Types: Approved, Rejected, … Blob Container Address: {immutable_storage_connection_string} JSON Schema: { Auto generated on save based on values } JSON Payload example Unencrypted values saved in the digital record Encrypted values saved in the digital record Digital Signature Journey"
  },
  "_site/documentation/DLP/Dataverse-Tenant-Wide-DLP.html": {
    "href": "_site/documentation/DLP/Dataverse-Tenant-Wide-DLP.html",
    "title": "Configuration of Data Policies for Power Automate Flow Connectors | GOC Theme Documentation",
    "keywords": "Configuration of Data Policies for Power Automate Flow Connectors Download PDF Pre-Requisites User must have the Global Administrator Role assigned Purpose The PowerPlatform includes hundreds of first- and third-party connectors allowing developers and configurators to integrate with other services using PowerAutomate Flow and custom connectors (for custom API’s). You can adopt a zero-trust policy in your organization, and the PowerPlatform by leveraging the Data Policies feature. It is recommended to block all third-party connectors in your PowerPlatform subscription and only allow connectors via your change management process (ticketing system). The PowerPlatform not only provides the ability to block or allow connectors ‘tenant-wide’ but also allow your organization’s client (or specific environments) seek exemptions for their use case. Configure Tenant Wide DLP Navigate to the PowerPlatform Admin Centre and select Policies from the side menu and press “New Policy” (directly link) Set the policy name to “Global (Tenant)” (or similar). IN the Prebuild connectors menu, select the “nonbusiness” tab and filter the publisher column to “Microsoft” and press Apply. For a more restrictive list, see Annex A. Once filtered, select all and press “Move to Business” Next press “Clear all filters”. Select all connectors and press “Block” (this will block all third-party connectors). Press next, leave Custom connectors set values as default, press next and make sure that Add all environments is checked Press “Create Policy” *SECTION TO BE COMPLETED IN JUNE 2023* Configure Environment Specific DLP You can configure DLP exemptions by creating a new policy that allows a connector(s). (TODO) Configure Tenant Isolation (Preview) *Should not be implemented in a production environment (until late 2022 or when in general availability) Configure Lockbox Policy (Preview) *Should not be implemented in a production environment (until late 2022 or when in general availability) Configure Customer Lockbox Policy (Preview) *Should not be implemented in a production environment (until late 2022 or when in general availability) Annex A: DLP Minimum Business Connectors If you’re organization would like to restrict everything including Microsoft connectors, the following connectors could be part of the Business group while everything else is blocked by default. These will allow your organization to build and maintain integrations safely without losing key features the platform offers. Connector Justification Approvals Required for CoE Kit HTTP Required for CoE Kit HTTP with Azure AD Required for CoE Kit Microsoft Dataverse Required for CoE Kit Microsoft Dataverse (legacy) Required for CoE Kit Microsoft Teams Required for CoE Kit Office 365 Groups Required for CoE Kit Office 365 Outlook Required for CoE Kit Office 365 Users Required for CoE Kit Power Apps for Admins Required for CoE Kit Power Apps for Makers Required for CoE Kit Power Automate for Admins Required for CoE Kit Power Automate Management Required for CoE Kit Power Platform for Admins Required for CoE Kit RSS Required for CoE Kit SharePoint Required for CoE Kit OneDrive for Business Required for transitory docs (if required in future requirement) – restricted to tenant only (by default)) OneDrive Required for transitory docs (if required in future requirement) – restricted to tenant hosted one drive only (by default)) SQL Server Dynamics 365 Sales Insights PowerBI / General Reporting & Cross environment connections Dynamics 365 Customer Insights PowerBI / General Reporting & Cross environment connections Power Query Dataflows PowerBI / General Reporting & Cross environment connections Fin and Ops Apps (Dynamics 365) PowerBI / General Reporting & Cross environment connections Excel Reporting and Data administration Excel Online (OneDrive) Reporting and Data administration Excel Online (Business) Reporting and Data administration Notifications Teams notifications (for CRM/Teams integration) Form Recognizer Useful for hand filed forms (to support accessibility, paper based processes, and advanced scanning + printing official hard copy records) Microsoft 365 Compliance SIEM, Monitoring Microsoft 365 Apps Health SIEM, Monitoring Dynamics 365 Customer Voice Customer Voice (calling, contact us, support) Microsoft Graph Security General API usage for Azure PowerApps Notifications Used for configuring notifications within the PowerApps applications PowerApps Notifications V2 Used for configuring notifications within the PowerApps applications (additional features) Power Virtual Agents Client engagement (direct messaging & voice) Service Bus Used for messaging queues PowerBI Reporting Azure KeyVault Protection of certs and keys for Dataverse apps & ALM (DevOps) Azure AD Authorization / RBAC Azure AD Identity Protection Authorization / RBAC integration (e.g. AAD groups integration with D365) Azure App Service Used for hosting static sites and apps to support requirements Azure Blob Storage Leveraged for diagnostics, web file hosting, and system attachments Azure Log Analytics SIEM/Health Monitoring"
  },
  "_site/documentation/Email-Configuration/Email-Integration.html": {
    "href": "_site/documentation/Email-Configuration/Email-Integration.html",
    "title": "Email - Configuration Guide | GOC Theme Documentation",
    "keywords": "Email - Configuration Guide Download PDF The file managemement / attachments in this implementation is primarily managed via the out of the box integration between Dynamics 365 and SharePoint. This feature is abstracted from staff and external users. The benefits of using SharePoint to store documents from the Dataverse rather than storing documents directly in the Dataverse includes: Storage (File storage costs lowered) SharePoint is built as a mature IM repository that goes beyond simple file storage. Users who are not CRM users (authorized users) can access documents from clients, without using CRM. The integration is seamless, CRM users view all client attachments directly in the same CRM UI / form that they interact with when working on the case files. Pre-Requisites Dataverse environment administrator including user with PowerPlatform Administrator rights and System Administrator security role assigned / access to the environment for which positional mailboxes are being configured. Global Administrator, Exchange Administrator role of the M365 subscription hosting the organization email services (Exchange Online). Mailboxes to Configure *For the email address domain – for development and test, when creating the shared mailbox, choose a domain that is available in cloud. *UPDATE WITH ALL THE EMAILS* Display Name Email Address Alias Permissions No-reply-d365-test No-reply-d365-test@{domain.ca} No-reply-d365-test Group: Power Platform Admins No-reply-d365-uat No-reply-d365-uat@{domain.ca} No-reply-d365-uat Group: Power Platform Admins Support-centre-test Support-centre-test@{domain.ca} Support-centre-test Group: Power Platform Admins Support-centre-uat Support-centre-uat@{domain.ca} Support-centre-uat Group: Power Platform Admins Purpose The PowerPlatform includes an “out of the box” integration with email as a feature for Dynamics 365 and Model Driven Apps (both considered CRM apps) within any Dataverse environment hosted in the organization’s PowerPlatform subscription. This feature allows CRM users to administer email directly within the CRM app (sending, receiving, managing calendar events etc.). Since email is integrated within the platform it can work directly with its automation features such as (but not limited to) workflows, actions, plugins and Microsoft Flows. This feature allows integrating individual mailboxes (user mailboxes) within the apps so that users can track and manage their email within the platform by associating these to records they are working on AND / OR manage mail from shared mailboxes (e.g. Positional Mailboxes) for use cases such as support centres, help desks, line of business email (e.g teams) and any other use case that warrants the usage of a shared mailbox. This guide outlines the steps required to integrate shared mailboxes in CRM and integrating these to Queues for both 2 way email and “no-reply” mailboxes (for outbound notifications). Shared Mailbox Configuration Go to the Exchange Admin Centre and click on “Add a share mailbox” and enter the following details. Press Save once completed. Next, add members to the mailbox. These members should include the Dynamics 365 App Administrator(s) who will be synchronizing the mailbox in the Dynamics 365 App. *Skip the next steps for positional mailboxes that are not for “no-reply” or outbound only mail” Once added, block all incoming mail by selecting the Mail flow menu item and clicking on rules. Press the plus sign and “create a new rule” Name the new rule e.g. No-reply blocking. In the Apply this rule if list select The recipient is. Provide a name, select the “No Reply D365 Dev” mailbox just created under the “Apply this rule if.” Drop down menu. Select “Delete the message without notifying anyone” under the “Do the following…” drop down menu and press Save. Enabling Email Integration in Dynamics 365 (Queue Mailboxes) Pre-Requisite Dataverse environment administrator including user with PowerPlatform Administrator rights and System Administrator security role assigned / access to the environment for which positional mailboxes are being configured. Global Administrator, Exchange Administrator role of the M365 subscription hosting the organization email services (Exchange Online). Navigate to the Dynamics 365 Advanced Settings and select Email Configuration. Go to https://make.powerapps.com, select the environment that requires email integration and press the gear icon and then “Advanced Settings”. In the email configuration menu select “Settings” and uncheck “Process emails only for approved queues” * This will allow Dynamics 365 System Administrators to synchronize mailboxes independently. If you’re implementation limits the usage of email to a few positional mailboxes, configuration of positional mailboxes from Global Administrators and/or Exchange Administrators can be incorporated within the Cloud team’s operational support (SLA) process. Alternatively, and in scenarios where your organization will extensively make use of the email integration feature by synchronizing a large number of positional mailboxes across multiple environments, individuals within the organization who have been designated as “PowerPlatform Administrators” can be assigned the Exchange Administrator Role in Azure via Privileged Identity Management (PIM) as an eligible assignment that requires “Approval” so that the workload can be effectively and securely distributed among multiple members of your organization. Once completed, alert the Dynamics 365 Administrator that they can synchronize the positional mailboxes."
  },
  "_site/documentation/GOC-Theme-Configuration/overview.html": {
    "href": "_site/documentation/GOC-Theme-Configuration/overview.html",
    "title": "Configuration of the Enterprise Portal Theme (Power Platform) | GOC Theme Documentation",
    "keywords": "Configuration of the Enterprise Portal Theme (Power Platform) Download PDF Pre-Requisites PowerApps/B2C App Registration: You will need to request a ClientID and Secret to {POWER PLATFORM ADMINISTRATOR MAILBOX TBD – CANVAS APP – COE KIT} PowerApps Theme Deployment: Request a ClientID and Secret to Download the latest version of the GAC PowerApps Theme (WET) System Administrator security role assigned to App Registration User in New Dynamics/PowerApps instance requiring the portal. This user will be Azure PIM roles activated: Active Assignments of the following roles applied to user account creating the new Dataverse environment hosting the newly created Portal: PowerPlatform Administrator, Dynamics 365 Administrator, Application Administrator. To test the theme go to: https://goc-theme-release.powerappsportals.com PowerApps Portal Theme Deployment Navigate to the https://admin.powerplatform.microsoft.com and In the environments menu press “New Environment”. Fill in a title (format should be EN(Acronym)-FR(Acronym)-Dev(Environment Type), description and ensure to select custom URL to ensure that the subdomain is not random and that you’ve selected English and $CAD as the default language and currency respectively. Since the {ORGANIZATION} Portal Theme relies on the Dynamics 365 Customer Service App to be installed, ensure to check “Enable Dynamics Apps” and select customer service pro. *Optional (but recommended) * - {ORGANIZATION} should assign an Azure AD security group to govern access to any environment. This can be applied later, however if one is provided beforehand, make sure to set the Security group in this wizard. This environment provisioning can take up to 30 minutes. Once the environment has been provisioned, navigate to the https://make.powerapps.com and select the newly created environment. In the side panel, select “Create” and once the application library is rendered, select “Customer self-service”. The follow the convention, it is recommended that the portal title and subdomain matches the title of the environment. The portal provisioning process can take up to 20 minutes. Once provisioned, navigate to the portal application to ensure its running Delete the newly created portal application in PowerApps as this will be replaced with the Enterprise Theme. The portal installation is required to ensure the environment has the necessary solutions and web application configured. Go to the Portal Management Model Driven App and Delete the website record Navigate to the Dynamics 365 System Settings and temporarily remove file attachment and file size (adjust) restrictions. This ensures that the theme’s JavaScript files can be uploaded and not blocked by the API. Make sure to copy the file restrictions to your notepad or elsewhere as once the theme has been uploaded, you will need to add these restrictions back. Next install the French Language Pack by going back to the System’s administration settings and selecting Languages, and then French (1036) Deploying the theme To deploy the {ORGANIZATION} Enterprise Theme, download the portal CLI extension in Visual Studio Code. Once downloaded, create a new project in an empty directory on your computer. Next, once the empty project is opened in your IDE, connect and download the {ORGANIZATION} Portal Theme – Release environment by using the commands below. Before running these commands, download the portal from the following GIT repository: https://github.com/Cloudstrucc/PowerApps-WET-Canada (e.g download the files, or clone the repository) - pac auth create --url \"https://{ORGANIZATION}-portal-theme-dev.crm3.dynamics.com\" --applicationId \"cbe003cd-ecfe-4324-84cb\" --tenant \"b644-288b930b912b\" --clientSecret \"tzm7Q\\~ \" **\\<-Connect to theme environment** - pac paportal upload -p .\\\\customer-self-service\\\\ **\\<-Folder of downloaded portal from GITHUB** Once the upload is completed, return to make.powerapps.com, select your environment, and create a new portal application and make sure to check “use existing website record” and select “customer-self-service”. Allow the Dataverse to finalize the deployment for 10 minutes. Once the 10 minutes has elapsed, you should be able to navigate to the portal and the Canada.ca theme will render. Convert the Portal from Trial to Production. Go the portals admin console and in the Portal details menu select Convert and confirm. Before doing so locate the App Registration user in the Azure Portal and yourself (and other administrators of this environment) as Owner. Note you will need to have the Application Administrator Azure Role in your active assignments. Validate you are working with the correct App Registration user by navigating to the Authentication blade and inspecting the Redirect URLs to ensure it matches your newly created environment. Add the PowerPlatform-CICD App Registration user to the newly created environment. Go to the environment in the PowerPlatform admin centre and select Users on the right and follow the rest of the steps below. Restart the portal If the portal is still rendering the original theme, you can verify and set the binding to point to the theme’s binding. Once uploaded, go to portal’s admin centre, and restart. Once restarted, the portal will should render the enterprise theme. Now that the theme is installed, edit the authentication setting entitled “Enterprise SSO” and enter in the ClientID, Secret, and the redirect URL’s domains to match the newly created portal’s domain and restart the portal. *NOTE you will need to send your portal domain to the Azure B2C administrator who will provide you with a ClientID and Secret to replace. (Optional Step) Set the IP Restrictions to the Portal to only allow network flows from the {ORGANIZATION} Network (VPN or Direct). The IP(s) must be in CIDR notation. You can reference the “{ORGANIZATION}-portal-theme-dev” deployment to obtain the IP restriction list. Azure B2C – Configuring a new Client. As the Azure B2C ({ORGANIZATION}-gccf-dev domain) Administrator, navigate to the Azure B2C tenant and follow the steps below to register an application to leverage the enterprise SSO. Once completed, provide the ClientID to the PowerApps Portals administrator to set the value in the PowerApps Portals site settings. You will need to receive the portal domain from the PowerApps administrator before completing the steps below. Provide the Application (client) ID to the PowerApps administrator Training process Created by Frederick Pearson on 8/18/2023 Edit original on dubble 1. Go to Home · Cloudstrucc inc. Open the model driven app 2. Click on All Services 3. Click on Training"
  },
  "_site/documentation/index.html": {
    "href": "_site/documentation/index.html",
    "title": "Government of Canada Power Platform Theme Documentation | GOC Theme Documentation",
    "keywords": "Government of Canada Power Platform Theme Documentation Welcome to the {ORGANIZATION} Documentation website. You will be able to browse through all of our product and services documentation. For organization specific documentation, you will need to be a registered user. Tip To become a registered user contact (TBD) - for other inquiries please visit our website website's contact us page here Tip Our goal is to provide our customers with the most up to date and relevant documentation surrounding the various products and services we support. Despite our strong focus to using PAAS and SAAS services, which are already well documented online, it is often a challenge to find the most relevant information based on the specific implemementation details of what our company has deployed to your environment(s). Important The way to authenticate to access our private documentation related to your organization's implementation is through your Azure AD account that is federated with your on-premises active directory. Important Each customer has a dedicated site for documentation, the site is not hosted in a shared infrastructure model. Instead, each site has its own hosting boundary and network security group making it impossible for other users who are registered to this site to ever access another customer's documentation."
  },
  "_site/documentation/SharePoint/Combined.html": {
    "href": "_site/documentation/SharePoint/Combined.html",
    "title": "SharePoint - Configuration & Integration Guide (CRM & Portals) | GOC Theme Documentation",
    "keywords": "SharePoint - Configuration & Integration Guide (CRM & Portals) Download PDF Pre-Requisites Dataverse environment administrator including user with PowerPlatform Administrator rights and System Administrator security role assigned / access to the environment for which SharePoint integration is being configured. Global Administrator or SharePoint Administrator Creating a SharePoint Site to host Subsites that will integrate with Dataverse environments Navigate to the SharePoint Online Admin Centre and create a new Site. Ensure to set ownership rights to the PowerPlatform Service Administrator (not the Application layer / Dataverse environment administrator). This will allow the PowerPlatform Service Administrators to create additional subsites in the future for new Dataverse environments without intervention (optional). Go to https://admin.microsoft.com and ensure that all admin centers are rendred (click “show all” on the left menu) and press “SharePoint”. In the SharePoint admin centre, select Sites and Active Sites, then press “Create” and “Team Site”. Enter the required details for the Team Site and ensure to set additional site owners, press next and finish (assigned site owners can add members to the site in the future). Note the site can take up to 1 hour to fully provision. Once the site is created provide the Site URL to the PowerPlatform administrator (owner set for the PowerPlatform-Development site) – you can click on the link in the Site list Creating Subsites Once a site has been created, you or another Site owner will need to create a subsite(s) per Dataverse environment. To create a subsite, navigate to the Site Contents menu, press New and select Subsite. Enter the site name which should match the name of the Dataverse environment (suggested convention) and the site address. Ensure to leave the “use same permissions as parent site” radio button selected. Press Create. This process can take up to 30 minutes. Once created provide the new Subsite’s URL to the Dataverse application administrator to set up the integration with the Dataverse environment Purpose The Dataverse provides document storage out of the box using the Notes table. While this is suitable for small applications, it is often better to use an enterprise grade IM repository. Thankfully, the Dataverse provides the ability the integration to SharePoint Online sites natively. This guide provides the configuration steps to integration SharePoint Online Sites & Subsites to your Dataverse environment for both Model Driven and Portal Apps. SharePoint Online – Prepare Sites and Subsites for Integration with Dataverse Environments Preface The recommended architecture to implement SharePoint Online integration with the Dataverse is to use Subsites. By using subsites, your organization can manage all the security and access control policies at the site level and therefore, all subsites deployed within the site will inherit these policies. Security administrators can also opt to add and or override these policies at the subsite level for specific environments with unique requirements. Additionally, it is recommended that a site is created for non productionized environments and another for production environments for those whose security requirements allow for shared infrastructure. For production environments that require a dedicated site the same steps can be followed but instead of creating a subsite, the production Dataverse environment integrating with the dedicated site will use the site URL to integrate rather than a subsite. Model Driven Apps (Dynamics 365 Apps) SharePoint Integration To complete the steps below you will need to be an Owner of the SharePoint subsite created in the previous step and System Administrator access to the Dataverse environment that requires the SharePoint integration Steps to configure SharePoint Integration Navigate to https://make.powerapps.com and select the environment where SharePoint integration will be configured. Once selected, click on the Gear icon and “Advanced Settings” In the advanced settings navigate to document management. In the Document Management Settings, click on “Configure Server-Based SharePoint Integration” and in the modal press next, enter the SharePoint Subsite URL, press next again In the Validate Sites step, press finish, once validated. If you receive an error it is likely due to permission issues. Next, click on “Document Management Settings” and select the tables whose records allow document attachments to be stored in SharePoint, paste in the Subsite and press next. Select “based on entity”. This will ensure that each configured table will have a dedicated folder and within these folders, a folder for each record will be created that houses the attachments associated to the record. Press “Finish” once completed Now that the SharePoint Subsite is integration with this environment, you can test by navigating to any record from a table you’ve configured and you will notice a new relationship entitled “Documents”. You can test uploading a document and verifying that the document is found in the SharePoint Subsite PowerApps Portal – Configure SharePoint Integration Pre-requisites Global Administrator Role The Dataverse environment hosting the Portal must have the SharePoint Online Server Side Integration setting activated (see previous section) Environments to apply this setting (non-productionized environments) Environment URL Type https://portaladmin-can.portal-infra.dynamics.com/?tenantProductId=086e308f-b708-4ab1-835a-987bb121c21a&lcid=1033&geo=CAN Dev https://portaladmin-can.portal-infra.dynamics.com/?tenantProductId=32b2d9b8-e69b-4789-a22a-3f86feaa2de9&lcid=1033 Staging https://portaladmin-can.portal-infra.dynamics.com/?tenantProductId=4fcb5e8c-43f3-4760-b9bd-c813cc12b10d&lcid=1033 UAT https://portaladmin-can.portal-infra.dynamics.com/?tenantProductId=3f8697b3-f8b2-4917-99d0-eef6aef28a7d&lcid=1033&geo=CAN CUT Steps to configure the SharePoint Intergration for PowerApps PORTALS In the PowerApps Portals admin centre, navigate to “Set up SharePoint Integration” and click on “Enable SharePoint Integration” Select “Enable” in the confirmation window. This will enable the portal to communicate with SharePoint. While the SharePoint integration is being enabled, the portal restarts and will be unavailable for a few minutes. A message appears when SharePoint integration is enabled. You will be prompted to authenticate the request and attest an amendment performed to the Portals App Registration record. Once accepted the process will update the PowerApps Portal App registration record in the background by adding the following API Permissions. You can view these changes by navigating to “App Registrations” in the Azure Portal and searching for the app registration highlighted above. You can then navigate to the App Registration’s API Permissions menu blade to view the changes. The update will also include an additional “redirect URI” which is part of the OAUTH 2.0 specifications for authorization. This same redirect URI is registered in the SharePoint Site Configuration for OAUTH 2.0 API integration. *This registration record has no owners beyond the Portal provisioner identity and is only leveraged for machine-to-machine flows between Portals, Dynamics 365, and SharePoint Online (for environments whose portals require integration with SharePoint Online). Example integration of SharePoint upload feature in Portals To expose an upload documents control in the Portal that points to the SharePoint document library associated to a record, the first step is to add a basic form metadata attribute to a form you are exposing to the portal. The configuration is detailed below. Step 1: Create a table permission In this example, we are allowing only users who are primary contacts of an Account (Organization) to upload documents related to the organization. But as long as a table permission is created for the Document Location Table (Entity) the baseline security to allow the upload is configured. However it is recommended to scope the document management feature at the contact (user level) or organization level (e.g. for portals where you have multiple users belonging to the same organization and submitting documents on behalf of the entire organization rather than for their own application). This example demonstrates allowing uploads only by the user and associated to their user account only Step 2: Basic Form Metadata To render the document gird on a page or form, you must add a basic form metadata attribute of type subgrid. This subgrid must exist on the CRM form (and tab) that is being exposed to the portal When creating the metadata record, select “Subgrid” and PowerApps will allow you to select the subgrid to render next to the subgrid name field. This subgrid must existing on the CRM form and must point to the “Document Location” table. Once configured, the table should look like this (depending on your styling). Steps to configure the SharePoint Integration for PowerApps Portals In the PowerApps Portals admin centre, navigate to “Set up SharePoint Integration” and click on “Enable SharePoint Integration” Select “Enable” in the confirmation window. This will enable the portal to communicate with SharePoint. While the SharePoint integration is being enabled, the portal restarts and will be unavailable for a few minutes. A message appears when SharePoint integration is enabled. You will be prompted to authenticate the request and attest an amendment performed to the Portals App Registration record. Once accepted the process will update the PowerApps Portal App registration record in the background by adding the following API Permissions. You can view these changes by navigating to “App Registrations” in the Azure Portal and searching for the app registration highlighted above. You can then navigate to the App Registration’s API Permissions menu blade to view the changes. The update will also include an additional “redirect URI” which is part of the OAUTH 2.0 specifications for authorization. This same redirect URI is registered in the SharePoint Site Configuration for OAUTH 2.0 API integration. *This registration record has no owners beyond the Portal provisioner identity and is only leveraged for machine-to-machine flows between Portals, Dynamics 365, and SharePoint Online (for environments whose portals require integration with SharePoint Online). Example integration of SharePoint upload feature in Portals To expose an upload documents control in the Portal that points to the SharePoint document library associated to a record, the first step is to add a basic form metadata attribute to a form you are exposing to the portal. The configuration is detailed below. Step 1: Create a table permission In this example, we are allowing only users who are primary contacts of an Account (Organization) to upload documents related to the organization. But as long as a table permission is created for the Document Location Table (Entity) the baseline security to allow the upload is configured. However it is recommended to scope the document management feature at the contact (user level) or organization level (e.g. for portals where you have multiple users belonging to the same organization and submitting documents on behalf of the entire organization rather than for their own application). This example demonstrates allowing uploads only by the user and associated to their user account only Step 2: Basic Form Metadata To render the document gird on a page or form, you must add a basic form metadata attribute of type subgrid. This subgrid must exist on the CRM form (and tab) that is being exposed to the portal When creating the metadata record, select “Subgrid” and PowerApps will allow you to select the subgrid to render next to the subgrid name field. This subgrid must existing on the CRM form and must point to the “Document Location” table. Once configured, the table should look like this (depending on your styling)."
  },
  "_site/documentation/System Design Guide/SDD.html": {
    "href": "_site/documentation/System Design Guide/SDD.html",
    "title": "Introduction | GOC Theme Documentation",
    "keywords": "Introduction Download PDF The introduction section of the solution architecture document provides an overview of the purpose and scope of the document, which is to describe the architecture of the CRM case management system and its components. It also covers the technologies and platforms involved in the implementation. Purpose and scope of the document Overview of the CRM case management system and its components Overview of the technologies and platforms involved. Purpose and scope of the document The scope of this document focuses on the Dynamics 365 Customer Service implementation in the organizations Power Platform subscription which is part of the M365 product family and is considered as a “SAAS” technology. Dynamics 365 CS is an enterprise CRM platform that the organization plans on leveraging to modernize the digital relationship with the “reporting entities” which are financial institutions across Canada that are responsible to report financial data and through various forms and mediums to {ORGANIZATION} for compliance officers to analyze to ensure compliance with the financial laws in Canada in this sector. This technology also ships with a Portal technology that integrates natively with Dynamics within the Power Platform which allows Contacts (external portal users) to submit data in a more efficient and secure manner directly to {ORGANIZATION}. Officers typically will generate reporting cycles by sector and invite all financial institutions within that sector to fill in compliance forms and attach supporting documents. This document demonstrates the implementation of this technology to meet the use this case. Moreover, {ORGANIZATION} has an API driven architecture whereby the platform can interface with read data thus aligning with the organization’s overall digital strategy and cloud adoption. By leveraging this technology, particular a SAAS technology that has been assessed by CSE/SSC/TBS as Protected B ready, the complexities associated with administering a custom implementation or infrastructure is now abstracted and thus simplifies the implementation and assures a higher level or security due to the fact that operators cannot interfere with the OS or IAAS and encryption is handled end to end both in transit and at rest using AES and RSA and falls within the realm of the organizations Active Directory policies thus ensures that only {ORGANIZATION} employees on approved devices with Federal Government CA authorization can access the platform. Moreover, the portal is also administered by Active Directory service principles only accessible by privileged user roles (app admin, GA) and external users (Contacts (external portal users)) must be invited formerly to the portal and are forced to use 2FA to access the portal. Overview of the Dataverse environments (CRM case management system) and its components Before diving into the specific use case implemented using the Dynamics 365 Customer Service Module with Portals, the section will first describe what each of these technologies are to provide context and explain how the compliance management process will fit into or has been built leveraging these technologies: Dynamics 365 Customer Service Dynamics 365 Customer Service is a customer relationship management (CRM) software application that enables businesses to manage and streamline their customer service operations. The platform offers a wide range of features, including case management, SLA management, business process flows, automation, and reporting capabilities. Case management is a critical aspect of the Dynamics 365 Customer Service platform. It allows customer service agents to efficiently manage and resolve customer issues by creating cases, tracking case progress, and escalating cases as needed. The platform also includes robust SLA management features, which allow businesses to establish service level agreements with customers and track performance against those agreements. Additionally, the platform offers extensive automation capabilities, including the ability to automate routine tasks, such as email responses and case routing. Another strength of Dynamics 365 Customer Service is its business process flows feature, which enables businesses to automate and standardize their customer service processes. With this feature, businesses can create predefined workflows that guide agents through each step of the customer service process. The platform also offers robust reporting capabilities, allowing businesses to track key performance indicators (KPIs) and gain insights into customer service operations. Overall, Dynamics 365 Customer Service is a powerful tool that can help businesses improve their customer service operations and streamline their workflows. This technology aligns well with the compliance process at {ORGANIZATION} whereby employees are responsible to generate reporting cycles by sector which automatically trigger an invitation process and notifications to Contacts (external portal users) within that sector to fill in the data of the compliance form associated with the cycle, for example the risk questionnaire type using out of the box case management feature. Furthermore, the SLA feature is being leveraged to track the progress and timeline obligations that Contacts (external portal users) are responsible for submitting the information to {ORGANIZATION} within a given (configurable timeframe). Furthermore, the auditing feature allows {ORGANIZATION} to report and examine all activity on the case and audit logs are immutable. The platform’s business process flow and out of the box state machine (statuses) are being leveraged to track where in the process a compliance form is. For example, the process starts in draft until the employee runs a the workflow (flow) to activate the cycle which triggers the invitation process and generates all child compliance case per RE which also has its own state machine. Once this is successfully triggered the status is automatically set to in progress and only transitions to under review once all submissions have been received and triaged for accuracy. The state machine is further described in the application layer implementation details. Throughout this process however, Contacts (external portal users) and {ORGANIZATION} employees can collaborate digitally over email or virtual agent feature and all correspondence is automatically linked as an activity associated to the case. Furthermore, {ORGANIZATION} employees can return submissions to the Contacts (external portal users) to request more information and extend the timeline for submission when warranted. Most of these features come with the platform’s tooling with additional development to account for specific data elements and configuration of state machine and business process flows and rules build by the CRM developer. Power Pages Sites Power Pages Sites (customer self-service type) is a low-code, self-service portal technology that integrates natively with a Dataverse environment licensed with customer service license. The platform allows {ORGANIZATION} to provide their Contacts (external portal users) with a self-service portal that enables them to access information and perform tasks such as submitting and tracking cases, updating their profile information, and accessing other data such as attachments and notifications. One of the strengths of Power Pages is its SAAS nature, which means that businesses do not have to worry about managing infrastructure or performance. The platform is hosted on Microsoft Azure and provides automatic scaling and failover capabilities, ensuring high availability and reliability. Portals also offers native integration with Azure B2C, which enables businesses to provide single sign-on (SSO) with 2FA to external users. This integration provides a secure and seamless authentication experience for users accessing the portal and aligns with the organizations current architecture to provide SSO and API authorization to Contacts (external portal users). Another key feature of Portals is its RBAC (Role-Based Access Control) capabilities, which enable businesses to govern access and CRUD (Create, Read, Update, Delete) operations to tables and columns in the Dataverse. The platform provides granular control over access permissions using table permissions and column permissions coupled with web roles. Power Pages Sites also offers advanced features such as rendering CRM forms using advanced forms and rendering lists using existing views. This allows businesses to customize the user experience and provide a seamless integration with their existing Dataverse environment. Finally, the invite-only feature of Power Pages Sites provides an added layer of security, ensuring that only invited users can access the portal. Overall, Power Pages Sites is a powerful tool that can help businesses improve their customer service operations by providing a secure and customizable self-service portal for their customers. Contacts (external portal users) will leverage this portal to note only submit data and documents to {ORGANIZATION} but also seek support via the omnichannel modules (chat/virtual agents/voice) and the portal has been adapted to meet the WET / WCAG 2.0 theme and compliance that is a commitment to Canadian citizens to ensure a seamless experience to any user regardless of vision impairment or other ailments making it difficult to use a web-based tool. {ORGANIZATION} has incorporated this theme in the platform and is responsible to maintain it by issuing new releases as the theme is modernized year over year. Moreover, by providing 2FA and presenting users with transparent consent on what {ORGANIZATION} is collecting and its obligations to protect the data and handle data retention and disposition rules to meet the GOC’s Protected BMM posture, users are informed each time they sign on of the terms and conditions and what to expect when interacting with the portal. The illustration below provides the look and feel and SSO with consent into the platform (subject to change, and this is development). The purpose is to demonstrate the integration the WET theme and the SSO service integration with the PBMM consent screen: Figure 1: Home Page Figure 2: Azure B2C SSO Login Pages Figure 3: Terms and Conditions (PB) Consent Page Figure 4: Authenticated User Landing Page The Power Pages Site setting to force the PBMM consent on login is illustrated in the table below. System Architecture This section provides a high-level diagram of the system architecture and the components involved in the CRM case management system, which include Dynamics 365 Customer Professional App, Power Pages Site, SharePoint Online, Exchange Online, and Azure B2C. The section provides a detailed description of each component, including its architecture and data model, customizations and extensions, and integration with other components. Additionally, it discusses how the components integrate with each other, including data flow and synchronization, security and authentication requirements, and integration patterns and best practice. The system is implemented in the {ORGANIZATION} Power Platform subscription which is a SAAS residing in the organizations M365 subscription. The Power Platform has been configured with guardrails to adhere to IT standards which would allow the ability for {ORGANIZATION} to store and interact with Protected B data. These guardrails protect and govern this implementation and are further described in the Power Platform guardrails chapter of this chapter. However, the full platform guardrail implementation documentation is separate from this document. However, the reader should take into account that this implement operates within the confines of these guardrails. Similarly, this system operates the confines of the Azure B2C PB guardrails and the Azure and M365 baseline guardrails mandated to Federal Government Department and Agencies implementing PB workloads outside the GOC owned network devices hosted in Government owned and or operating datacenters. In this implementation our solution is comprised exclusively of Microsoft owned network assets and SAAS and PAAS offerings which is a deliberate decision to further secure this application by relying on a trusted partner whose datacenters meet and exceed the GOC standards for physical and network security and has been fully assessed by our intelligence agencies and internal IT experts. These physical and network assets are subject to random audits by impartial parties to validate adherence to the GOC’s ISO based set of network and hosted network physical requirements such as multiple checkpoints, specific security clearances/screening of staff, and safely purging of stale physical network assets such as any device hosting data such as SDD/HDD/RAM etc. Another important factor for cloud security is the integration of {ORGANIZATION}’s Active Directory to Contacts (external portal users) Active Directory using AD Connect / ADFS / WAP to ensure that our internal user data is owned by our hardware thus our conditional access policies such as the requirement to access any of the elements in our implementation must be done by an operator who’s physically accessing our network using VPN and authorize to our on premise hosted Entrust Certificate Authority. This means that Microsoft has no ability to compromise access to our systems without the same requirement. Moreover, MS cannot access our Global Admin credentials and thus cannot restore this account which is the reason why we have multiple G.As and break glass accounts. Finally, the GOC has implemented the Express Route which is a physical link between our ISP’s to the Azure Datacenters, thus extending our existing data-centers to Azure giving us more control over who and what data can traverse through between both organizations. Finally, {ORGANIZATION} is also implementing CMK and an HSM to own the encryption keys. This will also apply to the Power Platform, however {ORGANIZATION}, as of 2023, does not mean the minimum requirements (1k licenses) to activate this feature. Once the feature is available to the organization, a migration will be required however will be vital to elevate the data confidentiality and integrity guardrails by guaranteeing that even if MS is compromised the nefarious actor (even with access to the MS CA infrastructure) will not be able to decrypt the data. High-level diagram of the system architecture The target state architecture aligns with {ORGANIZATION}’s perimeter services which is comprised of using Azure Gateway as a proxy to both Azure B2C and the Power Pages Site. Figure 5: Software Architecture Target State The current state architecture is leveraging the Microsoft default SAAS security perimeter services which is abstracted from {ORGANIZATION} and fully under the control of Microsoft’s security team. The TBS cloud usage profile allow this for PBMM because SAAS infrastructure is not accessible by GOC employees thus is less vulnerable to security threats or failure to patch security vulnerabilities quicker than the teams at Microsoft administering the platform. {ORGANIZATION} however can control the perimeter services for both B2C and Power Pages Sites by using Azure Gateway or Azure Front Door and use this to also configure a custom domain for both services. Figure 6: Software Architecture Current State Technical Architecture Dynamics 365 Customer Professional & Enterprise This is a series of model driven applications that are installed in each Dataverse environment that include additional features that come with this license including case management, service level agreements, customer (client) insights, omnichannel for agent (messaging and voice), additional capacity per user (.75gb per licensed user), and additional Power Automate Flow throughput. The case (incident) is the pillar of the compliance application as every type of compliance form is managed via the case table. Each type of case (which is the case subject tree) has a dedicated custom table that holds the data that clients will submit via the Power Pages Site and is linked as a N:N relationship to the case. Compliance staff will create reporting cycles using the case management feature, and select a subject such as Risk Questionnaire, and select a selector or manually choose which reporting entity(ies) the cycle is scoped for. By doing so invitations to complete the form are sent to each reporting entity associated with the cycle (the primary contact of the RE) and the SLA is activated. The SLA feature is an OOB feature available through the CS module and provides the ability for system administrators and customizations to configure date driven rules to communicate to both internal and external stakeholders’ deadlines for various actions such as submitting a compliance form, reviewing a submitting, deciding on a form’s process to transition to a new state (such as complete) and actioning incident (tickets for support). The feature provides visual cues such as green, yellow, and green dots next to records in the Model Driven App views and in portal lists shown to external users for transparency. The feature is also useful to create automated events such as sending email reminders and report on adherence to service standards providing {ORGANIZATION} with aggregate metrics to examine its own SLA standards vis a vis its internal capabilities. This feature implementation for the compliance management system will be illustrated in this document. Figure 7: SLA's Power Pages Site The Power Pages Site technology is internet facing and is on the powerappsportals.com domain by default. The security perimeter is maintained by Microsoft just like all other SAAS services and non productionized sites (e.g. dev, test, uat) are internal to {ORGANIZATION} employees only and not accessible by external users who do not belong or exist in the 139gc domain. Therefore, just like teams, and other M365 SAAS products, users who are developing and or testing the portal must be on a {ORGANIZATION} device and authorized to Active directory and entrust CA to access the portal. For the production portal, the settings of privatization are turned off and the site is made “public” and which point the anonymous page (home page) of the portal is accessible via the internet however nobody who has not been invited to use the portal and is within the Azure B2C domain can access the authenticated user pages. This app type is provisioned within the same Dataverse environment as the D365 CS app and it’s a public facing website that is configured for invitation only. Meaning, open registration by external users to the portal is prohibited. An RE must first exist in the Azure B2C tenant, and then be sent an invitation from the D365 CS application that includes a link with embedded invitation code to redeem. Once in the portal the RE primary contact and start filling in the form and assign the form (optionally) to an authorized agent, which are contacts in the CRM that are associated to the Primary Contact’s organization (the financial institution). However, only the Primary contact has permission to submit the form to {ORGANIZATION} for review. Once submitted, the user is notified that the form is no longer editable and that a {ORGANIZATION} officer will be reviewing the submission. If the officer determines that the form is incomplete or requires clarification or more information, they have the ability to change the state of the Compliance case to “Draft” and send a “notification” activity to the primary contact, which is done via a Power Automate Flow, at which point the Primary Contact receives a generic email instructing them to log into the portal and review the notification activity where more instructions are provided by the {ORGANIZATION} employee and a direct link to the form which is now editable again for submission. This process can repeat itself until the {ORGANIZATION} employee deems the form complete and transitions the state of the compliance case from under review for ready for approval. SharePoint Online This technology is the official IM repository for the entire system. Each Dataverse environment from dev through prod is linked to a SharePoint subsite that lives within a site. {ORGANIZATION} has provisioned 2 SharePoint Sites, 1 for non-production and 1 for production each hosting x number of “subsites” that point to 1 environment. For example, dev is integrated with the dev subsite in the ftnc-compliance-np site, and pre-prod and prod both have their own subsite hosted within the production SharePoint compliance site. The environment lists all the subsites associated with each environment. This setting is managed the advanced settings or admin console for Power Platform (admin.powerplatform.onmicrosoft.com) where a site (subsite) is synched with a Dataverse environment and each table that is configured to accept attachments will automatically create a folder within the subsite whereby each record such as a case will have its own folder holding all associated attachments. This is configured by the D365 System Administrator role. Because this app supports Power Pages Site attachments to SharePoint, the G.A must provide consent to allow the integration of the site to the integrated SharePoint subsite to the Site’s integrated SharePoint subsite environment. The table below lists every table that attachments are allowed: Table Name Incident (case) Risk Questionnaire Account (Organizations) Contact User Notification .. .. Table 2: Tables integrated with SharePoint. SharePoint Sites with X Number of Subsites Each Mapped to Non-Production Environments (e.g., Dev, Test, Staging, UAT, Sandbox) and another site that hosts the production environment. In this implementation, there are 2 SharePoint sites created – one for non-production environment integration and 1 dedicated for production environments. The table below lists each site, subsite and environment URL mapping. Subsite Name/URL Type CRM environment Table 3: SharePoint Subsite & CRM Environment Mapping The illustration below depicts the folder structure in SharePoint for each environment’s subsite. Figure 8: SharePoint - Native Integration The integration between SharePoint Online and CRM is a native configuration as both systems live within the same platform, M365. However, to configure the SharePoint integration between Power Pages Sites and SharePoint requires a Global Administrator to activate this setting and provide consent. This action will generate an API permission in the existing App Registration record that is leveraged to integrate the Portal with CRM. The illustration below depicts the new API permission created by this process. Figure 9: Power Pages Site Admin Portal - SharePoint Integration Activation Feature Figure 10: SharePoint API Permissions Automatically Added by Activating the Portal SharePoint Integration Feature Activating the SharePoint Online integration between the Portal and CRM requires additional setting such as Table Permissions and basic form metadata to expose the SharePoint subgrid allowing external users to be able to upload documents. These settings are provided in the table below. Table 4: SharePoint Table Permission Settings (review) Exchange Online The compliance case management system leverages the D365 email integration feature with Exchange online by integrating 2 cloud native Shared Mailboxes with the platform. This is configured using the email server-side sync which automatically integrates with the 139gc exchange online environment and must be turned on by an exchange administrator or G.A. However, for non-production, the system wide settings to allow D365 system administrators to synchronize mailboxes without intervention of the former privileged roles are turned off. Once server-sync is activated, the mailboxes are synchronized for each environment. Below is the list of emails synchronized by environment. To activate this feature, a system administrator must go to the email settings or system wide settings and configure email “server-side sync” and choose Server-Side Sync Exchange for both inbound and outbound mail. Appointments and tasks can also be synched from outlook. However, this implementation is designed to use CRM exclusively to administer email and not synchronize staff mailbox, which in this case integration using the outlook plugin would be useful. Each mailbox is first configured in exchange as a “Shared Mailbox” Type with delegation privileges to the Power Platform-administrator group who are responsible to synchronize them to each environment. These mailboxes are of type “Cloud” thus there is no requirement for this phase to synchronize on premise exchange mailboxes (e.g., Hybrid Exchange). Finally, its important to state that a mailbox can by synched to one environment a time therefore we’ve created an outbound only and in and outbound mailbox for each environment for development and testing. Mailbox Environment Table 5: Exchange Shared Mailboxes Synched in each D365 environment. The illustration below depicts the exchange email server-side sync architecture – note for this implementation, the synchronization only has been configured for the CRM organization – the outlook app is not being leveraged nor required as all email is administered in the model driven app. Figure 11: Email -server-side sync The use case for emails is listed below. These are subject to change. Secondly, this system will never, or is prohibited to send Protected B content in the emails being sent. Thus, this implementation includes a “Notification Centre” module which integrates with email, whereby an email to the person notified who is responsible to log into the portal and navigate to the notification center to view the details of the notifications. The table below demonstrates the data model for the notification center. The feature enables administrators to configure custom templates using a rich text editor and use workflows and flows to send notifications using both the do not reply email queue mailbox OR the help center email queue. When a user receives a notification, an unread badge (visual cue) is displayed, and the user can choose when to mark the notification as “read” by pressing the “Mark as Read” notification which updates the status reason of the notification item record. Figure 12: Notification feature ERD Figure 13: Example unread notification Internal users can manually issue notifications in the model driven app. The process is illustrated below. Most notifications however are automated via processes and or Power Automate Flows. Azure B2C Azure AD B2C (Business to Customer) is a cloud-based identity and access management solution provided by Microsoft. It enables businesses to manage customer identities and authentication in a secure, scalable, and cost-effective manner. With Azure B2C, businesses can offer their customers a seamless, personalized experience across multiple applications and platforms. Azure B2C provides features such as social identity integration, multi-factor authentication, self-service password reset, and more. It also supports industry-standard protocols such as OAuth 2.0 and OpenID Connect. Power Pages Sites can be integrating natively with Azure B2C using app registrations. App registrations are created for each site to integrate with Azure B2C using the OIDC setting out of the box and therefore each environment is provided a client ID, Secret, and the user flow metadata (OIDC metadata) from B2C NP, and for production the B2C production tenant. B2C is an active directory tenant separate to 139gc but linked to a pay as you go subscription, the non-prod and prod subscriptions at {ORGANIZATION}. The table below lists all the client IDs by environment. The Client IDs are unique identifiers to App Registration are records generated by administrators of B2C or application developers in the B2C AD domain. Client ID (Registration) Environment Table 6: App Registrations for Power Pages Portal SSO The detailed integration details are further described in the implementation details and baseline configuration sections of this document. The sequence diagram below demonstrates the user journey for authorization to B2C from Power Pages Figure 14: B2C Journey For this to work, Power Pages site settings must be configured with a “Client ID, Secret, Tenant ID, the User Flow, and mapped claims (email, first name and last name)” from B2C so that the portal can generate a “bearer token at runtime which is a hashed value (RSA 2056) of a JSON Web Token (JWT) containing these “claims” and the session ID and its lifespan (19 minutes as per LOA 2/PB requirements). The diagram below depicts this process. Figure 15: B2C Technical System to System OIDC Journey In this diagram, the Application uses the Client ID and Secret to authenticate with the B2C Tenant and obtain an Access Token. The Access Token is then used to request User Claims and make API calls. If the Access Token has expired, the Application uses the Refresh Token to request a new Access Token and then uses it to request User Claims and make API calls. The hashed JWT includes: Username App ID SID (session ID) Azure API Management (APIM). APIM is leveraged as the API management system, or the org master and report ingest endpoints (REST). API requests are executed to the APIM perimeter by using a client ID and secret generated by an App Registration in B2C which in turn issues a bearer token for subsequent calls to the endpoints to return payloads such as the RE intuitional data, primarily the basic organization information and the various contacts associated to these institutions from the primary contact to authorized agents that will be interacting with the portal. (RICK + TEAM INPUT REQUIRED) Dynamics 365 Customer Service - Architecture and data model (OOB) The table below lists all the tables that are used in this implementation and includes the CRUD operations including append and append to privileges by security role. The table lists all the column permission rules set by team in the system. This feature secures specific field(s) within a table that provides a more granular level of data governance. For example, only a manager can set a specific value in a column (field) in the case table. The table lists all the relationships and their cascading rules in the system. This is important for data governance as retention and disposition rules that will govern the purging of the records will rely on the cascading rules when using the bulk delete feature whereas cascading rules that configured as “referential” will be purged using Power Automate flow as when a relationship is set to referential, the parent record that’s being deleted will not purge its child records that are referential, instead it will simply remove its reference to it or clear the lookup field value. In certain scenarios this may make sense however the cascading set to parental is key so that for example if an organization is purged, all its child contacts, cases, and related records are also purged automatically without having to configure additional flows or workflows to destroy these records. However, the account table has multiple relationship types to the contact record to track the various types of contacts in 1:N and the system only allows for 1 parental relationship type by table thus both bulk delete and Power Automate flows will be responsible to purge data. Table Name Key(s) Relationship Name Assign Delete Share Re-Parent Delete Merge Table 7: D365 Table / Shema Relationship Types/Cascade Configurations Table Name Create Read Update Delete Append Append To Team Team’s associated security role(s) Table 8: Table CRUD and Append/Append to/Assign privileges by security roles/team Case Management & Subject Convention A key convention in the design of this application is that every center around the case table. This means that each time {ORGANIZATION} generates a reporting cycle, an out of the box Case record is created of type “Reporting Cycle”. This case record has 1 or more associated case records of a particular type (or Subject) such as “Risk Questionnaire”. The Subject field is an OOB field that allows a system administrator/configurator to configure Case types using a tree like structure. The complete Subject tree is provided below (subject to changes over time, thus this version is based on the 2023 available compliance form types. Customizations and extensions Dynamics 365 Customer Service module offers a wide range of customization and configuration features that can be tailored to fit the specific needs of a case management system for financial institutions. With Dynamics 365, users can easily customize the look and feel of the system, including the layout, branding, and navigation. The module also allows for the creation of custom fields and forms, enabling users to capture and track relevant data specific to financial regulations. Additionally, Dynamics 365 provides automated workflows and business process flows that can be customized to streamline case management processes and increase efficiency. These features, combined with a comprehensive reporting and analytics dashboard, enable financial institutions to effectively manage cases and ensure compliance with regulations. Power Pages Sites is another powerful tool that can be utilized to enhance the case management system. Power Pages Sites provides a customizable web portal that allows external users, such as financial institutions, to submit and track the progress of adherence to financial regulations. The portal can be configured to include specific forms and fields for data capture, as well as custom branding to ensure consistency with the financial institution's branding guidelines. Additionally, Power Pages Sites allows for the integration of various third-party applications, such as document management systems or payment gateways, to streamline processes and improve the user experience. With its robust customization and configuration capabilities, Power Pages Sites can provide financial institutions with a powerful tool to effectively manage adherence to financial regulations. This section delves into the details of the various customizations and extensions leveraged for features developed for this implementation. Dynamics 365 workflows Dynamics 365 workflows are automated processes that streamline business processes and enable users to define and automate business logic. Workflows can be used to automate tasks, such as sending email notifications or creating tasks, based on specific conditions or events in the system. They can also be used to enforce business rules and help ensure data consistency. Workflows can be created and customized by users with appropriate permissions using the Workflow Designer in Dynamics 365.\\ Name Description Async/Sync Solution Table 9:Workflows Dynamics 365 actions Dynamics 365 actions are custom methods that can be created and exposed through the system's web API. Actions can be used to encapsulate business logic and provide a simplified interface for external systems to interact with the Dynamics 365 system. Actions can be synchronous or asynchronous and can return data or perform operations on the system. They can be created and customized by developers using the Dynamics 365 SDK. Name Description Async/Sync Solution Table 10: Actions Dynamics 365 plugins & Custom Workflow Steps Plugins Dynamics 365 plugins are custom code that can be executed in response to system events, such as creating or updating records. Plugins can be used to extend the system's functionality, automate business processes, or integrate with external systems. Plugins are written in .NET and can be registered to execute in a synchronous or asynchronous manner. They can be created and customized by developers using the Dynamics 365 SDK. Plugins are “signed” binaries generated by .NET and must be registered to CRM using the Power Platform CLI and DevOps Pipelines (sourced in GIT). The table below lists all custom plugins, their purpose and direct link to repository. Finally, plugins run within a sandbox which secures the platform from unwanted customizations from third party libraries or misconfigurations and unwanted code by forcing the use of a specific first party libraries, signed binaries using developers private key and published via Pipelines. Plugin assemblies are comprised of one of more classes that can be configured as “plugin steps” whose purpose is to run on any CRUD operation and the developer can specify which fields are allowed to be read and interacted with in code and best practice dictates to only use the tables and fields required to make a working configuration. Furthermore, developers are instructed to adhere to professional and verbose logging using the plugin trace logs and this is inspected during code reviews and test automation will calculate the % of plugin trace logs vs lines of codes and the benchmark is 20%. Custom workflow steps These are almost the same as plugins however provide more flexibility to non-developers who prefer to leverage workflows (processes) to call custom steps that are not available. For this implementation, custom workflow step libraries have been imported to help with mundane and repetitive needed features such as querying 1:N relationships for process logic and more robust email automation. No custom step library has been developed in house (yet). Assembly Name Purpose Type Repository Location Owner e.g. Plugin, Custom Workflow Step {ORGANIZATION}, Third Party (provide full name of lib) Table 11: Plugins Power Automate (flows) Power Automate flows (formerly known as Microsoft Flow) are cloud-based workflows that can automate business processes and integrate with other systems and services. Flows can be triggered by events in the system, such as creating or updating records, or by external services, such as receiving an email or a tweet. Flows can be used to perform a wide variety of actions, including sending notifications, creating records in other systems, and generating reports. Flows can be created and customized by users with appropriate permissions using the Power Automate Designer. The table below outlines and describes each Flow developed for this implementation. Flow Name Purpose Trigger Primary Table Table 12: Power Automate Flows Azure B2C & Integration to the Organization Master & Report Ingest APIs B2C has been described in length in previous sections, but to elaborate at our D365 application layer and its “indirect” relationship to the portal, B2C is not only leverage for SSO into the portal but also the OAUTH 2.0 authorization provider of bearer tokens for the backend D365 Power Automate Flows. An app registration with API permissions to both the org master and report ingest APIs are dedicated to each environment (1 per environment) calling the UT1 up to ET1 and eventually prod APIM endpoints. The table below maps the Dataverse environments to each APIM REST service and their respective App Registration whose secret must rotate every 24 months for NP and 6 months for prod (optional – c/b up to the maximum of 24 months). Dataverse Environment API Instance / base endpoint w/o query params or payload App Registration’s CLIENT ID for OAUTH 2.0 token Service for which the flow context executes (API Permissions) Dev UT1 Staging UT1 Test UT1 UAT UT1 (Tbd) Sandbox UT1 (Tbd) Training UT1 (Tbd) Preprod ET1 Prod TBD Table 13: App Registrations for Power Automate Flow REST API Calls to Org Master and Report Ingest Power Automate performs the HTTPS request to B2C to obtain a B2C bearer token to perform the API nightly. System administrators can invoke the refresh of the API calls directly in flow but running it manually as well and a full log of all calls is available directly in the flow interface. Failures will provide a verbose stack trace of the issue. It is important to note that the flow must be associated to the Power Platform Automation service account which is a licensed user as Flows won’t support App Registrations to perform API calls. This account’s credentials are secured in our KeyVaults instance. The following table includes the JSON payload for GET and POST requests (if applicable) and which table and column they map to in the Dataverse environment. This is available in a Swagger API definition endpoints/file below: Endpoints HTTP Verb Payload / Query Param (sender) Payload / Query Param (receiver) - Map Table 14: Org Master & Report Ingest Endpoints, Payload Schema, an Query Param requirements. Mapping repsonses to the Dataverse Schema Figure 16: B2C - Daemon Flow (API's interfacing with our internal API's) In this diagram, the Daemon first makes an API call to the Application API. The Daemon then authenticates with the B2C Tenant using the Client ID and Secret to obtain an Access Token. The Daemon then makes another API call to the API with Authorization layer using the Access Token to authenticate and obtain the API response. Handling downtime In the event the API(s) are not accessible for which ever reason, our CRM implementation supports data imports using excel and the SDK thus this section describes what developers (or persona responsible to monitory/sync the data is required to do in this scenario. Business Rules Dynamics 365 Business Rules are a feature that enables users to define and enforce custom logic within the system without requiring any code. Business Rules can be used to enforce data validation, automate field calculations, and control the visibility and behavior of form elements based on specific conditions. Business Rules can be configured at both the entity and form level, allowing users to create rules that apply to specific entities or forms within the system. Business Rules are executed in the context of the current user invoking some functionality in Dynamics 365 that triggers one or more of these rules. When a user performs an action, such as creating or updating a record, the system evaluates the associated Business Rules and executes any actions that are defined in those rules. The sequence of execution for Business Rules is determined by the order in which they are defined within the system. Business Rules can be used in conjunction with other Dynamics 365 features, such as workflows, Power Automate flows, and web API calls. Workflows and flows can be configured to trigger based on specific conditions or events and can include actions that invoke Business Rules as part of their execution. Web API calls can also be used to execute Business Rules programmatically. Overall, Dynamics 365 Business Rules provide a flexible and powerful way for users to enforce custom business logic within the system, without requiring any code or development expertise. By combining Business Rules with other Dynamics 365 features, users can create complex, automated processes that help streamline their business operations and improve efficiency. The table below lists all the business rules and what level they’ve configured at. It is important to note that form level business rules execute at run time on the client, whereas entity level configured business rules run on the server thus are more reliable as when using API calls, workflows and other automation we want to ensure that business rules are ran. Form level business rules are useful for im../images/SDD/te feedback when populating forms, however entity level will do the same. The only valid scenario that form level business rules are relevant if there is a client-side rule that needs to be implemented and doesn’t need to be interpreted or governed service / targets a specific form. Table Business Rule Level Table 15: Business rules by Table Azure AAD Groups / Teams & environment integrations In the summer of 2022, the wave 2 general available release of the D365 update included support for full integration of AAD groups to D365 Teams and unlike its previous implementation now supports full CRUD. This means that {ORGANIZATION} can administer RBAC directly in AD which is best practice for access control across the organization. The way its designed in system is that we have an AAD group (M365 type) for each team in CRM and that team is mapped to 1 or more security roles. This means that when a user is onboarded to {ORGANIZATION} to requires access to the compliance case management system, the typical request to IT to have the user added as a member to an AAD Group(s) will automatically replicate to CRM thus giving the user automatic access based on their persona governed by the CRM team’s associated security role. Therefore, no one will have privileged access and user administration privileges in the CRM reducing risk of human error and potential security threats by providing a user with a security role that they should not be allowed to be associated it. Furthermore, the security posture is further optimized by automatically revoking access to CRM if the user leaves {ORGANIZATION} and is removed from AD or the AAD group. If a user is promoted into a new role, they would be added to the AAD group associated with the higher-level CRM team and thus inherit the additional security parameters associated with the role tied to the team. The overall strategy is to leverage 1 RBAC at the organization level rather than app level in terms of access. The table below lists the AAD Groups assigned to its respective CRM team and the security role(s) associated to the team. In certain edge cases, a system administrator may need to troubleshoot an issue in production thus an AAD group for system administrator has been created and mapped to each environment (minus dev which this doesn’t apply) to troubleshoot issues. However since this is a privileged role in the CRM, it is recommended that the group is configured in Azures privileged identity management system (PIM) so that we can control the time and force the user to provide a quick justification for why they need to obtain sys admin in an environment that is not development (e.g. sometimes post deployments, we need to manually activate a workflow, or a model driven app change, this is a rare occurrence and due to a faulty deployment but still needs to be done especially in production to avoid bugs). AAD M365 Group Team/GUID CRM Team Security Role(s) / Field Level Permission Profile(s) Table 16: AAD Group Mapping to CRM Team Figure 17: Azure AD Group/RBAC integration with CRM Environment groups AAD groups are also applied at the environment level. This means that each environment user provisioning is restricted to users who are members of these groups. This also doesn’t mean that they automatically get access, this is done via team AAD group integration however, this is best practice to ensure that the entire directory is not available for security role assignment without using the Organization. The table below lists the security groups associated with each CRM organization. Also included is the security group that has the licensing tied to it – which means each user who requires case management and or customer service licensing (same licensing) will inherit these. Once in that group, they must be added to the environment group and the group(s) for RBAC in the environment(s) they access. Team Name Type Environment Power Pages Site (customer self-service) System Configurations & Integrations This section outlines the site settings configured for each portal. Site settings and settings are the key configurations and integration. It is important to note that all non-production portals are “private” and therefore available only to internal users and protected by Azure AD. The public site is available on the internet and has a Canada.ca DNS CNAME entry configured (described in this section). Setting Name Value Description Table 17: Portal Settings Figure 18: Privatization of Portal Site by Azure AD Group (applicable to non production sites)a Diagnostics (Azure Storage) The Power Apps platform allows for the storage of diagnostic data in an Azure Storage account. This feature enables the collection and analysis of diagnostic data for Power Apps. This data can include app telemetry, usage data, and error logs. The storage account is also leveraged to host a static website of this implementation’s technical and business documentation using the Microsoft DocFX framework, which is the same technical documentation framework leveraged by Microsoft documentation and is part of the leading technical documentation frameworks that is also corporately backed (and open sourced) which aligns with TBS cloud usage guidelines. To implement this feature, the following steps were taken (applies to all environments including production): Creation of an Azure Storage account: An Azure Storage account was created using the Azure portal. Acquisition of connection string: The connection string for the Azure Storage account was obtained. This connection string was in the format of \"DefaultEndpointsProtocol=https;AccountName=<your_storage_account_name>;AccountKey=<your_storage_account_key>;EndpointSuffix=core.windows.net\". Configuration of diagnostic settings: Diagnostic settings were configured for the Power Apps environment by navigating to the Power Page Site, selecting the relevant environment, and choosing \"Diagnostics\" from the left-hand navigation menu. Selection of Azure Storage as destination: In the diagnostic settings, Azure Storage was selected as the destination for the diagnostic data. Provision of connection string: The connection string obtained in step 2 was provided. Selection of data types: The types of data to be collected and stored in the Azure Storage account were selected. Saving of settings: The diagnostic settings were saved. Upon implementation, Power Apps began storing diagnostic data in the Azure Storage account. This data can be analyzed using Azure services such as Azure Monitor, Azure Log Analytics, and Azure Data Explorer. Overall, this integration provides a means to collect and analyze diagnostic data, thereby aiding in the identification and resolution of issues that may arise within the Power Apps environment. For example, if the portal has poor performance or integration between B2C or SharePoint is problematic, examining the diagnostic logs provides a more verbose exception trace. The logs are organized by date, and each date has a dedicated folder with x number of log files to analyze. Diagnostic logs are retained for 30 days and in this implementation, logs are used by developers only to help with troubleshooting, the storage accounts are not being monitored by a SIEM or sent to a log analytics workspace. Finally, the storage account blob containers are set to private (no anonymous usage) and are restricted to VPN traffic and Active Directory user thus inherits the access policies of the organization. The table below lists each storage connection by environment. Storage Connection Storage Account / Subscription Environment SharePoint Online Figure 19: SharePoint document journey/Integration with CRM Exchange Online & notification center Figure 20: Emails & Notifications Architecture and integration with Dynamics 365 Server-side integration The portal integrates with CRM tables using a combination of table permissions to allow developers to configure which tables are allowed to be exposed to the portal. These table permissions are described futher in this document. Once table permissions are configured, the developer uses “Liquid” which is the HTML templating language used to code web templates to expose data from the configured tables to the UI. When a portal is created in a Dataverse environment, the engine creates an “App Registration” record in Azure AD which acts as the primary service account for server-to-server communication between the portal and the integrated Dataverse environment. The table below lists all the App Registration records for each portal generated for each downstream environment up to production. App Registration ID Environment Table 18: App Registration for each Portal Data flow and synchronization Power Pages Sites are external-facing websites built on top of the Dataverse platform, which is a cloud-based data storage and management platform provided by Microsoft. These portals allow users to interact with the data stored in Dataverse without requiring them to have direct access to the underlying database. The data flow between Power Pages Sites and the Dataverse backend follows a specific pattern. Here is a step-by-step breakdown of how the data flows to and from Power Pages Sites and the Dataverse backend: User Interaction: Users interact with the Power Page Site using a web browser or mobile app. They can access various types of data, such as customer records, invoices, or service tickets. Portal Website: The Power Page Site website is hosted on Microsoft's cloud infrastructure, and it acts as a front-end interface for users to interact with the data stored in the Dataverse backend. Authentication: Users are authenticated before they can access the portal. They can either authenticate using their Microsoft account or a custom authentication mechanism provided by the portal. Portal Pages: Users navigate through the portal's web pages to access and interact with the data they require. Each page consists of web components such as forms, lists, and charts. Data Requests: When a user requests data, the portal sends a request to the Dataverse backend. The request includes information about the data the user wants to access or modify. Dataverse: The Dataverse backend processes the data request and retrieves the relevant data from the database. The data is returned to the portal in a JSON format. Data Display: The portal displays the data returned by the Dataverse backend in the web components of the portal page. User Actions: Users can take actions on the displayed data, such as updating customer records or creating new service tickets. When the user performs an action, the portal sends a request to the Dataverse backend to update the database. Dataverse Updates: The Dataverse backend processes the user action request and updates the database with the new data. Confirmation: The portal confirms to the user that the action was completed successfully. Data Sync: In case of offline access, the Power Pages Sites support data synchronization, where changes made to the data in the portal are synced back to the Dataverse backend once the connection is re-established. In summary, the data flow between Power Pages Sites and the Dataverse backend involves users interacting with the portal's web pages, and the portal making requests to the Dataverse backend to retrieve or update data. The Dataverse backend processes these requests and sends the requested data back to the portal for display. Users can take actions on the displayed data, and the portal sends requests to the Dataverse backend to update the database accordingly. The illustration below describes the integration of the portal and the integrated CRM environment. Figure 21: Portal’s integration with Dynamics (OOB) Client-side integration For more dynamic functionality, developers can use JavaScript to perform CRUD operations to Dataverse tables. However, a convention has been applied to our implementation that this will be leveraged for read operations only. To configure a table to be read using the web API (and JavaScript), beyond the table permissions, the following site settings must be configured for each table and fields the developer wants available for JavaScript. In the diagram below, the \"Dataverse Portal\" is represented as node A, the WebAPI configured in site settings is represented as node B, and the Dataverse is represented as node C. The arrow indicates the flow of data between the Dataverse Portal and the WebAPI, which is used to read and write data to the Dataverse. Figure 22: WebAPI Flow Diagram For our initial implementation, this feature is being used for the Notification center. The table below demonstrates the site settings used for exposing the notification center to JavaScript. Table 19: WebAPI Settings for Notification Centre This feature is further described in the next section. Portal notifications Portal notifications are available to any user who receives notifications generated by the notification center that contains protected information rather than receiving the information via email. The email will notify the user to login to their account to view the notification. Figure 23: Notification Centre Feature Screen Captures It is important to note that external users have the freedom to choose when they mark a notification item as read which will automatically update the status reason of the notification item. Another key feature of the notification center is the ability for both internal and external users to communicate via a comment thread. The comment thread is captured also as an activity called “Portal Comment” and provides the flexibility for communicating through the notification center and avoid sharing protected information via email. Figure 24: Portal - Notification Center Comments Figure 25: Internal Portal Comment Response The data model behind the notification center is simple, it includes a templating system, a notification item and a direct link to a contact record which is the user of the portal. Figure 26: Notifications ERD Most portal notifications are sent using processes and or flows (listed in table below) thus are automated. However internal users can create manual notifications using the model driven app. The illustrations below demonstrate the process of creating notification items. Finally notifications are configured as “activity” entities thus are treated as activities in CRM such as emails and tasks and therefore are polymorphic and can be linked to any other activity record, in particular for this implementation, case files, contacts and organizations (Reporting Entities). Figure 27: Creating a manual notification item from model driven app Once saved, an email is sent to the recipient with a link to the portal notification centre advising them to sign in to view the notification details. Figure 28: Example notification item email notification Data Architecture, CRM & Portal Design Conventions The Data Architecture section covers the data model and schema design for the CRM case management system, including entities, fields, and relationships, naming conventions and standards, processes and automations for data management, workflows and business process flows, Power Automate Flows, and business rules and validation. Additionally, it discusses validation rules and custom scripts, the business rules engine, and custom actions. CRM & Portal Design Conventions This section describes the design conventions applied to the portal implementation. By applying conventions developers have a simple reference source to minimize the interpretation of how to implement features. Naming conventions and standards for Portal Artifacts The system is maximizing the usage of the D365 customer service & case management licensing feature which comes with features such as enterprise ISO standard case management features such as SLA timers, alerts, integrations with the organizations primary RBAC system (AD), client insights, email integration and robust monitoring and immutability for auditing of a case management lifecycle. In addition, the {ORGANIZATION} specific requirements not only has been adapted to fit within this feature set, but the build has a series of conventions that must be followed when designing the system when it comes to creating tables, fields, relationships from naming conventions and configurations. The next sections describe these conventions and developers must follow these and the DevOps CI/CD will automatically test against any violations of these conventions to ensure that the system is healthy, maintainable and knowledge transfer is implied: Feature name Description Convention (naming) Convention (configuration) Auto-numbering (Case) Auto-numbering (Related Case Form) Table 20: Portal conventions Dataverse (CRM) Tables, columns, and relationships The data architecture implementation has been standardized to leverage and customize the case management feature of the platform. This means that that the “Case (Incident)” table / feature set is the focal point for every type of compliance form process. Each compliance form type is captured as a Subject which is a case artifact that ships with the customer service module. Furthermore, each subject that is “external facing” such as the “Risk Questionnaire” is linked to a Portal Wizard Form which in turn leverages the out of the box Power Pages web link set / web links & basic forms with a custom framework built around it to meet the WET Canada.ca framework standards, accessibility requirements and overall usability and ease of configuration. A compliance case of type “reporting cycle” is what governs the overall process of inviting reporting entities to submit compliance forms. A reporting cycle has several types including “financial institutions and sector specific” (full list in table below) which allows for dynamic assignment of organizations to a reporting cycle (1 or more) – for example, by choosing sector specific the reporting cycle is automatically populated with all active organizations in that sector, by choosing financial institutions, the case creator can select 1 or more Contacts (external portal users) regardless of sector. Figure 29: Logical ERD - Case Management Process. & Integration with Compliance forms (convention) Solutioning (patches) & Managed vs. Unmanaged A primary solution entitled {ORGANIZATION} is created with all assets from table, fields, custom controls, forms, plugins, flows etc. Developers provision patches off this solution each sprint to perform their duties. They use the UI or CLI with VS Code to make changes to their patches and once tested Development they run the CI pipeline to issue a release to staging to validate a successful deployment of their feature. This pipeline will test best practices and other tests described further in this document. Solutions and patches are “unmanaged” in development and “managed” everywhere else. Managed means that configurators cannot make changes to the system without upgrading (re-deploying) a new version of a solution thus guarding against misalignment between configurations between environments. The diagram below depicts how an unmanaged patch or solution is exported as managed and moved downstream using our pipeline. Figure 30: Unmanaged/Managed Solution Deployments This flow chart represents the process of exporting an unmanaged Dynamics 365 solution patch as a managed solution, and then releasing it through a DevOps release pipeline. The process starts with an unmanaged solution patch, which is then exported as a managed solution using the Dynamics 365 solution export tool. The managed solution is then added to a DevOps release pipeline, which includes several stages such as development, test, and production. When the solution is deployed to each environment, it is tested to ensure that it works correctly and does not cause any issues. Once the solution has been successfully deployed to the development environment, it can be promoted to the test environment for further testing. Finally, when the solution has been thoroughly tested and validated, it can be promoted to the production environment for release to end users. The entire process is managed through the DevOps release pipeline, which ensures that the solution is deployed consistently and reliably across all environments. Best practice dictates that the solutions are deployed not only as managed but the setting to “upgrade” the solution is flagged in the pipeline to ensure that that any components that no longer exist is in the new version but present in the previous version are removed from the system. The pipeline also handles staging the upgrade which allows for the pipeline to deploy reference data or remove data prior to deletion of the previous version. The three types of deployment options for managed solutions are described below: Upgrade This is the default option and upgrades your solution to the latest version and rolls up all previous patches in one step. Any components associated to the previous solution version that are not in the newer solution version will be deleted. This option will ensure that your resulting configuration state is consistent with the importing solution including removal of components that are no longer part of the solution. Stage for Upgrade This option upgrades your solution to the higher version but defers the deletion of the previous version and any related patches until you apply a solution upgrade later. This option should only be selected if you want to have both the old and new solutions installed in the system concurrently so that you can do some data migration before you complete the solution upgrade. Applying the upgrade will delete the old solution and any components that are not included in the new solution. Update This option replaces your solution with this version. Components that are not in the newer solution won't be deleted and will remain in the system. Be aware that the source and destination environment may differ if components were deleted in the source environment. This option has the best performance by typically finishing in less time than the upgrade methods. Web Pages/Page Templates/Web Templates Web pages Web pages in Power Page Sites are the fundamental building blocks of the portal site. They define the content, layout, and functionality of the pages that are displayed to end-users. Each web page is associated with a specific entity or set of entities and can contain a variety of components such as forms, lists, charts, and web files. Page templates Page templates define the layout and structure of web pages in Power Page Sites. They provide a standardized framework that can be used to create consistent and reusable pages across the portal site. Each page template is associated with a specific entity or set of entities and can contain a variety of pre-configured components and placeholders that can be filled with content when the web page is created. Web templates Web templates in Power Page Sites define the overall layout and design of the portal site. They provide a high-level view of the portal, including the navigation menu, header, footer, and other site-wide elements. Web templates can be used to create a consistent and professional-looking portal site that is easy to navigate and use. When a user requests a web page, the following steps are typically taken: The user's request is sent to the portal server, which checks to see if the user is authenticated and authorized to access the requested page. If the user is authorized, the portal server retrieves the appropriate web page and any associated page templates. The page template is applied to the web page, which provides the overall layout and structure for the page. Any placeholders or components in the page template are filled with the appropriate content, such as fields from the entity record or custom code. The completed web page is then rendered and sent back to the user's browser for display. Overall, web pages, page templates, and web templates are critical components of Power Pages Site for this implementation as it enables developers to create and manage our wizard forms and other pages relevant to compliance, it’s basically the foundational of the framework. The table below lists all the web pages, associated page and web templates in this implementation along with its naming conventions. Table 21: Web Pages & Templates Content Snippets The content snippet feature in Power Page Sites allows developers and content creators to create reusable blocks of content that can be easily added to web pages and templates. These content snippets can include text, images, links, and other types of content that are frequently used across multiple pages or sections of a site. One important aspect of content snippets in Power Page Sites is that they can be made editable for static content. This means that content creators can modify the content of a content snippet without having to modify the underlying web page or template. This is especially useful for static content that is used across multiple pages, such as a company address or copyright notice. From a translation perspective, the content snippet feature is extremely important. It allows content creators to create a single version of a piece of content, such as a button label or a heading, and then translate that content into multiple languages using the portal's translation feature. This eliminates the need to duplicate content for each language, which can be time-consuming and error prone. For this implementation, we support both French and English content, thus a content snippet of the same name but different language selection is created to automatically generate the content in the user’s language of choice. This can save time and reduce the risk of errors in the translation process. This tool allows developers and content creators to create reusable blocks of content that can be easily added to web pages and templates. By making content snippets editable for static content, content creators can modify the content of a content snippet without having to modify the underlying web page or template. And from a translation perspective, content snippets can greatly simplify the translation process by allowing translators to focus on translating only the text that needs to be translated, rather than translating an entire web page or template. The table below lists all content snippets used in this implementation along with its naming convention. Table 22: Content Snippets Site Markers The Site Markers feature in Power Pages is a tool that allows users to easily create visual markers on their website pages. These markers can be used to highlight important areas of the page or draw attention to specific elements, such as calls-to-action or anchor (URL) tag reuse (most important). Benefits of using Site Markers include: Increased Engagement: Site markers can help grab the attention of visitors and encourage them to interact with the page. Improved User Experience: By highlighting key elements on the page, site markers can help users quickly find the information they are looking for. Enhanced Branding: Site markers can be customized to match a website's color scheme or design, helping to reinforce brand identity. Better Analytics: Site markers can be used to track user behavior and engagement on specific areas of the page, providing valuable insights for optimization. Use of variable feature to point to site marker liquid object rather than a direct hard coded URL. Therefore, in the event of a URL change, the developer can update the Site Marker webpage / URL and wherever the marker is referenced in the application, it will be updated with the new URL. One of the main benefits of using Site Markers is that they can be leveraged for re-usability and inheritance, which can greatly enhance the developer experience. Instead of hard-coding URLs in anchor tags, developers can reference a Site Marker, which can be updated globally across the website. For example, our footer has a series of Site Markers for Canada.ca and {ORGANIZATION} links that appears at the bottom of every page. Rather than hard coding the URL in every anchor tag on the website, the developer can reference the Site Marker for the footer. If one or more of the footer URL changes or the URL(s) needs to be updated, the developer can simply update the Site Marker, and every anchor tag that references that Site Marker will automatically be updated as well. This not only saves time and effort for developers, but also helps ensure consistency and accuracy across the website. Additionally, if the website is ever redesigned or updated, the Site Markers can be easily updated to reflect the changes, without having to manually update every individual anchor tag on the website. In summary, the re-usability aspect of the Site Markers feature in Power Pages allows developers to leverage inheritance and reference markers rather than hard-coding URLs in anchor tags. This can save time and effort, ensure consistency and accuracy, and enhance the developer experience overall. Overall, Site Markers can be a valuable tool for improving the developer experience by creating URL objects thus levering inheritance and avoid the pitfalls of having to update URL’s manually across multiple pages, increasing engagement, and driving conversions to the compliance portal. In this implementation, we make usage of site markers for inheritance exclusively. The table below lists all the site markers used in this implementation. This includes also the naming convention that needs to be applied to the marker name. Site Marker Name URL Web Template/Web Page Table 23: Site Markers Web Roles Power Page Sites, web roles are used to control access to specific areas and features of a portal site. A web role is a collection of permissions that are assigned to a user or group of users, allowing them to perform specific actions or access specific areas of the site. There are several types of web roles in Power Page Sites, including: Parent web role: This type of web role is typically used to control access to the entire portal site. Users with a parent web role have full access to all areas and features of the site. Global web role: This type of web role is used to control access to specific areas of the site that are common to all portal pages. Users with a global web role have access to all portal pages, but only to the areas and features that are covered by the global web role. Contact web role: This type of web role is used to control access to areas of the site that are specific to a contact record in Dynamics 365. For example, a contact web role might be used to control access to a contact's personal information or account history. Self-web role: This type of web role is used to control access to a user's own data and profile information. Users with a self-web role have access to their own data and profile information, but not to the data and profile information of other users. Web roles are assigned to users or groups of users using security roles in Dynamics 365. Once a user has been assigned a web role, they will be able to access the areas of the site and perform the actions that are covered by that role. In summary, web roles in Power Page Sites are used to control access to specific areas and features of a portal site. There are several types of web roles, including parent, global, contact, and self-roles, each with its own set of permissions and access controls. Web roles are assigned to users or groups of users using security roles in Dynamics 365. Note that out of the box, an administrator web role is available and used by developers to perform development tasks and testing. This role is never assigned to any user who hasn’t been approved. This list is tracked as a JSON object in the test folder in the pipeline and if a user who is not approved is discovered in the delta the release is rejected. The table below lists all web roles in this solution. Table 24: Web Roles Translations Translations in Dynamics 365 and Power Page Sites allow you to create multilingual versions of your application or portal. This means that you can translate your user interface, forms, and other components into different languages to support users who speak different languages. In Dynamics 365, translations are managed using a translation file. This file is essentially a spreadsheet that contains all the labels, messages, and other text that appears in your application. You can export the translation file to Excel, translate the text into your desired languages, and then import the translated file back into Dynamics 365. Power Page Sites also support translations and use a similar process to Dynamics 365. When you create a portal, you can specify the languages that you want to support. You can then export the translation file for your portal and translate the text into your desired languages using Excel. Once you have completed the translations, you can import the translated file back into your portal. In both Dynamics 365 and Power Page Sites, translations can be applied to various components, including forms, views, and fields. You can also use translations to localize your application or portal for different regions or countries, by translating region-specific terms or phrases. To make it easier to manage translations in Power Page Sites, you can use the Content Snippet feature. Content snippets are blocks of text that can be reused throughout your portal, such as for page headers or footer text. When you create a content snippet, you can specify which languages it should be available in. This allows you to create translated versions of your content snippets, which can be used throughout your portal. In summary, translations in Dynamics 365 and Power Page Sites allow you to create multilingual versions of your application or portal. Translations are managed using a translation file, which can be exported, translated, and imported back into your application or portal. Content snippets can be used to manage translations more efficiently in Power Page Sites. Table and column permissions The table and column permission feature of Power Pages site allows you to control access to specific tables and columns in your application. This feature is used to restrict access to certain sensitive data in this implementation to ensure that only certain users can view or modify certain data. The way the table and column permissions in Power Pages site were set up is done in the \"Security\" section of the application's settings. From there, the \"Table and Column Permissions\" option has been configured. Once selected, a list of all the tables in the application is displayed for selection. Developers then selected the tables to set permissions for and choose which web roles should have access to these tables. Similarly, the column-level permissions for each table, which allows to control which web roles can view or modify specific columns within that table has been configured. This is like field level permissions in CRM whereby you can configure security at the field level instead of just at the table level. For example, there are certain fields on the case file that can only be available to certain CRM teams and web roles. By setting up table and column permissions in Power Pages site, we ensure that only authorized users have access to the data they need to perform their job functions, while also protecting sensitive data from unauthorized access. Finally, there are different types of permissions such as parent/child, self, contact and global permission types. For example, the case and form table permissions are set up as “parent” to the “Contact” table permission to ensure that only the currently logged in user can view the cases assigned to them rather than having access to all cases in the system. The same applies to notifications and contact specific data. The table below outlines both the table and column permissions configured in this implementation by web role. Table Permission Name Table Type Parent (if applicable) Web Role Figure 31: Table Permissions Column Permission Name Field Type Web Role Table 25: Column Permissions When and when not to use client-side scripting for portals The convention and rule of thumb for applying the ability to use JavaScript (client-side scripting) on the portal is for optimizing the user experience by providing im../images/SDD/te feedback to the user without having to reload the page or redirect. This is the only use case whereby this feature can be leveraged – we do not allow create, update, and delete operations using client-side scripting. The reason is primarily for security – the way browsers work is that it will download the entire HTML, CSS and JavaScript files on the page and users have access to this code and can interact with it using most browser’s developer tools. This means that if we allowed write operations to the portal via JavaScript we may risk attacks such as a user with an active session, auto creating unlimited amounts of records (however the security perimeter would guard against this) or perhaps try and circumvent validation rules on the forms thus despite this feature being well designed and built with security features that make it difficult to perform nefarious actions, we made the conscious choice of not exposing this feature beyond read operations as there are no risks associated with this. The reason for this is that table and column permissions will persist to the WebAPI (so client-side scripting) and therefore is a user attempts to leverage JavaScript to perform read operations, they will be limited to reading their own data that is governed by the permissions feature. Wizard Forms Due to some of the limitations associated with the “advanced forms” feature of Power Pages Sites, this implementation leverages basic forms with a series of custom web templates that render a wizard form using a combination of web link sets, web links and basic forms. This means that a user will first create a web page that is associated to a basic form of type insert which will serve as a parent to all sections of the wizard. This is because the user will first be presented with either a consent screen (which is a webpage with an insert form) and upon submit is redirected to the first step of the wizard which is another web page associated with a basic form of type “edit” that uses the GUID generated by the insert step and appended to the query string of the browser path. Each step of the wizard is a child of the parent insert form web page. Once this is all configured, the user will then create a “web link set” that will be equivalently named to the parent web page and the links created in the web link set will point to each child web page of the insert form web page in the desired order. Finally, the child pages (wizard steps) must use the wizard form step page template for the wizard to render correctly. The illustration below depicts an example web link set which is a wizard. Table 26: Wizard example (web link set) Figure 32: Parent Page (Wizard Insert Page) Figure 33: Wizard Step (child page) Table 27: Wizard Forms Versioning forms Each year, or periodically, {ORGANIZATION} may issue a new version of a form. The requirement however states that retention of the previous form and its data remains intact. Therefore, staff will be required to clone (save and copy) the existing form to change, make the changes, clone (save and copy) the existing portal wizard and configure new web pages (optional) and basic form record to point to the new form. The sequence diagram below depicts this. Furthermore, a step-by-step guide of how to do this is also illustrated (and subject to change below) Basic Form Metadata The Power Pages Basic Form feature is a tool for creating and customizing forms on a website or landing page. With this feature, users can create forms with a variety of field types, including text fields, drop-down menus, and checkboxes, and customize the form's appearance to match the design of their website. The Basic Form feature includes a drag-and-drop interface that makes it easy to add and arrange form fields, and users can preview the form as they build it. The feature also allows for the creation of multi-page forms, where users can split longer forms into smaller sections for easier completion by the user. Additionally, the Power Pages Basic Form feature includes options for configuring form submission settings, such as email notifications and confirmation messages for users. The feature also allows users to set up integrations with third-party tools like email marketing software or customer relationship management (CRM) platforms. Overall, the Power Pages Basic Form feature provides a comprehensive solution for creating and managing forms on a website or landing page, with customizable fields, design options, and integration capabilities. This feature is also pivotable to the wizard form, each form step has a web page that is associated with a basic form. The table below lists all basic forms in the system. The name includes the convention. Power Pages Sites also includes its own feature for muti-step complex forms. However, it was deemed that the feature did not meet our requirements and was jhnacehk Basic form name Table Webpage Table 28: Basic Forms Libraries: Plugins and custom workflow extensions (DRY & Namespace) Plugins and custom workflow steps are required for more complex functionality that are either inefficient to create in flows or processes or impossible to implement using these features. This is comprised of C# class libraries that are signed by developers for security and run within a sandbox thus cannot interact with third party libraries or security reasons. In this implementation, the table below lists all the class libraries leveraged in this implementation and the namespace (convention). This implementation uses a third party custom workflow step library that is open source to help with mundane behavior that would take weeks to develop and test and these are listed in this table as well. Class library name Type Managed/Unmanaged Table 29: Class Libraries Email templates The Email Template feature in Dynamics 365 is a tool that enables users to create and customize email templates for various purposes, such as marketing campaigns, customer communication, and sales outreach. The feature allows users to create a template with a pre-defined layout and content, which can then be reused for future emails. The Email Template feature includes a drag-and-drop interface that makes it easy to add images, text, and other elements to the email. Users can also use placeholders to dynamically populate email content with data from Dynamics 365 records, such as the recipient's name or company information. Additionally, the Email Template feature allows users to configure email settings such as sender information, subject lines, and delivery options. Users can also preview the email template before sending it, to ensure that it looks and functions as intended. The Email Template feature also includes capabilities for tracking email engagement and performance, such as open rates and click-through rates. Users can use this data to refine their email content and improve the effectiveness of their outreach efforts. Overall, the Email Template feature in Dynamics 365 provides a comprehensive solution for creating, customizing, and sending emails, with powerful design and tracking capabilities that can help improve engagement and drive results. This feature is used in this implementation for sending notification emails to recipients and invitations to register to the portal. Workflow (processes) Processes are already listed in this document; however the convention is simple: Always use async processes unless the process throws a run time validation Create a process for Create and Update and using the naming convention: Create-Update-{name of table} CRM Form design Forms should always have its parent record anchor in the header, the status reason, and modified on at the minimum. Secondly, forms that are being published to the portal must have 1 column layout. Third, the related menu should never display relationships with the same name. This means you need to edit the form to edit the relationship names on the form itself. PCF Controls and Third-Party Libraries PCF Controls are framework driven controls that are official first party open-source tools that enhance user experience in CRM and now has support for portals as well. In this implementation, PCF controls are unnecessary. However, the following third-party libraries are being used to enhance the developer experience when creating processes: Library Name GIT URL When and when not to use the CLI The Power Platform CLI is a key pillar of the release pipelines however developers can leverage the CLI when cloning the GIT repository and working within their own developer environment. Thus, when using VSCode to develop instead of the interface, use the CLI to publish your changes. Never use the CLI to deploy changes to other environments beyond your own development local clone such as registering plugins and making changes to portals and the solution in dev only. Deployments are restricted to DevOps only thus refrain from deploying to downstream environments using the SDK, only the pipeline. Reference Data Reference data should be generated by the SDK and the schema file must be committed to the Data/Schema folder in the GIT repository. When running the pipeline, make sure to include the full time including the extension of the schema file (xml) in the artifact variable. The pipeline will handle the export and import to target (and generate the artifact for downstream releases). Secondly, we have a reference-data.xml file in the Data/Schema folder that will be used to host every table and field that is used for reference data to move across environments. It is preferable for developers to edit this file. However, for quick testing in staging follow the process described in the previous section. User Interface Design This section covers the user interface design principles and best practices, including usability and user experience guidelines, design patterns and templates, navigation and menu structure, site map and navigation controls, dashboards and reports, forms, views, and dashboards, customization options and considerations, interactive and responsive design, accessibility and localization requirements, and accessibility guidelines and standards. User interface design principles and best practices This section describes and outlines the user interface conventions for this implementation. This includes a combination of both industry best practices and best practices that have been devised for this specific implementation. Model Driven Apps (CRM backend) These apps are accessible by {ORGANIZATION} employees. In the initial phase, we have a convention whereby there are two applications targeted to and based on the personas (team membership & security role assignments) of CRM users. The first application is targeted to transactional work comprised of views, forms, and automation such as flows, web resources, and processes designed to interact with data submitted from financial institutions. The second application is targeted to configurators responsible for configuration and development of features that are inherited by the transactional application and the portal application. There is a third important application entitled “Portal Management” which includes every portal configuration artifact however, the {ORGANIZATION} Administration application is designed to include the same artifacts and exclude features or records that are not being leveraged by this application to simplify the configurators and developers’ experience and target just the artifacts leveraged by this application. Finally, its important to note that developers of the platform can implement a third type of application entitled “Canvas Apps” whose purpose is to create a completely customized user interface / experience whereas Model Driven Apps have a more specific set of controls and tailored/specific user interface artifacts which simplifies development and configuration and provides a “low code” experience. In summary, the interfaces in this system have been built using both Power Pages Sites (website application) and Model Driven Apps but future implementations can include Canvas Apps and perhaps other types as the platform continues to mature over the years. Navigation and menu structure The navigation should be tailored to be as efficient as possible for the employee to know what needs to be actioned next. This means that the site map, which is the term that describes the menu system, should prominently display a dashboard of relevant data tailored to the users’ persona. A series of dashboards have been developed, one for each “security role” in the system, and users can choose which dashboard to set as ‘default’ when they first open the application. Users also can create their own dashboards (personal). The table below lists the dashboards created for each persona and provides a screen capture of how it looks. Users should be able to interact with the dashboard efficiently, which means – if the user is responsible for reviewing newly submitted forms, they should see a table with clickable links to applications who’ve they’ve been assigned to work on. A manager or coordinator dashboard would list records that have yet to be assigned and other views such as forms that have been assigned but have yet to change status or might be approaching or have surpassed the service standards (configured in the SLA feature) for actioning a record. The other menus accessible on the same site map should include the activities in the system (so emails, tasks, cases etc.), list of organizations, lists of contacts who belong to these organizations, lists of cases and views of each type of form implemented in the system. Finally, each user will have both a team and personal view of their queue of work, which, despite this data being available in the dashboard, provides a richer set of data columns and features such as actioning multiple files at ones using Flows and processes and re-assign one or more records to other queues (e.g. for escalations). {ORGANIZATION} application Menu Item (in order) Section Purpose Table 30: {ORGANIZATION} Model Driven App Menu Structure (Convention) {ORGANIZATION} Administrator Application Unlike the {ORGANIZATION} application, the administrator app is designed to lists menu items in order of importance or frequency of the menu item’s artifacts’ purpose. For example, reference data menu items, are designed to create records that the transactional application or portal application will see and utilize in drop down menus such as countries, case types, provinces, notification templates, email templates etc. The full list is provided below. Menu Item (in order) Section Purpose Table 31: {ORGANIZATION} Administrator Model Driven App Menu Structure (Convention) Forms & views After adherence to the site map (menu structure) the artifacts within each menu such as views and forms also have a set of conventions primary aimed to show the user relevant information prominently and provide ease of use when it comes to interacting with the records for which the user is responsible to action. When a user navigates to any menu item, they are im../images/SDD/tely presented with a view, unless the menu item is targeted to a dashboard record. User’s can interact with data directly within these views by selecting one or more records and pressing a “flow” or “process” or a button in the “ribbon” which is a utility that lives at the top of each view horizontally that presents the user with various buttons and drop-down menus to export reports, data, import data, open the view directly in excel online to make changes in bulk etc. For more complex work and review of data, the user clicks on a view record and are presented with a form. Thus, it is important that guidelines are followed to ensure that both of these artifacts have a consistent user experience across the system so that users get used to the interface quickly and issue feature requests for future releases based on a consistent and repeatable user experience rather than an experience whereby each menu item presents views in a different manner and when clicking on records, forms are organized differently. Moreover, efficiency is a key factor in these conventions, and this really means that what we present to the user should be organized in a way that makes the interaction with the system as efficient and quickly as possible to not only provide them with a better experience but also resulting in higher efficiencies which translates ultimately down to the client (the financial institutions who will be getting faster response times and more efficient reviews etc.). The list below describes the conventions for forms. Convention Title Purpose Example Table 32: {ORGANIZATION} Model Driven App Form Design Conventions The list below describes the conventions for views. Convention Title Purpose Example Table 33: {ORGANIZATION} Model Driven App Views Design Conventions Customization options and considerations In this implementation, we stress the importance of using out of the box tooling as much as possible. In only very rare cases, should a developer bring in third party libraries to enhance usability unless well supported by the community behind it or is a first party (MS supported) library. By introducing the usage of third party libraries and or controls, the more technical debt is added to the system thus adding more complexity and potential maintenance and run time exception problems in the future due to things such as depreciations not being addressed by the third party library or running into the risk of over-complicating the system just to simplify a feature for the sake of convenience. There are legitimate user experience enhancements that do require the usage of third-party libraries or web resources (custom HTML/JS resources) which are listed below. Also listed below are conventions used when customizations outside the standard OOB tooling provides is implemented. Library Name Purpose Table 34: Third/First Party Library/Controls Usage/Conventions Convention Title Purpose Example Table 35: Conventions when developing Class Libraries and Web Resources Portals (Power Pages Site) The website for this application adheres to WCAG regulations that are facilitated by the implementation of the Canada.ca WET framework implementation in the portal. This CSS/JS framework supports screen readers and other tools used by folks with an impairment in using a traditional keyboard and mouse or trackpad. This framework has been fully implemented on this web site and affects everything from tables shown to users, buttons, forms, links, and navigation. In addition to addressing accessibility, web usability and industry norms are followed so that users who interact with the portal are experiencing a familiar user experience in comparison to other frequently visited web sites with the exception of heavy usage of JavaScript as this will often violate accessibility guidelines. JavaScript is being used in the portal in many areas but our convention is to ensure that for accessibility and security reasons, JavaScript is leveraged for read operations only and rarely used for writing or purging data using the web API. Exceptions to these rules are described in the following sections. Interactive and responsive design The portal has been designed using the WET Canada.ca styling framework which doesn’t only help with achieving accessibility but also since its based off the bootstrap theme, is responsive to any device, including desktop, mobile, and tablet. The screen and content will automatically adjust based on the users’ device. Accessibility guidelines and standards The portal is implemented using the WET Canada.ca styling framework but beyond this, the build team has had to make several adjustments to the portal technology to meet the WCAG 2.1 requirements. This report is available to the communications group at {ORGANIZATION}. Figure 34: Desktop View Figure 35: Mobile View Figure 36: Tablet View Multilingual and cultural considerations The portal is implemented to support both French and English. Users also can toggle between languages and their language. They also can set the language of preference in their profile, otherwise, the behaviour is that the browser will default to the last know locale set by the user pressing the language toggle. Integration Architecture The Integration Architecture section covers integration patterns and principles, including API-based integration and connector-based integration, and the APIs and connectors used for integration, such as Dynamics 365 Web API, Microsoft Graph API, and Power Automate Connectors. Additionally, it covers data synchronization and replication, integration scenarios and patterns, data mapping and transformation, and security and authentication requirements for integration, such as OAuth and Azure AD authentication and authorization and permissions management. Integration patterns and principles. API-based integration Connector-based integration APIs and connectors used for integration. Dynamics 365 Web API Microsoft Graph API Power Automate Connectors Data synchronization and replication Integration scenarios and patterns Data mapping and transformation Security and authentication requirements for integration OAuth and Azure AD authentication Authorization and permissions management Security and Access Control The Security and Access Control section covers role-based access control (RBAC) design, user roles and permissions, hierarchical security models, Azure Active Directory integration with Dynamics 365 teams, user and group synchronization, user provisioning and de-provisioning, permissions and privileges management, security roles and profiles, privileges and access levels, authentication and authorization requirements, password policies and security standards, multi-factor authentication, and data protection and compliance considerations, such as data encryption and protection, GDPR, and data privacy regulations. Hierarchical security model (business units / teams) Azure Active Directory integration with Dynamics 365 teams (user provisioning) Azure Active Directory (Azure AD) groups can be integrated with Dynamics 365 teams to enable centralized role-based access control (RBAC) management across Dynamics 365 applications. RBAC is a security model that assigns permissions to users based on their roles within an organization. By using Azure AD groups to manage RBAC in Dynamics 365 teams, organizations can benefit from a centralized and efficient approach to managing user access to Dynamics 365 applications. Contacts (external portal users) a step-by-step explanation of how Azure AD groups integrate with Dynamics 365 teams: Create an Azure AD group: To begin, create an Azure AD group that will be used to manage access to Dynamics 365 teams. This group should include all of the users who require access to Dynamics 365 applications. Assign roles to the Azure AD group: Next, assign one or more roles to the Azure AD group that correspond to the access levels required by users. Add the Azure AD group to Dynamics 365 teams: Once the Azure AD group has been created and roles have been assigned, it can be added to Dynamics 365 teams. This will automatically assign the appropriate roles to all the users in the Azure AD group. Manage RBAC through Azure AD group membership: From this point on, RBAC can be managed centrally by managing Azure AD group membership. If a user requires access to additional Dynamics 365 applications, simply add them to the appropriate Azure AD group and assign the corresponding roles. By leveraging this feature, organizations can benefit from several advantages: Centralized management: By using Azure AD groups to manage RBAC in Dynamics 365 teams, organizations can benefit from centralized management of user access across multiple Dynamics 365 applications. Simplified administration: By managing RBAC through Azure AD group membership, administrators can more easily add or remove users from roles as needed, without having to manage individual user accounts. Increased security: By using Azure AD groups to manage RBAC, organizations can benefit from the security features of Azure AD, including multi-factor authentication, conditional access policies, and more. Increased efficiency: By centralizing RBAC management, organizations can streamline the process of managing user access across multiple Dynamics 365 applications, reducing the time and effort required for administration. In summary, Azure AD groups can be integrated with Dynamics 365 teams to enable centralized RBAC management across multiple Dynamics 365 applications. This approach provides several benefits, including centralized management, simplified administration, increased security, and increased efficiency. In the following section, the implementation of this feature is described in detail for this implementation. The goal is to leverage Active Directory to manage the RBAC of the system rather than relying solely on the application layer thus avoiding pitfalls such as stale users or users who are not assigned to the right team and permissions. User and group synchronization & User provisioning and de-provisioning The table below lists the teams, their associated security roles, and the associated Azure AD group. By adding the user to the Azure AD group, they automatically are added as members to its associated team and inherits the teams’ permissions. This applies to also removing a user from an Azure AD group and adding them to another group. Users who are disabled in AD or purged also have their access revoked from CRM. Team Security Role(s) AAD Group Table 36: Teams & AD Groups Figure 37: D365 Team Integration with AD Group (repetitive) Azure KeyVaults for storing certificates, secrets, and keys. Encryption and decryption Logging and monitoring A log analytics workspace has been provisioned in the nonproduction and production Azure subscriptions hosting the azure workloads that support the Power Platform. In this implementation, the Power Platform logs (Portal and CRM) are sent to the workspace, so are the B2C & SharePoint logs. These workspaces are scanned by the CCCS sensors and logs sent to the GOC SOC. The log analytics workspace locations are listed below. Subscription Name Workspace Name File and data storage Files (unstructured data) is mostly stored in SharePoint. This includes form related attachments and attachments to various case files, accounts, contacts, and forms. Diagnostic logs, which are files as well (JSON format) are stored in Azure Storage accounts located in the Power Platform Azure supporting subscriptions. The Dataverse also stores “web files” which are files such as CSS, JS, and images to support portal asset rendering. Azure Gateway as a proxy perimeter to the portal {ORGANIZATION} has implemented SCED thus a physical connection to the Canadian Azure Data Centres using express route. Therefore, the portal (pre-prod and production) is proxied through this service allowing {ORGANIZATION} to control the perimeter services via a series of assigned IP’s controlled by SSC and governed by the F5 firewall. This is an unconventional but working solution as Power Page Sites provides native support to Azure Front Door as a CDN / Proxy service however {ORGANIZATION} has no plans to implement front door. Network security and perimeter protection Microsoft 365 provides several network security perimeter services to its customers who use the Power Platform, including Power Apps, Power Automate, and Power BI. These services are designed to help ensure that customer data and applications are protected against security threats, both within and outside the Microsoft 365 environment. This is why SAAS is outside of the scope of SCED as GOC operators have no access to the perimeter or infrastructure hosting M365 (so the Power Platform).6 Some of the key network security perimeter services provided by Microsoft 365 include: Azure Active Directory (Azure AD) - This service provides authentication and access management for Power Platform applications, allowing customers to control who can access their data and applications, and what they can do with them. Data Loss Prevention (DLP) - DLP policies can be configured to help prevent data loss or leakage from Power Platform applications, by monitoring and controlling access to sensitive data. By default, a global DLP has been applied to the Power Platform tenant to block all third party and first party connectors to services outside of the Microsoft ecosystem. Conditional Access - This feature allows customers to set policies that control access to Power Platform applications based on a range of factors, including user location, device type, and more. Network security groups - These are firewall rules that can be used to control inbound and outbound network traffic to and from Power Platform applications, helping to prevent unauthorized access and data exfiltration. Azure Private Link - This service allows customers to securely access their Power Platform applications over a private network connection, rather than over the public internet. In addition to these network security perimeter services, Microsoft 365 also provides protection for public Power Pages sites created using Power Apps. Specifically, Power Pages sites are protected by Azure Front Door, which provides security and scalability for web applications. Azure Front Door provides several security features, such as SSL termination, DDoS protection, and bot protection, to protect against common web-based attacks. Overall, Microsoft 365 provides a comprehensive set of network security perimeter services to its customers who use the Power Platform, helping to ensure that customer data and applications are protected against security threats both within and outside the Microsoft 365 environment. Data Loss Prevention (DLP) The DLP features include policy-based controls that restrict data access and usage, data classification and labeling to identify sensitive data, and auditing and monitoring to track data usage and detect policy violations. Additionally, the platform includes built-in encryption and data retention policies to help protect data at rest and in transit. However, while these features can help prevent data leaks within the Power Platform, they do not extend to other parts of an organization's IT infrastructure. This is where Azure DLP configurations can come in, as they can provide additional layers of protection by extending DLP policies to other Microsoft and third-party services, as well as on-premises systems and endpoints. Azure DLP can also provide centralized policy management and reporting, making it easier for organizations to enforce consistent DLP policies across their entire IT ecosystem. For the initial configuration of the Power Platform, we have a global policy implementation that defaults to blocking every third-party connector and first party connectors (developed by Microsoft) that do not talk to Microsoft product. Every time a new Dataverse environment is requested, the requester must issue a formal request to except one or more connectors they will require for their project. In our case, we need to leverage the HTTP Connector to be able perform API calls to the Organization Master API. Here are the steps to create a DLP policy in the Power Platform: Sign into the Power Platform Admin center with your administrator account (must be G.A, or Power Platform Administrator). In the left-hand navigation pane, select \"DLP policies\". Click the \"+ Create policy\" button at the top of the page. On the \"Create DLP policy\" page, enter a name and description for the policy. Choose the type of data that the policy will protect. This can include Common Data Service (CDS) entities, canvas apps, flows, and connectors. Configure the policy rules to define what actions are allowed or blocked for the selected data types. For example, you can specify that certain users or groups are not allowed to export or print data, or that sensitive data cannot be shared outside of the organization. Set up notifications to alert users or administrators when a policy violation occurs. You can choose to send an email notification, display a message, or both. Review and save the policy. Once the policy is saved, it will be applied globally to all environments within your organization's tenant. Note that some of the policy settings may take up to 24 hours to take effect. Additionally, it's important to test your policies thoroughly before enforcing them to ensure that they don't inadvertently block legitimate activities or workflows. To create the Azure DLP policy, as the above can be circumvented easily by developers who use class libraries (Plugins/workflows), the general steps to create a DLP (Data Loss Prevention) policy in Azure: Log in to the Azure portal. Go to the Azure Information Protection service. Click on \"Create a label\" and create a label that will be used in your policy. Click on \"Create a policy\" and give your policy a name (Dataverse DLP Global). Select the label you created in step 3 and configure the policy rules. Define the conditions for the policy, such as the location of the data or the type of data. Define the actions to be taken when the policy is triggered, such as blocking access to the data or sending a notification. Review and save the policy. Once you have created the policy, you can assign it to a specific user or group, or to a specific location or device. You can also monitor the policy to see if it is working as expected and to make any necessary adjustments. Traffic management and load balancing Due to the nature of the system being a SAAS technology – traffic management and load balancing is abstracted by Microsoft 365. Therefore, {ORGANIZATION} does not need to configure these features. If the applications are behaving slowly, it is typically due to mis configurations such as to many synchronous processes running which is a violation of our conventions and easy to identify by viewing the process logs. If the performance issues are difficult to identify, a ticket can be opened with Microsoft to help identify and resolve the issue. Azure KeyVaults for Certificates, Secrets, and Keys An Azure KeyVaults instance has been provisioned for both nonproduction and production Subscriptions that host the Azure Workloads that support the Power Platform. In these KeyVaults is where we store the application App Registration secrets, TLS certificates, private IP addresses and other sensitive information such as DevOps library variable groups. Access to the KeyVaults is constrained to subscription contributors only thus developers do not have access to view this information, they must request temporary access to access a key, secret or certificate if needed for their development work. The table below lists the secret names stored in KeyVaults (not values). KeyVault Artifact Name Purpose Subscription Table 37: Azure KeyVault Artifacts KeyVaults are especially useful to protect our pipeline variable secrets. The diagram below depicts how this connection works, how variable groups connect to KeyVaults to obtain the data required to issue deployments. Figure 38: DevOps Variable Group integration with KeyVaults In this flow chart, the Azure DevOps variable group (represented by the node A) is integrated with a Key Vault (represented by the node B). The flow starts with the variable group, which needs to retrieve secrets from the Key Vault to set the values of its variables. This is done by getting secrets from the Key Vault (represented by the node C), which are then used to set the variable values in the Azure DevOps variable group (represented by the node D). Overall, this flow chart illustrates the basic steps involved in integrating Azure DevOps variable groups with a Key Vault to store and reference their values. Azure DevOps (GIT & CI/CD) Azure DevOps is a powerful platform that can be utilized to automate the deployment of Dynamics 365 solutions, reference data, and portals (PowerApps) using the Power Platform CLI and marketplace helpers. The Power Platform CLI is a command-line interface tool that allows developers to manage and automate the deployment of Power Platform resources, while marketplace helpers are pre-built scripts that automate common deployment scenarios. By integrating these tools into Azure DevOps pipelines, developers can easily deploy and test Dynamics 365 solutions, reference data, and portals with a few simple commands. In addition to simplifying the deployment process, utilizing Azure DevOps pipelines for deployment offers several benefits over manual deployments in Dynamics. Firstly, it allows for greater control and visibility over the deployment process, as developers can easily track changes and errors in the deployment pipeline. This ensures that any issues are identified and resolved quickly, reducing the risk of errors or misconfigurations in downstream environments. Furthermore, by automating the deployment process, developers can save time and reduce the risk of human error, allowing them to focus on other important tasks. With the implementation's strict policy to only allow system administrator privileges to developers in a developer environment, using Azure DevOps pipelines to deploy to a staging environment ensures that only approved changes are promoted to downstream environments. The pipeline outputs errors and misconfigurations from the assets the developer is trying to deploy, ensuring that only valid and functioning changes are deployed to downstream environments. This approach ensures a controlled and consistent deployment process, reducing the risk of errors and minimizing downtime. Once successfully deployed to staging, a release manager can issue the release of the artifacts generated from the successful release to staging (build) to other downstream environments such as QA, UAT, PREPROD, and PROD, ensuring that the deployment process is repeatable and scalable. This section discusses various design paradigms for the application layer, including schema, naming conventions, processes (automations), business rules, Power Automate Flows, and Role-Based Access Control. It also covers the APIs and connectors used for integration. Developer main tools CLI Developer tooling required for this implementation is primarily the CLI extension in VSCode. \\ Ensure the following tooling is installed using these commands. > pac tool list ToolName Installed Version Nuget Status CMT No N/A 9.1.0.80 not yet installed; 'pac tool CMT' will install on first launch PD No N/A 9.1.0.104 not yet installed; 'pac tool PD' will install on first launch PRT Yes 9.1.0.155 9.1.0.155 ok Follow the same procedure to download and launch the CMT and PD tools. If a tool is already installed, the pac tool <toolname> command will simply launch the latest installed version of the tool. You need to periodically update these tools using the following commands > pac tool list ToolName Installed Version Nuget Status CMT No N/A 9.1.0.80 not yet installed; 'pac tool CMT' will install on first launch PD No N/A 9.1.0.104 not yet installed; 'pac tool PD' will install on first launch PRT Yes 9.1.0.155 9.1.0.155 ok > pac solution pack help Help: Package solution components on local filesystem into solution.zip (SolutionPackager) Commands: Usage: pac solution pack --zipfile [--folder] [--packagetype] [--log] [--errorlevel] [--singleComponent] [--allowDelete] [--allowWrite] [--clobber] [--map] [--sourceLoc] [--localize] [--useLcid] [--useUnmanagedFileForMissingManaged] [--disablePluginRemap] [--processCanvasApps] --zipfile The full path to the solution ZIP file (alias: -z) --folder The path to the root folder on the local filesystem. When unpacking/extractins, this will be written to, when packing this will be read from. (alias: -f) --packagetype When unpacking/extracting, use to specify dual Managed and Unmanaged operation. When packing, use to specify Managed or Unmanaged from a previous unpack 'Both'. Can be: 'Unmanaged', 'Managed' or 'Both'; default: 'Unmanaged' (alias: -p) --log The path to the log file. (alias: -l) --errorlevel Minimum logging level for log output [Verbose|Info|Warning|Error|Off]; default: Info (alias: -e) --singleComponent Only perform action on a single component type [WebResource|Plugin|Workflow|None]; default: None. (alias: -sc) --allowDelete Dictates if delete operations may occur; default: false. (alias: -ad) --allowWrite Dictates if write operations may occur; default: false. (alias: -aw) --clobber Enables that files marked read-only can be deleted or overwritten; default: false. (alias: -c) --map The full path to a mapping xml file from which to read component folders to pack. (alias: -m) --sourceLoc Generates a template resource file. Valid only on Extract. Possible Values are auto or an LCID/ISO code of the language you wish to export. When Present, this will extract the string resources from the given locale as a neutral .resx. If auto or just the long or short form of the switch is specified the base locale for the solution will be used. (alias: -src) --localize Extract or merge all string resources into .resx files. (alias: -loc) --useLcid Use LCID's (1033) rather than ISO codes (en-US) for language files. (alias: -lcid) --useUnmanagedFileForMissingManaged Use the same XML source file when packaging for Managed and only Unmanaged XML file is found; applies to AppModuleSiteMap, AppModuleMap, FormXml files (alias: -same) --disablePluginRemap Disabled plug-in fully qualified type name remapping. default: false (alias: -dpm) You can use the Table definition browser to view information for all the tables your Dataverse environment. The Table definition browser is a managed solution you can download here: Microsoft Downloads: MetadataBrowser_3_0_0_5_managed.zip After you download the solution, you must import it to be able to use it. Sign into Power Apps. In the left navigation pane, select Solutions, and then select Import on the command bar. On the Import a solution page, select Browse to locate the solution file (.zip) you downloaded, and select it. Select Next. Information about the solution is displayed. Select Import, and then finish the import process. After you import the solution successfully, locate the app by selecting Apps in the left navigation pane; the app is listed as Metadata Tools. On opening the app, Entities is the default view that lets you view all the tables. Entitles view. You can perform the following actions: View Entity Details: Select a table to view using the Entity Metadata view. Edit Entity: Open the selected form in the default organization, if the table supports this. Text Search: Perform a text search to filter displayed tables using the following table properties: SchemaName, LogicalName, DisplayName, ObjectTypeCode, or MetadataId. Filter Entities: Set simple criteria to view a sub-set of tables. All criteria are evaluated using AND logic. Filter Properties: Filter the properties displayed for any selected table. There are nearly 100 properties in the list. Use this to select just the ones you are interested in. Entity Metadata view Select Entity Metadata to inspect individual tables. Metadata view. You can perform the following actions for a single table: Entity: Select the table from the drop-down list that you want to view. Properties: View all the properties for the table and filter the properties displayed. Edit Entity: Open the selected table edit form in the default organization if the table supports this. Filter Properties: Filter the properties displayed for any selected table. There are nearly 100 properties in the list. Use this to select just the ones you are interested in. Attributes: View the table columns in a master/detail view. With this view you can: Edit Attribute: Open the selected attribute form in the default organization if the attribute supports this. Text Search: Perform a text search to filter displayed columns using the following attribute properties: SchemaName, LogicalName, DisplayName, or MetadataId. Filter Attributes: Filter columns by any attribute property values. Filter Properties: Filter the properties displayed for the selected attribute. Keys: If alternate keys are enabled for a table, you can examine how they are configured. Relationships: View the three types of table relationships: One-To-Many, Many-To-One, and Many-To-Many. With these views you can: Edit Relationship: Open the selected relationship form in the default organization if the relationship supports this. Text Search: Perform a text search to filter displayed relationships using values relevant to the type of relationship. Filter Properties: Filter the relationship by any relationship property value. Privileges: View table privileges. With this view you can: Filter the displayed privilege using the PrivilegeId. Tables are defined by table definitions. By defining or changing the table definitions, you can control the capabilities of a table. To view the table definitions for your environment, use the metadata browser. Download the table definitions browser. More information: Browse table definitions for your environment This topic is about how to work with tables programmatically. To work with tables in Power Apps See Tables in Dataverse. Tables can be created using either the organization service or the Web API. The following information can be applied to both. With the organization service you will use the EntityMetadata class. More information: Create a custom table using code and Retrieve, update, and delete tables With the Web API you will use the EntityMetadata EntityType. More information : Create and update table definitions using the Web API. Table definitions operations How you work with table definitions depends on which service you use. Since the Web API is a RESTful endpoint, it uses a different way to create, retrieve, update, and delete table definitions. Use POST, GET, PUT, and DELETE HTTP verbs to work with table definitions entity types. More information : Create and update table definitions using the Web API. One exception to this is the RetrieveMetadataChanges Function provides a way to compose table definitions query and track changes over time. If working with Organization Service, use RetrieveMetadataChangesRequest class. This class contains the data that is needed to retrieve a collection of table definitions records that satisfy the specified criteria. The RetrieveMetadataChangesResponse returns a timestamp value that can be used with this request at a later time to return information about how table definitions has changed since the last request. Message Web API SDK Assembly CreateEntity Use a POST request to send data to create a table. CreateEntityRequest DeleteEntity Use a DELETE request to delete a table. DeleteEntityRequest RetrieveAllEntities Use GET request to retrieve table data. RetrieveAllEntitiesRequest RetrieveEntity RetrieveEntity Function RetrieveEntityRequest UpdateEntity Use a PUT request to update a table. UpdateEntityRequest RetrieveMetadataChanges Used together with objects in the Microsoft.Xrm.Sdk.Metadata.Query namespace to create a query to efficiently retrieve and detect changes to specific table definitions. More information: Retrieve and detect changes to table definitions. RetrieveMetadataChanges Function RetrieveMetadataChangesRequest Options available when you create a custom table The following lists the options that are available when you create a custom table. You can only set these properties when you create a custom table. Option Description Create as custom activity You can create a table that is an activity by setting the IsActivity property when using the organization service or Web API respectively. For more information, see Custom Activities in Dynamics 365. Table Names There are two types of names, and both must have a customization prefix: LogicalName: Name that is the version of the table name that is set in all lowercase letters. SchemaName: Name that will be used to create the database tables. This name can be mixed case. The casing that you use sets the name of the object generated for programming with strong types or when you use the REST endpoint. Note: If the logical name differs from the schema name, the schema name will override the value that you set for the logical name. When a table is created in the application in the context of a specific solution, the customization prefix used is the one set for the Publisher of the solution. When a table is created programmatically, you can set the customization prefix to a string that is between two and eight characters in length, all alphanumeric characters and it must start with a letter. It cannot start with “mscrm”. The best practice is to use the customization prefix defined by the publisher that the solution is associated with, but this is not a requirement. An underscore character must be included between the customization prefix and the logical or schema name. Ownership Use the OwnershipType property to set this. Use the OwnershipTypes enumeration or OwnershipTypes EnumType to set the type of table ownership. The only valid values for custom tables are OrgOwned or UserOwned. For more information, see Table Ownership. Primary Column With the Organization service, use CreateEntityRequest.PrimaryAttribute property to set this. With the Web API the JSON defining the table must include one StringAttributeMetadata with the IsPrimaryName property set to true. In both cases string column must be formatted as Text. The value of this column is what is shown in a lookup for any related tables. Therefore, the value of the column should represent a name for the record. Enable table capabilities. The following lists table capabilities. You can set these capabilities when you create a table or you can enable them later. Once enabled, these capabilities cannot be disabled. Capability Description Business Process flows Set IsBusinessProcessEnabled to true in order to enable the table for business process flows. Notes To create a relationship with the Annotation table and enable the inclusion of a Notes area in the form. By including Notes, you can also add attachments to records. With the Organization service, use the CreateEntityRequest or UpdateEntityRequest HasNotes property With the Web API set the EntityMetadata.HasNotes property. Activities To create a relationship with the ActivityPointer table so that all the activity type tables can be associated with this table. With the Organization service use the CreateEntityRequest or UpdateEntityRequest HasActivities property. With the Web API, set the EntityMetadata.HasActivities property. Connections To enable creating connection records to associate this table with other connection tables set the IsConnectionsEnabled.Value property value to true. Queues Use the IsValidForQueue property to add support for queues. When you enable this option, you can also set the AutoRouteToOwnerQueue property to automatically move records to the owner’s default queue when a record of this type is created or assigned. E-mail Set the IsActivityParty property so that you can send e-mail to an e-mail address in this type of record. Editable table properties The following lists table properties that you can edit. Unless a managed property disallows these options, you can update them at any time. Property Description Allow Quick Create Use IsQuickCreateEnabled to enable quick create forms for the table. Before you can use quick create forms you must first create and publish a quick create form. Note: Activity tables do not support quick create forms. Access Teams Use AutoCreateAccessTeams to enable the table for access teams. See About collaborating with team templates for more information. Primary Image If a table has an image column you can enable or disable displaying that image in the application using PrimaryImageAttribute. For more information see Image columns Change display text The managed property IsRenameable prevents the display name from being changed in the application. You can still programmatically change the labels by updating the DisplayName and DisplayCollectionName properties. Edit the table Description The managed property IsRenameable prevents the table description from being changed in the application. You can still programmatically change the labels by updating the Description property. Enable for use while offline Use IsAvailableOffline to enable or disable the ability of Dynamics 365 for Microsoft Office Outlook with Offline Access users to take data for this table offline. Enable the Outlook Reading Pane Note: The IsReadingPaneEnabled property is for internal use only. To enable or disable the ability of Office Outlook users to view data for this table, use the Outlook reading pane. You must set this property in the application. Enable Mail Merge Use IsMailMergeEnabled to enable or disable the ability to generate Office Word merged documents that use data from this table. Enable Duplicate Detection Use IsDuplicateDetectionEnabled to enable or disable duplicate detection for the table. For more information, see Detect duplicate data using code Enable SharePoint Integration Use IsDocumentManagementEnabled to enable or disable SharePoint server integration for the table. For more information, see Enable SharePoint document management for specific entities. Enable Dynamics 365 for phones Use IsVisibleInMobile to enable or disable the ability of Dynamics 365 for phones users to see data for this table. Dynamics 365 for tablets Use IsVisibleInMobileClient to enable or disable the ability of Dynamics 365 for tablets users to see data for this table. If the table is available for Dynamics 365 for tablets you can use IsReadOnlyInMobileClient to specify that the data for the record is read-only. Enable Auditing Use IsAuditEnabled to enable or disable auditing for the table. For more information, see Configure table and columns for Auditing. Change areas that display the table You can control where table grids appear in the application Navigation Pane. This is controlled by the SiteMap. Add or Remove Columns As long as the managed property CanCreateAttributes.Value allows for creating columns, you can add columns to the table. For more information, see Column definitions. Add or Remove Views As long as the managed property CanCreateViews.Value allows for creating views, you can use the SavedQuery table to create views for a table. Add or Remove Charts As long as the managed property CanCreateCharts.Value allows for creating charts and the IsEnabledForCharts table property is true, you can use the SavedQueryVisualization table to create charts for a table. For more information, see View data with visualizations (charts). Add or Remove table relationships There are several managed properties that control the types of relationships that you can create for a table. For more information, see Table relationship definitions. Change Icons You can change the icons used for custom tables. For more information, see Change model-driven app custom table icons Can Change Hierarchical Relationship CanChangeHierarchicalRelationship.Value controls whether the hierarchical state of relationships included in your managed solutions can be changed. Messages supported by custom tables. Custom tables support the same base messages as system tables. The set of messages available depends on whether the custom table is user-owned, or organization owned. User-owned tables support sharing, so messages such as GrantAccess, ModifyAccess, and RevokeAccess are available. Best practices and guidance when using Microsoft Dataverse Microsoft Dataverse provides an extensible framework that will allow developers to build highly customized and tailored experiences. While customizing, extending, or integrating with Dataverse, a developer should be aware of the established guidance and best practices. Within this section you will learn about the issues we have identified, their impact, and guidance to resolve those issues. We will explain the background about why things should be done in a certain way and avoid potential problems in the future. This can benefit the usability, supportability, and performance of your environment. The guidance documentation supports the existing information within the Developer and Administration guides. Targeted customization types The documentation targets the following customization types: Custom workflow activities and plug-ins Working with Dataverse data Integrations extending Dataverse. Sections Each guidance article includes most or all the following sections: Title - description of the guidance Category - one or more areas impacted by not following the guidance. Impact potential - the level of risk (high, medium, or low) of affecting the environment by not following the guidance. Symptoms - possible indications that the guidance has not been followed. Guidance - recommendations that may also include examples. Problematic patterns - description or examples of not following the guidance. Additional information - supporting details for a more extensive view. See also - references to learn more about something mentioned in the article. Categories Each guidance article is classified with one or more of the following categories: Usage – improper usage of a particular API, pattern, or configuration Improper usage of a particular API, pattern, or configuration are re../images/SDD/ted by providing the build team with the necessary training in system design document that out lines every convention they must adhere to. Failure to adhere to these conventions, their work will not be deployable by our pipeline as some of its tasks is to verify to adherence to our conventions. Design – design flaws in a customization Design flaws and customizations are re../images/SDD/ted by providing the build team with the necessary training in system design document that out lines every convention they must adhere to. Failure to adhere to these conventions, their work will not be deployable by our pipeline as some of its tasks is to verify to adherence to our conventions. Performance – customization or pattern that may produce a negative effect on performance in areas such as memory management, CPU utilization, network traffic, or user experience. We have strict convention that all of our processes which include actions, workflows and flows run async with the exception of run time validation rules (not always). By leveraging async workflows on the server and async functions in JScript on the portal, we prevent the blocking of “threads” which is the cause of poor performance the majority of the time with most systems. Otherwise, due to the nature of this system being a SAAS, it is Microsoft’s legal obligation to meet industry standards for the lower level networking performance paradigms. Security – potential vulnerabilities in a customization that could be exploited in a runtime environment. We recommend that a ftnc_power_platform_administrators security group with PIM privileges to associate the Power Platform Administrator, Dynamics, Administrator, and the Application Administrator be created. Only certified Power Platform engineers and Azure Architects should be allowed to be granted this privilege. This same group should be able to activate contributor privileges to the support Azure subscription hosting the Power Platform supporting workload suing PIM as well. Upgrade Readiness - customization or pattern that may increase risk of having an unsuccessful version upgrade. All environments are set to not automatically automatic when a new “Wave” is released by Microsoft. Microsoft will release a wave in preview for interested developers to try its new feature set and send analytics and bug reports to Microsoft in preparation for the general availability release. This typically happens 2 or 3 times per year, and to prepare, {ORGANIZATION} provisioned a SANBOX instead that is in the scope of the pipeline automation thus will be in the same state as QA/UAT at all times. This is important so that when the administrator upgrades the portal to the review version in SANDBOX. Subsequently, a person or a team will run our test cases, the automated and manual ones to identify and prepare for inevitable upgrade which Microsoft does not force upon its clint base for years although, {ORGANIZATION} may be interested in new features thus this exercise should take place each preview released. Maintainability – customization that unnecessarily increases the amount of developer effort required to make changes, the frequency of required changes, or the chance of introducing regressions. A defined and dedicated scheduled releases will be aligned with the platforms and its supporting artifacts coupled with the needs and obligations of {ORGANIZATION} to meet its mandate to regulate financial institutions. The next section will describe the process that has been implemented to make changes and test the system in a mature standard SDLC process that aligns with ISO an ITSG Supportability – customization or pattern that falls outside the boundaries of published supportability statements, including usage of removed APIs or implementation of forbidden techniques. We describe later in this section the libraries we use that are considered as “third party”. However, these libraries are used by most if not all CMR implementation and are supported and maintained by Microsoft MVP’s. We have a well-defined list of these libraries and API’s that our implementation can leverage to enhance the user experience for both end users (internal and external). Its important to note that this list is mutable and subject to change as the platform grows and more features are added and perhaps more apps added. However this process will need to align with our existing process of evulating the API or third party tool we are bringning couple with a justification. If approved, it is then tadded to the lst and developers who atttemp to circumvent this policy will not have a technical ability to release their new artifact as the pipeline is referencing the approved list of automation tools being used (approved by security) and anything falling outside of this list stops the pipeline and instructs the developer to seek approval for their new library or API endpoint. Release Pipeline The release pipeline for this project is implemented in Azure DevOps. Is comprised of not only automated release pipelines but our repository of requirements and version control using a GIT repository. There are two key pillars of the pipeline, one being the frequently executed from entitled “{ORGANIZATION}-CI” (CI=Continuous integration) and the second being the actual releases to all downstream environments up to production. The CI pipeline gives flexibility to developers to release at a staging environment at will to test their builds and store their artifacts into a release. Come time for a deployment to UAT and beyond, developers can choose any CI release and issue the release to the desired environment where tech leads are responsible for approving. This process is visualized below in both diagrams, one describing the GIT repository process and the second the release pipeline. Figure 39: Release Pipeline - releases (CI) In this diagram, the following pillars are key to each step to the pipeline: Source Environment: The environment from which the Developer-specified solution patch is fetched. Unpack Solution: A step in the pipeline that unpacks the solution or patch. Repack Solution: A step in the pipeline that repacks the solution or patch as a zip file. Reference Data Migration: A step in the pipeline that migrates reference data, if needed. Create Snapshot: A step in the pipeline that creates a manual snapshot of the existing target. Here's a description of the latest pipeline: The Developer specifies the solution patch via an artifact variable. Azure DevOps fetches the solution patch from the Source Environment. Azure DevOps runs the Solution Checker, Power Portal Checker, and Custom Scripts. Azure DevOps unpacks the solution or patch. Azure DevOps stores the results in the GIT Repository associated with the project. Azure DevOps repacks the solution or patch as a zip file and manages it for deployment to the target. Azure DevOps deploys the solution or patch to Production, subject to an Approval Gate. Production reviews the changes and either approves or rejects them. If the Developer has specified that reference data needs to be migrated, Azure DevOps migrates the reference data. Azure DevOps creates a manual snapshot of the existing target. Azure DevOps notifies the Developer of the Approval result. 6a Once a successful build is ready to de deployed to downstream environments up to production, the developer will issue a release directly from the successful pipeline execution. This process is further illustrated below. Figure 40: Deployment to Downstream Environments (CD) Build and Test Automation The process of building solutions for this implementation involves creating a master solution entitled “{ORGANIZATION}” and developers will provision patches off this solution to perform their work. Once they’ve made their development and configuration changes inside their patch, they are required to deploy this to a staging environment using the {ORGANIZATION}-CI pipeline to test their work. If successful, they can issue the release from the pipeline release’s generated artifacts. This applies to both solution patches and portal changes. For class libraries which are C# .net framework based, the need to commit their code to the repository and register their plugin or custom workflow step using the “plugin registration” tool available in the CLI using the following command: pac tool prt This will launch the tool and the developer must authenticate to the tool using their AAD account and register the plugin and their steps (and images when required). Furthermore, plugins must be signed by a developer key to avoid de-compilation by a nefarious actor. The table below lists the plugins developed specifically for this implementation – for now, the only plugin required is to enhance the bilingualism behavior of lookup fields on CRM form and views. Plugin Name Class Library Table 38: Class Libraries (custom) Once a plugin or custom workflow step is registered to the development environment, the developer is responsible to add it to their patch and deploy via the pipeline. Under no circumstances a developer will manually register plugins or workflow steps to any “managed” environment, only in development. Deployment Automation Developers have access to the COMPLIANCE-PATCH pipeline whereby they can release their patches, configuration data and portal configurations to the Staging environment (managed). Patches do not need to be committed by the develop to the repository, the only responsibility for the developer is to provide the logical name (not display name) of the patch or solution being deployed and provide a version number that is increment by a number (4th integer) lower than a patch whose been provisioned later than the one being deployed. For data files being transferred, the developer is responsible to generate the schema file using the configuration migration tool from the SKD and commit the xml file into the Data/Schema directory. Once committed, the developer will be responsible for providing the filename and extensions (always filename.xml) and the pipeline will handle the export from source and import to target. Developers can run up to 4 artefacts containing both solutions and data schema files and these artifacts will run in order. Furthermore, the unmanaged solutions will be installed always as managed in the target environment as per best practice and to avoid future major issues with out of sync environments. Finally, developers also can deploy their latest portal changes by setting “Yes” in the “Deploy Portal (Yes/No)” variable (which is set to know by default. The Comments parameter is leveraged for populating a git commit message as the pipeline will commit all artifacts to the repo and if no comment is found a default one is set which concatenates the personas name and time stamp. The pipeline will also automatically produce a backup of the target environment before deployment to recover for disaster recovery. This backup is stored in the same Azure Storage account that stores the portal diagnostics and has a 30 day retention policy. A pipeline is also available to developers to restore the backup if the deployment has caused an issue that is irreparable. The table and figure below outline the pipelines and their variables. Pipeline Name Variable Permitted Executor Role Example deployment run configuration (TODO -> screen cap of same pipeline (once I can access network) Figure 41: CI Pipeline Variables The figure below illustrates the automation of our deployment process. This aligns with Microsoft’s recommendations with the exception of our specific environment implementation which includes more emphasis of testing for conventions, mis-configurations, an the environment backup snapshot we trigger as the first state of the pipeline for disaster recovery (which is stored and retained in our Azure Storage account as per Microsoft recommendation – this lowers our capacity costs). Figure 42: ALM Illustration Build tools Azure DevOps is the exclusive version control (GIT) and deployment tool for our build. However, the pipeline developed for this uses the Power Platform Build Tools first party plugin in DevOps in conjunction with the Power Platform Command Line Interface (CLI) for more flexibility. When writing a CLI script in a pipeline step, we use PowerShell instead of Bash, as we use a Windows Server (latest) image to host our agent. The CLI support also takes precedence over UNIX based OS support unfortunately (for now) therefore we do have some limitations around path length and some of the GIT capabilities you get with a Linux agent (in particular path lengths). In a later phase of this project, we plan on moving our ALM to a Linux based agent as it’s a well supported operating system for the web and pipelines. However, since we are using the CLI this is not really an issue in this implementation as the OS is abstracted from developer and the pipeline ALM (CICD) is working flawlessly. Its important to note that the DevOps project is linked to a subscription has we are running parallel jobs (up to 2 at time) – instead of incremental stages. The free tier of DevOps gives us 5 free basic licensed users and 1 free hosted agent pool agent job. The CLI is comprised of the following commands, when developers clone or pull the current repository version of our code base for this implementation, they are recommended to use VSCode with the Power Platform Build Tools extensions installed in VSCode to run the commands to grab the latest version of the code, download, unpack, test, and re-back their patches. They also use the CLI to invoke tools such ss the plugin registration tool (for C# class libraries to publish plugins and custom workflow steps), and the configuration migration utility (rarely as they need to simply update the existing schema (XML file) in the repository to include to new tables and for fields that are considered as reference data or dependent data for a process they are building (e.g. a business rule may depend on the existence of a country record to exist thus this record must live in target otherwise wont activate. All this work happens within their own branch, and they are required to merge their work into our main development branch, resolve merge conflicts (if any) then issue a pull-request with appropriate comments and link to work items and approve and auto complete themselves (only for development branch). Once done, they run the {ORGANIZATION}-PATCH pipeline and provide the variable values of what they are deploying, and this will run the entire pipeline tasks including all tests and provide logging. This pipeline only deploys to staging, which is an environment that developers only have access to test their deployments and create artifacts of their release for an actual release to downstream environments up to production. These releases are managed by the release manager and there is a separation of duty whereby the releaser cannot approve his / her own release, this must be completed by someone else on the team. Releases have a retention policy of 30 days (the same applies to release artifacts stored in DevOps). Note, and this must be emphasized, our pipeline should be able to build an environment from scratch and provide a simple tool for developers to avoid making common mistakes and create security vulnerabilities. {ORGANIZATION} must ensure to closely monitor the group of folks who have system administrator access in any other environment than development. Under no circumstances will system administrative privileges will be granted to managed environment (anything other than dev) as deployments can also be done manually (like any other custom framework or COTS) and therefore we’ve created the group ftnc_compliance_dev_administrators who belong to the system administrator team in development via the AAD Group integration with CRM team (explained in this document). The log analytics workspace will log security role assignments and if this scenario takes place, a high incident will be alerted to the appropriate folks to take action. There are rare scenarios whereby system administrator privileges are required to issue quick fixes or diagnose an issue post deploy but in these scenarios, these folks must PIM themselves in the ftnc_compliance-{env}-administrators group (4 hour limit) Below is another more elaborate illustration of this process. Furthermore, developers will be using the CLI to invoke the Plugin Registration Tool, Configuration Migration Utility and the package deployer using the following commands: Repositories (GIT) A GIT repository is used to source control our CRM solutions, reference data schema files (e.g., languages, genders, countries, teams, orgs, and individuals. Below illustrates the repo structure and the YAML and PS1 script files to support the pipeline (CI/CD). It’s important note that for security reasons, the .gitignore file exclude environment variables stores in text and JSON files. This means that the pipeline(s) operating. Agent which is hosted in our Azure subscription rather than using the default hosted agent. The agent specifications are provided in this table: Agent Name Operating System Specifications (capacity/threads) Table 39: Custom Agents/Hosted Pools The repository folder structure is the convention we are applying for any Power Platform project although some may not require canvas and portal apps. Branching Strategy / Code Review Process For continuous integration, developers have the freedom to deploy their pipeline artifacts (if it passes our solution and portal checker tasks in the pipeline. If it fails, developers are alerted via email with an anchor tag to view the verbose log of the failure reason and then act (e.g., make the necessary fixes/add missing dependencies etc. and re-deploy). This is the extent to which developers can deploy and test using staging. One developer successfully deploys to staging he / she can appeal to the project manager or release to other downstream environments provided the tech lead reviews and approves the release (mandatory 1 approval). Branches will be assigned to each developer and will be cloned from Development. Even though we are using the same environment / code base, we will be leveraging patches for this mapping and each developer will have his or her own developer portal type development to avoid / minimize conflicts. (SECOND SCENARIO BE AUTHORED) Table 40: Branch mapping strategy (Patching) Service Connections Service Connections in Azure DevOps provide a secure and efficient way to manage connections to external resources, including Dataverse environments. By creating a Service Connection for Dataverse, you can easily reference and manage configurations across different Dataverse environments in your pipelines and other DevOps resources. To use Service Connections for Dataverse, you will need to create a new connection and enter the required connection details, such as the URL of your Dataverse environment and your user credentials. Once you have created the Service Connection, you can use it in your pipelines to deploy and manage configurations across multiple Dataverse environments, such as plugins, workflows, and data synchronization. Using Service Connections in this way helps to ensure that your pipeline executions are secure and efficient, by managing the authentication and authorization details required to access your Dataverse environments in a centralized and reusable way. This approach also makes it easier to manage changes to your Dataverse environments over time, by allowing you to deploy and manage configurations consistently across multiple environments. The table below lists each service connection used by the release pipeline and their type. Service Connection Name Type {ORGANIZATION}-DEV Dataverse {ORGANIZATION}-STAGING Dataverse {ORGANIZATION}-QA Dataverse {ORGANIZATION}-UAT Dataverse {ORGANIZATION}-PREPROD Dataverse {ORGANIZATION}-TRAINING Dataverse {ORGANIZATION}-SANDBOX Dataverse Azure AD Service Account Connection for Power Automate Flows The {ORGANIZATION}-SYSTEM App Registration (App ID: ) is leveraged as a service account to authorize Flows to communicate with its Dataverse environment to perform automation. First-Party Microsoft Plugin to Use CLI Commands in the YAML File for the Pipeline Design To simplify the developer experience when building and optimizing the pipeline, the Power Platform Build Tools library from the DevOps Marketplace has been installed in the 139fc DevOps organization and being use for the Compliance Case Management Project’s pipeline automation scripts. Agent Pools & Self Hosted Agents (future state) In this current implementation, the Azure Pipelines built in Agent is leveraged to host the infrastructure required to run the pipelines. However, {ORGANIZATION} is implementing a self hosted agent which is further described below: The self-hosted agent acts as an inter../images/SDD/ry between the Developer and the CICD tool, performing builds and deployments as necessary. Benefits: Greater control and customization over the build and deployment process. Increased flexibility in terms of where and how builds and deployments are performed. Ability to use custom hardware and software configurations to meet specific needs. Potential issues: Requires additional hardware and infrastructure to maintain. May require additional setup and configuration time. Can introduce additional security concerns if not properly secured. In general, self-hosted agents can be a great choice for teams that need greater control and flexibility over their build and deployment processes. However, they require additional resources and maintenance, so teams should carefully weigh the benefits and potential issues before deciding to use them. Table 41: Self Hosted Agents (Detailed) Technical Documentation Process Microsoft DocFx has been implemented to host our build book, solution architecture documents, training guides (for various personas) in our private Storage Account (blob). Developers are responsible for updating our documentation each major release and thus must be part of their pull-request post staging (releases) otherwise the PR will be rejected. The pipeline will automatically update the DocFx static site hosted in Azure Storage. Since this storage account is private only and also has a requirement for users to authorize to Active Directory, {ORGANIZATION}’s RBAC (Access Control Policies) are inherited. The figure below illustrates the example home page of the documentation site. Its important to note that DocFx is the framework used by Microsoft for their documentation and is based on Markdown. It also provides the ability to export all documents to PDF for extended stakeholders. For now, any {ORGANIZATION} employee can access the documentation site only once they are on a {ORGANIZATION} device and authenticated to VPN and the CA. The benefit here is that this enterprise and free open-sourced corporate backed framework for technical documentation makes it extremely simply for staff to search anything and navigate and since its baked into our SDLC process, this documentation, unlike word or other types of static documents has less chance or updated. It also helps with on boarding users by providing them with material out of the gate to ingest. In this first implementation, the internal DocFx will be hosted in our strage account in a private (no anonymous acc [ }BB>’gp[l,ss with This will further refine by video generated content hosted in this site as well and tutorials (future phase). Figure 43: Document Static Site (Azure Storage) - DocFx Cost Management Power Platform ships with a billing policy feature allowing {ORGANIZATION} to link one or more of its environments to pay as you go subscriptions. This makes the process of tracking who to recovery costs from internally much simpler. A billing policy has been created for both the ftnc_complince_dev and ftnc compliance_prog subscription. This is beneficial not only for reporting and cost recovery purposes but enables less restrictions on creating new capacity tied to the subscription and must notify a CSP for more expensive capacity for GB. Where’s our capacity is based on the number of users we have and the type of licensing. The number of users automatically elevates the capacity of the Power Platform tenant depending on the licensing purchased per use r. In our implementation we’ve purchased Dynamics 365 Case Management (Customer Service Processioanl licenses) and 10 enterprise customer service licenses for tech leads and developers allowing providing them with the ability to MVP or showcase features not available in the regular license and access to more development tools that can benefit the system. For Power Pages Sites, the billing model is capacity based on active daily sessions and unique impressions to the website. {ORGANIZATION} can purchase packages of these annually and if the capacities fall short of the pre purchased plan, the amount is re-allocated to the following fiscal year thus lowering the costs. The same applies in the reverse scenario. Monitoring Costs and Usage: In the ftnc_compliance_{env} solutions, the budgets management feature will be implemented to provide stakeholders with, pass, present and future anticipated costs for using the Dataverse and its related services. {ORGANIZATION} will need to determine a Budget threshold(s) so that folks have complete visibility into the costs. This is especially important because costs can sometimes be high due to unnecessary or poorly designed functionality prompting action to resolve. Exchange Online This section describes the synchronization of shared mailboxes to send alerts and the Contact-Us mailbox for support linked to a queue in Dynamics. For nonproduction environments, cloud native share mailboxes using the 139gc.onmicrosoft.com domain are being leveraged for development and configuration whereas production will have mailboxes configured in a cloud domain dedicated to the application. This implementation does not require or rely upon “hybrid exchange” as we are not synchronizing any mailboxes that originate from on premises, instead, a new cloud native domain will be configured in Exchange online (which supports multiple domains) to send alerts to recipients and provide a help centre mailbox that is synched to Dynamics and exclusively managed in Dynamics. This means that internal users will be able to review emails from the help centre queue which has an associated help mailbox and promote these to a case or associate to existing case in the system. All email messages synchronized in Dynamics are created as “email message” records” and are of type “activity” thus can be associated manually or automatically to any other activity record such as cases, organizations, and contacts. Once an email is received and associated, it is removed from the queue and responses are automatically tracked in the same thread in the associated record such as a case, thus eliminating the need for users to automatically re-associate a response to the same record. This implementation also does not allow for synchronization of personal employee mailboxes (at this time) and this is another reason why hybrid exchange is not a dependency. Synchronization of Shared Mailboxes to Send Alerts The following mailboxes have been provisioned in M365 Exchange. The table below lists these mailboxes and their associated environments. It’s important to note that an email can only be synched to one environment at a time and that only a G.A or Exchange Administrator can approve a mailbox or uncheck the setting that prevents application layer system administrators to synchronize these mailboxes. Mailbox Environment Environment Map This final section includes a table that lists all environments and their integrated technologies including SharePoint Subsites, Storage container addresses, B2C App Registrations, Portal URL, Security Group and synched mailboxes. D365 URL Portal URL Private / Public Portal Dataverse Environment Security Group SharePoint Site/Subsite URL DevOps GIT Repository Branch B2C API App Registration ID B2C SSO Registration ID Storage Container Address Azure DevOps SPN Synched Shared Mailboxes Table 42: Environment Map Azure Subscriptions The Case Compliance System is primarily implemented in the Power Platform, the SAAS CRM technology that ships with Microsoft 365. However, there is a series of supporting services required to secure the platform, monitor it, and enhance the overall management of the entirety of the system. For this implementation, {ORGANIZATION} has two subscriptions, one dedicated to non-production and one for production. The tables below outline the services, resource groups and tags used for each service implemented in each subscription. Every resource’s implementation detail listed in this table is described in this document and hyperlinks are set on the resource name for ease of navigation. Resource Type Purpose Resource Group Tags IAM Table 43: Azure Subscription Resource List (Non-Production) Resource Type Purpose Resource Group Tags IAM Table 44: Azure Subscription Resource List (Production) Backup, Restore, and RTO/RPO This section describes the backup, restore, RTP and RPO features of the platform and its implementation for this solution. RTO and RPO RTO and RPO are terms used to describe the amount of time a system can be down and the amount of data that can be lost without causing significant harm to the business. For Dynamics 365 online, the RTO (Recovery Time Objective) is the maximum acceptable amount of time that it can take to restore the system after a disruption or outage. Microsoft guarantees an RTO of 4 hours for Dynamics 365 online. This means that Microsoft will work to restore the service within 4 hours of a disruption or outage, and customers can expect the service to be back up and running within that time frame. The RPO (Recovery Point Objective) for Dynamics 365 online is the maximum amount of data that can be lost during an outage or disruption. Microsoft guarantees an RPO of 12 hours for Dynamics 365 online. This means that in the event of an outage or disruption, customers can expect to lose no more than 12 hours of data. It is worth noting that RTO and RPO guarantees are subject to the terms of the Microsoft Service Level Agreement (SLA), which outlines the specific terms and conditions of the agreement between Microsoft and the customer. The SLA also outlines the remedies available to the customer in the event that Microsoft fails to meet these guarantees. Backup and Restore The backup and restore feature in Dataverse allow {ORGANIZATION} to create backups of your Dataverse environments and restore them in case of data loss or corruption. Backups can be taken manually or scheduled to run automatically. The backups can be stored in Azure Storage and can be accessed at any time. Restoring a backup overwrites the current environment with the data and configuration from the backup. Backup and Restore in Sandbox Environment In a Sandbox environment, you can create a backup manually by going to the Power Platform Admin Center and selecting the environment. From there, you can click on the \"Backup & Restore\" tab and select \"New Backup\". You can also schedule backups to run automatically on a daily or weekly basis. Once a backup is created, it will be stored in Azure Storage. To restore a backup in a Sandbox environment, you can go to the \"Backup & Restore\" tab in the Power Platform Admin Center and select \"Restore Backup\". You will then be prompted to select the backup you want to restore. Restoring a backup will overwrite the current environment with the data and configuration from the backup. Backup and Restore in Production Environment In a Production environment, you can create a backup manually by going to the Power Platform Admin Center and selecting the environment. From there, you can click on the \"Backup & Restore\" tab and select \"New Backup\". However, creating a backup manually is not recommended in a Production environment, as it can cause performance issues. It is recommended to schedule backups to run automatically on a weekly or monthly basis. To restore a backup in a Production environment, you can go to the \"Backup & Restore\" tab in the Power Platform Admin Center and select \"Restore Backup\". However, restoring a backup in a Production environment is a more complex process and requires additional steps. It is recommended to seek assistance from Microsoft support via submitting a ticket in the Power Platform Admin Centre. Finally, when restoring an environment, the environment is automatically set to “administrative mode” meaning that only system administrators can access the system to review that the restore was successfully completed. If successful, administrative mode can be turned off in the admin centre and other users can now access the environment. Implementing Backup and Restore in Release Pipeline using CLI You can use the Common Data Service (CDS) CLI to create and manage backups and restores as part of a release pipeline. The CDS CLI provides a set of commands that allow you to automate the process of creating backups and restoring them. Creating the backup using the CLI, uses the following command (step in pipeline for disaster recovery): cds backup create --environmentName <environmentName> --containerName <containerName> --description <description> --zip This command creates a backup of the specified environment and stores it in the specified container in Azure Storage. To restore a backup using the CLI, you can use the following command: cds backup restore --environmentName <environmentName> --containerName <containerName> --backupName <backupName> This command restores the specified backup to the specified environment. These commands have been implemented into our release pipeline as part of your deployment process. This help ensures that our environments are always backed up and can be restored quickly in case of data loss or corruption. Business Requirement Implementation Summary & Testing TO DO -> inc. user journey diagram, HL features, state machine Business requirements are tracked in Azure DevOps in the same project that hosts this implementation’s GIT repository, release pipelines and artifacts. This is beneficial for tagging work items such as tasks and user stories to specific releases when running the pipeline thus allowing QA and UAT (testers) to scope what to test for each release. Furthermore, a subset of users is assigned the basic+test licenses to make use of the testing tools of DevOps. We make use of both manual and automated testing, both described below. Azure Test Plans (phase): This is a testing solution for manual and exploratory testing. It allows users to create test plans, test suites, and test cases, and to execute tests against different configurations of their application. Azure Test Plans also integrates with Azure Pipelines, allowing users to run automated tests as part of their build and release pipelines. Azure Test Automation (current): Azure Test Automation also integrates with Azure Pipelines, allowing users to run automated tests as part of their build and release pipelines. For this implementation, integration testing is being automated by running a series of PowerShell scripts that validate the YAML configurations of tested portal artifacts such as Table Permissions, Column Permissions, Site Settings, Web Templates, Wizard Form Configurations and Global settings. This means that once a feature has been tested “manually” and validated by testers, the developer will store the YAML version of each of the portal tested artifacts in the Validated Portal Test folder in the GIT repository and will compare these against every subsequent release of these same artifacts. If a change has been made to a tested file, a log is generated in the pipeline artifact for the staging pipeline only, therefore will allow the release. However, it is the developer’s responsibility to either update the new version of the YAML file based on new requirements or repair the misconfiguration and re-issue a new release to staging that is clean. For any other environment, the pipeline will block the release if the YAML files do not match the tested artifacts. The diagram below depicts this process. For CRM (backend testing), there are several tools being used and described below: Power Apps Test Studio is an automated testing tool for Model-driven apps that developers use to create and run UI test cases and helps them verify that your app's features and functionalities are working as expected. The tool offers a simple, intuitive user interface that makes it easy to create and manage test cases, and it integrates seamlessly with the Power Apps platform. Some of the key benefits of using Power Apps Test Studio for automated testing of Model-driven apps include. It’s important to note that building a model-driven-app, unlike the portal requires very little coding thus the platform provides UX testing tools to test the user, and these are described below and used by developers: Efficient test case creation and execution: With Power Apps Test Studio, you can quickly create test cases using a visual recorder, or by manually creating test steps. You can also run your test cases in parallel, which helps to save time and improve efficiency. Improved test coverage: The tool allows you to test a wide range of scenarios and use cases, which helps to ensure that your app is thoroughly tested and that all features and functionalities are working as expected. Integration with Power Apps platform: Power Apps Test Studio integrates seamlessly with the Power Apps platform, which makes it easy to create and manage test cases and to collaborate with other team members. Easy to maintain and update: The tool makes it easy to maintain and update your test cases, even as your app evolves, and new features are added. Overall, Power Apps Test Studio is a powerful and flexible automated testing tool that is specifically designed for Model-driven apps in Dynamics 365. It can help you to improve the quality and reliability of your app and ensure that it meets the needs and expectations of your users. Moq is the automated testing tool used for C# class libraries that are needed for plugins and custom workflow steps for more complex functionality such as the bilingual plugin utility. Moq is a popular mocking framework for C# that can be used to create mock objects and simulate dependencies in your code. We also use XUNIT which integrates with Moq. Moq is important as it will create a “Moq” organization context thus allowing to create CRUD operation in our assertion functions without the need to write to a Dataverse environment. When developers check in their code and solutions both testing tools are run as tasks in the pipeline and must succeed in the build for the release to be issued. When it comes to tying in with a release pipeline, all these testing tools can be integrated with Azure Pipelines, which is the continuous integration and continuous delivery (CI/CD) service offered by Azure DevOps. This means that users can include their tests as part of their build and release pipelines, ensuring that their applications are thoroughly tested before they are deployed to production. For example, users can create a build pipeline that builds and packages their application and includes automated tests using Azure Test Automation. They can then create a release pipeline that deploys the application to a test environment and includes manual and exploratory tests using Azure Test Plans. Finally, they can create a separate release pipeline that deploys the application to production, after all tests have passed. Overall, the testing tools available in Azure DevOps provide Basic+Test licensed users with a comprehensive suite of solutions for testing their applications, from manual and exploratory testing to automated and load testing. These tools can be easily integrated with Azure Pipelines, allowing users to include their tests as part of their build and release pipelines, and to ensure that their applications are thoroughly tested before they are deployed to production. The same applies to the automated test suite using Moq and the Power Apps Studio Testing Tool."
  },
  "_site/images/SharePoint/Readme.html": {
    "href": "_site/images/SharePoint/Readme.html",
    "title": "| GOC Theme Documentation",
    "keywords": ""
  },
  "_site/index.html": {
    "href": "_site/index.html",
    "title": "GOC Theme Documentation | GOC Theme Documentation",
    "keywords": "GOC Theme Documentation Welcome to the {ORGANIZATION} Documentation website. You will be able to browse through all of our product and services documentation. For organization specific documentation, you will need to be a registered user. Tip To become a registered user contact <customer-service@{ORGANIZATION}.com>. Tip Our goal is to provide our customers with the most up to date and relevant documentation surrounding the various products and services we support. Despite our strong focus to using PAAS and SAAS services, which are already well documented online, it is often a challenge to find the most relevant information based on the specific implemementation details of what our company has deployed to your environment(s). Important The way to authenticate to access our private documentation related to your organization's implementation is through your Azure AD account that is federated with your on-premises active directory. Important Each customer has a dedicated site for documentation, the site is not hosted in a shared infrastructure model. Instead, each site has its own hosting boundary and network security group making it impossible for other users who are registered to this site to ever access another customer's documentation."
  },
  "_site/infrastructure/README.html": {
    "href": "_site/infrastructure/README.html",
    "title": "Create Documentation Website | GOC Theme Documentation",
    "keywords": "Create Documentation Website This folder contains the Terraform scripts for creating the documentation website. The terraform command to deploy this website is executed in the .pipelines/documentation.yml. More explanation of the use of these files can be found int the article Deploy the DocFx Documentation website to an Azure Website automatically. Running Terraform locally To run these scripts, first make sure you set the proper values of the variables. IMPORTANT: Make sure you modify the value of the app_name variable. This value is appended by azurewebsites.net and must be unique. Otherwise the script will fail that it cannot create the website. To install Terraform, you can use Chocolatey: choco install terraform To run the terraform scripts from your local developer machine, make sure to modify the providers.tf. Comment the backend definition as instructed. Next make the connection with the Azure environment by running the Azure Cli command: az login Next you can initialize Terraform: terraform init The you can plan the terraform actions: terraform plan And apply the terraform script with: terraform apply When asked for approval, type \"yes\" followed by ENTER. You can also add the -auto-approve flag to the terraform apply command."
  },
  "_site/product-releases/ALM/ALM.html": {
    "href": "_site/product-releases/ALM/ALM.html",
    "title": "Releases | GOC Theme Documentation",
    "keywords": "Releases Release 1.0 Release 1.1"
  },
  "_site/product-releases/Azure Storage/Releases.html": {
    "href": "_site/product-releases/Azure Storage/Releases.html",
    "title": "Releases | GOC Theme Documentation",
    "keywords": "Releases Release 1.0 Release 1.1"
  },
  "_site/product-releases/AzureB2C/Releases.html": {
    "href": "_site/product-releases/AzureB2C/Releases.html",
    "title": "Releases | GOC Theme Documentation",
    "keywords": "Releases Release 1.0 Release 1.1"
  },
  "_site/product-releases/Digital Signatures Module/Releases.html": {
    "href": "_site/product-releases/Digital Signatures Module/Releases.html",
    "title": "Releases | GOC Theme Documentation",
    "keywords": "Releases Release 1.0 Release 1.1"
  },
  "_site/product-releases/GOC Theme Configuration/Releases.html": {
    "href": "_site/product-releases/GOC Theme Configuration/Releases.html",
    "title": "Releases | GOC Theme Documentation",
    "keywords": "Releases Release 1.0 Release 1.1"
  },
  "_site/product-releases/index.html": {
    "href": "_site/product-releases/index.html",
    "title": "{ORGANIZATION} Product Releases | GOC Theme Documentation",
    "keywords": "{ORGANIZATION} Product Releases Tip This section provides you with {ORGANIZATION}'s product releases. This includes, the portal themes, portal extensions, d365 solutions, model-driven and canvas apps, ALM artefacts, B2C technical profiles and much more. You can subscribe to be notified of all future releases and can opt to be notifed of release updates on each product, so you can select those for which you are most interested in. For client specific releases, you need access to your organization's documentation pages. To gain access to your organization's product releases, you can get contact customer-service@{ORGANIZATION}.com. Important When downloading releases, make sure to carefully read and follow the configuration instructions. For some of the artefacts, especially the PowerPlatform ones (in particular portal themes/extensions, and d365 solutions), depending on your set up, you may overwrite your own cutomized changes to the same elements being deployed from one of our releases. So make sure that you leverage GIT (or any code versioning system) to review and action any merge conflicts. We are not responsbile for unwanted overrides in your tenant as this is your responsiblity. However, {ORGANIZATION} does offer SLA's with organizations, and you can opt to subscribe to an SLA whereby a technical specialist will be responsbile to help you deploy new releases, help unblock any roadblocks, and participate in the merging of the release(s) with your technical team."
  },
  "_site/product-releases/Portals CS Form Wizard/Releases.html": {
    "href": "_site/product-releases/Portals CS Form Wizard/Releases.html",
    "title": "Releases | GOC Theme Documentation",
    "keywords": "Releases Release 1.0 Release 1.1"
  },
  "_site/product-releases/SharePoint/Releases.html": {
    "href": "_site/product-releases/SharePoint/Releases.html",
    "title": "Releases | GOC Theme Documentation",
    "keywords": "Releases Release 1.0 Release 1.1"
  },
  "_site/product-releases/System Design Guide/SDD.html": {
    "href": "_site/product-releases/System Design Guide/SDD.html",
    "title": "Introduction | GOC Theme Documentation",
    "keywords": "System Design Guide Version: 2.6 Latest Issue: 4 May 2023 Revision Record Version Author Description Date Issued 1 Frederick Pearson Security Sept 27 1.1 Frederick Pearson Security / Arch / Pipeline October 29 1.2 Frederick Pearson Draft for team to provide some data/screenshots Dec 3 1.3 Frederick Pearson SSO Updates, OOB integrations and Org Master API integrations January 30, 2023 1.4 Frederick Pearson 2023 Enterprise CRM Implementation SDD best practice technical documentation refactors (outline only) Feb 24, 2023 1.5 Frederick Pearson Outline finalized, tables, figures, content table and narratives updated Feb 26, 2023 1.6 Frederick Pearson Update tables and figures + include the application layer artifacts April 4, 2023 1.7 Frederick Pearson New sequence diagrams and new narrative April 5 ,2023 1.8 Frederick Pearson Additional diagrams and narratives April 9, 2023 1.9 Frederick Pearson Additional diagrams and narratives updated site map April 11, 2023 2.1 Frederick Pearson Security details, diagrams, figures, SharePoint, Retention and Disposition April 12, 2023 2.2 Frederick Pearson Finalize sections up to exchange online and business requirements April 12, 2023 2.3 Frederick Pearson Finalize narrative before converting to markdown and populating tables with {ORGANIZATION} implementation data April 13, 2023 2.4 Frederick Pearson Typos and adjustments to the content table header 1 format (roman numerals) & updates to release pipeline section April 16, 2023 2.5 Frederick Pearson Typos, more content for ALM practices and security, supporting azure subscription list and its resources, and CRM & Portal Conventions April 26, 2023 2.6 Frederick Pearson DLP Settings May 4, 2023 Introduction The introduction section of the solution architecture document provides an overview of the purpose and scope of the document, which is to describe the architecture of the CRM case management system and its components. It also covers the technologies and platforms involved in the implementation. Purpose and scope of the document Overview of the CRM case management system and its components Overview of the technologies and platforms involved. Purpose and scope of the document The scope of this document focuses on the Dynamics 365 Customer Service implementation in the organizations Power Platform subscription which is part of the M365 product family and is considered as a “SAAS” technology. Dynamics 365 CS is an enterprise CRM platform that the organization plans on leveraging to modernize the digital relationship with the “reporting entities” which are financial institutions across Canada that are responsible to report financial data and through various forms and mediums to {ORGANIZATION} for compliance officers to analyze to ensure compliance with the financial laws in Canada in this sector. This technology also ships with a Portal technology that integrates natively with Dynamics within the Power Platform which allows Contacts (external portal users) to submit data in a more efficient and secure manner directly to {ORGANIZATION}. Officers typically will generate reporting cycles by sector and invite all financial institutions within that sector to fill in compliance forms and attach supporting documents. This document demonstrates the implementation of this technology to meet the use this case. Moreover, {ORGANIZATION} has an API driven architecture whereby the platform can interface with read data thus aligning with the organization’s overall digital strategy and cloud adoption. By leveraging this technology, particular a SAAS technology that has been assessed by CSE/SSC/TBS as Protected B ready, the complexities associated with administering a custom implementation or infrastructure is now abstracted and thus simplifies the implementation and assures a higher level or security due to the fact that operators cannot interfere with the OS or IAAS and encryption is handled end to end both in transit and at rest using AES and RSA and falls within the realm of the organizations Active Directory policies thus ensures that only {ORGANIZATION} employees on approved devices with Federal Government CA authorization can access the platform. Moreover, the portal is also administered by Active Directory service principles only accessible by privileged user roles (app admin, GA) and external users (Contacts (external portal users)) must be invited formerly to the portal and are forced to use 2FA to access the portal. Overview of the Dataverse environments (CRM case management system) and its components Before diving into the specific use case implemented using the Dynamics 365 Customer Service Module with Portals, the section will first describe what each of these technologies are to provide context and explain how the compliance management process will fit into or has been built leveraging these technologies: Dynamics 365 Customer Service Dynamics 365 Customer Service is a customer relationship management (CRM) software application that enables businesses to manage and streamline their customer service operations. The platform offers a wide range of features, including case management, SLA management, business process flows, automation, and reporting capabilities. Case management is a critical aspect of the Dynamics 365 Customer Service platform. It allows customer service agents to efficiently manage and resolve customer issues by creating cases, tracking case progress, and escalating cases as needed. The platform also includes robust SLA management features, which allow businesses to establish service level agreements with staff and track performance against those agreements. Additionally, the platform offers extensive automation capabilities, including the ability to automate routine tasks, such as email responses and case routing. Another strength of Dynamics 365 Customer Service is its business process flows feature, which enables businesses to automate and standardize their customer service processes. With this feature, businesses can create predefined workflows that guide agents through each step of the customer service process. The platform also offers robust reporting capabilities, allowing businesses to track key performance indicators (KPIs) and gain insights into customer service operations. Overall, Dynamics 365 Customer Service is a powerful tool that can help businesses improve their customer service operations and streamline their workflows. This technology aligns well with the compliance process at {ORGANIZATION} whereby employees are responsible to generate reporting cycles by sector which automatically trigger an invitation process and notifications to Contacts (external portal users) within that sector to fill in the data of the compliance form associated with the cycle, for example the risk questionnaire type using out of the box case management feature. Furthermore, the SLA feature is being leveraged to track the progress and timeline obligations that Contacts (external portal users) are responsible for submitting the information to {ORGANIZATION} within a given (configurable timeframe). Furthermore, the auditing feature allows {ORGANIZATION} to report and examine all activity on the case and audit logs are immutable. The platform’s business process flow and out of the box state machine (statuses) are being leveraged to track where in the process a compliance form is. For example, the process starts in draft until the employee runs a the workflow (flow) to activate the cycle which triggers the invitation process and generates all child compliance case per RE which also has its own state machine. Once this is successfully triggered the status is automatically set to in progress and only transitions to under review once all submissions have been received and triaged for accuracy. The state machine is further described in the application layer implementation details. Throughout this process however, Contacts (external portal users) and {ORGANIZATION} employees can collaborate digitally over email or virtual agent feature and all correspondence is automatically linked as an activity associated to the case. Furthermore, {ORGANIZATION} employees can return submissions to the Contacts (external portal users) to request more information and extend the timeline for submission when warranted. Most of these features come with the platform’s tooling with additional development to account for specific data elements and configuration of state machine and business process flows and rules build by the CRM developer. Power Pages Sites Power Pages Sites (customer self-service type) is a low-code, self-service portal technology that integrates natively with a Dataverse environment licensed with customer service license. The platform allows {ORGANIZATION} to provide their Contacts (external portal users) with a self-service portal that enables them to access information and perform tasks such as submitting and tracking cases, updating their profile information, and accessing other data such as attachments and notifications. One of the strengths of Power Pages is its SAAS nature, which means that businesses do not have to worry about managing infrastructure or performance. The platform is hosted on Microsoft Azure and provides automatic scaling and failover capabilities, ensuring high availability and reliability. Portals also offers native integration with Azure B2C, which enables businesses to provide single sign-on (SSO) with 2FA to external users. This integration provides a secure and seamless authentication experience for users accessing the portal and aligns with the organizations current architecture to provide SSO and API authorization to Contacts (external portal users). Another key feature of Portals is its RBAC (Role-Based Access Control) capabilities, which enable businesses to govern access and CRUD (Create, Read, Update, Delete) operations to tables and columns in the Dataverse. The platform provides granular control over access permissions using table permissions and column permissions coupled with web roles. Power Pages Sites also offers advanced features such as rendering CRM forms using advanced forms and rendering lists using existing views. This allows businesses to customize the user experience and provide a seamless integration with their existing Dataverse environment. Finally, the invite-only feature of Power Pages Sites provides an added layer of security, ensuring that only invited users can access the portal. Overall, Power Pages Sites is a powerful tool that can help businesses improve their customer service operations by providing a secure and customizable self-service portal for their staff. Contacts (external portal users) will leverage this portal to note only submit data and documents to {ORGANIZATION} but also seek support via the omnichannel modules (chat/virtual agents/voice) and the portal has been adapted to meet the WET / WCAG 2.0 theme and compliance that is a commitment to Canadian citizens to ensure a seamless experience to any user regardless of vision impairment or other ailments making it difficult to use a web-based tool. {ORGANIZATION} has incorporated this theme in the platform and is responsible to maintain it by issuing new releases as the theme is modernized year over year. Moreover, by providing 2FA and presenting users with transparent consent on what {ORGANIZATION} is collecting and its obligations to protect the data and handle data retention and disposition rules to meet the GOC’s Protected BMM posture, users are informed each time they sign on of the terms and conditions and what to expect when interacting with the portal. The illustration below provides the look and feel and SSO with consent into the platform (subject to change, and this is development). The purpose is to demonstrate the integration the WET theme and the SSO service integration with the PBMM consent screen: Figure 1: Home Page Figure 2: Azure B2C SSO Login Pages Figure 3: Terms and Conditions (PB) Consent Page Figure 4: Authenticated User Landing Page The Power Pages Site setting to force the PBMM consent on login is illustrated in the table below. System Architecture This section provides a high-level diagram of the system architecture and the components involved in the CRM case management system, which include Dynamics 365 Customer Professional App, Power Pages Site, SharePoint Online, Exchange Online, and Azure B2C. The section provides a detailed description of each component, including its architecture and data model, customizations and extensions, and integration with other components. Additionally, it discusses how the components integrate with each other, including data flow and synchronization, security and authentication requirements, and integration patterns and best practice. The system is implemented in the {ORGANIZATION} Power Platform subscription which is a SAAS residing in the organizations M365 subscription. The Power Platform has been configured with guardrails to adhere to IT standards which would allow the ability for {ORGANIZATION} to store and interact with Protected B data. These guardrails protect and govern this implementation and are further described in the Power Platform guardrails chapter of this chapter. However, the full platform guardrail implementation documentation is separate from this document. However, the reader should take into account that this implement operates within the confines of these guardrails. Similarly, this system operates the confines of the Azure B2C PB guardrails and the Azure and M365 baseline guardrails mandated to Federal Government Department and Agencies implementing PB workloads outside the GOC owned network devices hosted in Government owned and or operating datacenters. In this implementation our solution is comprised exclusively of Microsoft owned network assets and SAAS and PAAS offerings which is a deliberate decision to further secure this application by relying on a trusted partner whose datacenters meet and exceed the GOC standards for physical and network security and has been fully assessed by our intelligence agencies and internal IT experts. These physical and network assets are subject to random audits by impartial parties to validate adherence to the GOC’s ISO based set of network and hosted network physical requirements such as multiple checkpoints, specific security clearances/screening of staff, and safely purging of stale physical network assets such as any device hosting data such as SDD/HDD/RAM etc. Another important factor for cloud security is the integration of {ORGANIZATION}’s Active Directory to Contacts (external portal users) Active Directory using AD Connect / ADFS / WAP to ensure that our internal user data is owned by our hardware thus our conditional access policies such as the requirement to access any of the elements in our implementation must be done by an operator who’s physically accessing our network using VPN and authorize to our on premise hosted Entrust Certificate Authority. This means that Microsoft has no ability to compromise access to our systems without the same requirement. Moreover, MS cannot access our Global Admin credentials and thus cannot restore this account which is the reason why we have multiple G.As and break glass accounts. Finally, the GOC has implemented the Express Route which is a physical link between our ISP’s to the Azure Datacenters, thus extending our existing data-centers to Azure giving us more control over who and what data can traverse through between both organizations. Finally, {ORGANIZATION} is also implementing CMK and an HSM to own the encryption keys. This will also apply to the Power Platform, however {ORGANIZATION}, as of 2023, does not mean the minimum requirements (1k licenses) to activate this feature. Once the feature is available to the organization, a migration will be required however will be vital to elevate the data confidentiality and integrity guardrails by guaranteeing that even if MS is compromised the nefarious actor (even with access to the MS CA infrastructure) will not be able to decrypt the data. High-level diagram of the system architecture The target state architecture aligns with {ORGANIZATION}’s perimeter services which is comprised of using Azure Gateway as a proxy to both Azure B2C and the Power Pages Site. Figure 5: Software Architecture Target State The current state architecture is leveraging the Microsoft default SAAS security perimeter services which is abstracted from {ORGANIZATION} and fully under the control of Microsoft’s security team. The TBS cloud usage profile allow this for PBMM because SAAS infrastructure is not accessible by GOC employees thus is less vulnerable to security threats or failure to patch security vulnerabilities quicker than the teams at Microsoft administering the platform. {ORGANIZATION} however can control the perimeter services for both B2C and Power Pages Sites by using Azure Gateway or Azure Front Door and use this to also configure a custom domain for both services. Figure 6: Software Architecture Current State Technical Architecture Dynamics 365 Customer Professional & Enterprise This is a series of model driven applications that are installed in each Dataverse environment that include additional features that come with this license including case management, service level agreements, customer (client) insights, omnichannel for agent (messaging and voice), additional capacity per user (.75gb per licensed user), and additional Power Automate Flow throughput. The case (incident) is the pillar of the compliance application as every type of compliance form is managed via the case table. Each type of case (which is the case subject tree) has a dedicated custom table that holds the data that clients will submit via the Power Pages Site and is linked as a N:N relationship to the case. Compliance staff will create reporting cycles using the case management feature, and select a subject such as Risk Questionnaire, and select a selector or manually choose which reporting entity(ies) the cycle is scoped for. By doing so invitations to complete the form are sent to each reporting entity associated with the cycle (the primary contact of the RE) and the SLA is activated. The SLA feature is an OOB feature available through the CS module and provides the ability for system administrators and customizations to configure date driven rules to communicate to both internal and external stakeholders’ deadlines for various actions such as submitting a compliance form, reviewing a submitting, deciding on a form’s process to transition to a new state (such as complete) and actioning incident (tickets for support). The feature provides visual cues such as green, yellow, and green dots next to records in the Model Driven App views and in portal lists shown to external users for transparency. The feature is also useful to create automated events such as sending email reminders and report on adherence to service standards providing {ORGANIZATION} with aggregate metrics to examine its own SLA standards vis a vis its internal capabilities. This feature implementation for the compliance management system will be illustrated in this document. Figure 7: SLA's Power Pages Site The Power Pages Site technology is internet facing and is on the powerappsportals.com domain by default. The security perimeter is maintained by Microsoft just like all other SAAS services and non productionized sites (e.g. dev, test, uat) are internal to {ORGANIZATION} employees only and not accessible by external users who do not belong or exist in the 139gc domain. Therefore, just like teams, and other M365 SAAS products, users who are developing and or testing the portal must be on a {ORGANIZATION} device and authorized to Active directory and entrust CA to access the portal. For the production portal, the settings of privatization are turned off and the site is made “public” and which point the anonymous page (home page) of the portal is accessible via the internet however nobody who has not been invited to use the portal and is within the Azure B2C domain can access the authenticated user pages. This app type is provisioned within the same Dataverse environment as the D365 CS app and it’s a public facing website that is configured for invitation only. Meaning, open registration by external users to the portal is prohibited. An RE must first exist in the Azure B2C tenant, and then be sent an invitation from the D365 CS application that includes a link with embedded invitation code to redeem. Once in the portal the RE primary contact and start filling in the form and assign the form (optionally) to an authorized agent, which are contacts in the CRM that are associated to the Primary Contact’s organization (the financial institution). However, only the Primary contact has permission to submit the form to {ORGANIZATION} for review. Once submitted, the user is notified that the form is no longer editable and that a {ORGANIZATION} officer will be reviewing the submission. If the officer determines that the form is incomplete or requires clarification or more information, they have the ability to change the state of the Compliance case to “Draft” and send a “notification” activity to the primary contact, which is done via a Power Automate Flow, at which point the Primary Contact receives a generic email instructing them to log into the portal and review the notification activity where more instructions are provided by the {ORGANIZATION} employee and a direct link to the form which is now editable again for submission. This process can repeat itself until the {ORGANIZATION} employee deems the form complete and transitions the state of the compliance case from under review for ready for approval. SharePoint Online This technology is the official IM repository for the entire system. Each Dataverse environment from dev through prod is linked to a SharePoint subsite that lives within a site. {ORGANIZATION} has provisioned 2 SharePoint Sites, 1 for non-production and 1 for production each hosting x number of “subsites” that point to 1 environment. For example, dev is integrated with the dev subsite in the ftnc-compliance-np site, and pre-prod and prod both have their own subsite hosted within the production SharePoint compliance site. The environment lists all the subsites associated with each environment. This setting is managed the advanced settings or admin console for Power Platform (admin.powerplatform.onmicrosoft.com) where a site (subsite) is synched with a Dataverse environment and each table that is configured to accept attachments will automatically create a folder within the subsite whereby each record such as a case will have its own folder holding all associated attachments. This is configured by the D365 System Administrator role. Because this app supports Power Pages Site attachments to SharePoint, the G.A must provide consent to allow the integration of the site to the integrated SharePoint subsite to the Site’s integrated SharePoint subsite environment. The table below lists every table that attachments are allowed: Table Name Incident (case) Risk Questionnaire Account (Organizations) Contact User Notification .. .. Table 2: Tables integrated with SharePoint. SharePoint Sites with X Number of Subsites Each Mapped to Non-Production Environments (e.g., Dev, Test, Staging, UAT, Sandbox) and another site that hosts the production environment. In this implementation, there are 2 SharePoint sites created – one for non-production environment integration and 1 dedicated for production environments. The table below lists each site, subsite and environment URL mapping. Subsite Name/URL Type CRM environment Table 3: SharePoint Subsite & CRM Environment Mapping The illustration below depicts the folder structure in SharePoint for each environment’s subsite. Figure 8: SharePoint - Native Integration The process of uploading documents to the SharePoint site from an external user perspective and internal one is depicted below. The illustration also includes the archiving process to be implemented via an integration between SharePoint Online which is considered the IM repository for transitory and working documents whereas RDIMS is the official IM repository at {ORGANIZATION} to secure and archive documents. SharePoint Online is configured to purge all documents 2 years after being uploaded. However, the same documents live in RDIMS where existing disposition rules apply. The integration between SharePoint Online and CRM is a native configuration as both systems live within the same platform, M365. However, to configure the SharePoint integration between Power Pages Sites and SharePoint requires a Global Administrator to activate this setting and provide consent. This action will generate an API permission in the existing App Registration record that is leveraged to integrate the Portal with CRM. The illustration below depicts the new API permission created by this process. Figure 9: Power Pages Site Admin Portal - SharePoint Integration Activation Feature Figure 10: SharePoint API Permissions Automatically Added by Activating the Portal SharePoint Integration Feature Activating the SharePoint Online integration between the Portal and CRM requires additional setting such as Table Permissions and basic form metadata to expose the SharePoint subgrid allowing external users to be able to upload documents. These settings are provided in the table below. Table 4: SharePoint Table Permission Settings (review) Exchange Online The compliance case management system leverages the D365 email integration feature with Exchange online by integrating 2 cloud native Shared Mailboxes with the platform. This is configured using the email server-side sync which automatically integrates with the 139gc exchange online environment and must be turned on by an exchange administrator or G.A. However, for non-production, the system wide settings to allow D365 system administrators to synchronize mailboxes without intervention of the former privileged roles are turned off. Once server-sync is activated, the mailboxes are synchronized for each environment. Below is the list of emails synchronized by environment. To activate this feature, a system administrator must go to the email settings or system wide settings and configure email “server-side sync” and choose Server-Side Sync Exchange for both inbound and outbound mail. Appointments and tasks can also be synched from outlook. However, this implementation is designed to use CRM exclusively to administer email and not synchronize staff mailbox, which in this case integration using the outlook plugin would be useful. Each mailbox is first configured in exchange as a “Shared Mailbox” Type with delegation privileges to the Power Platform-administrator group who are responsible to synchronize them to each environment. These mailboxes are of type “Cloud” thus there is no requirement for this phase to synchronize on premise exchange mailboxes (e.g., Hybrid Exchange). Finally, its important to state that a mailbox can by synched to one environment a time therefore we’ve created an outbound only and in and outbound mailbox for each environment for development and testing. Mailbox Environment Table 5: Exchange Shared Mailboxes Synched in each D365 environment. The illustration below depicts the exchange email server-side sync architecture – note for this implementation, the synchronization only has been configured for the CRM organization – the outlook app is not being leveraged nor required as all email is administered in the model driven app. Figure 11: Email -server-side sync The use case for emails is listed below. These are subject to change. Secondly, this system will never, or is prohibited to send Protected B content in the emails being sent. Thus, this implementation includes a “Notification Centre” module which integrates with email, whereby an email to the person notified who is responsible to log into the portal and navigate to the notification center to view the details of the notifications. The table below demonstrates the data model for the notification center. The feature enables administrators to configure custom templates using a rich text editor and use workflows and flows to send notifications using both the do not reply email queue mailbox OR the help center email queue. When a user receives a notification, an unread badge (visual cue) is displayed, and the user can choose when to mark the notification as “read” by pressing the “Mark as Read” notification which updates the status reason of the notification item record. Figure 12: Notification feature ERD Figure 13: Example unread notification Internal users can manually issue notifications in the model driven app. The process is illustrated below. Most notifications however are automated via processes and or Power Automate Flows. Azure B2C Azure AD B2C (Business to Customer) is a cloud-based identity and access management solution provided by Microsoft. It enables businesses to manage customer identities and authentication in a secure, scalable, and cost-effective manner. With Azure B2C, businesses can offer their staff a seamless, personalized experience across multiple applications and platforms. Azure B2C provides features such as social identity integration, multi-factor authentication, self-service password reset, and more. It also supports industry-standard protocols such as OAuth 2.0 and OpenID Connect. Power Pages Sites can be integrating natively with Azure B2C using app registrations. App registrations are created for each site to integrate with Azure B2C using the OIDC setting out of the box and therefore each environment is provided a client ID, Secret, and the user flow metadata (OIDC metadata) from B2C NP, and for production the B2C production tenant. B2C is an active directory tenant separate to 139gc but linked to a pay as you go subscription, the non-prod and prod subscriptions at {ORGANIZATION}. The table below lists all the client IDs by environment. The Client IDs are unique identifiers to App Registration are records generated by administrators of B2C or application developers in the B2C AD domain. Client ID (Registration) Environment Table 6: App Registrations for Power Pages Portal SSO The detailed integration details are further described in the implementation details and baseline configuration sections of this document. The sequence diagram below demonstrates the user journey for authorization to B2C from Power Pages Figure 14: B2C Journey For this to work, Power Pages site settings must be configured with a “Client ID, Secret, Tenant ID, the User Flow, and mapped claims (email, first name and last name)” from B2C so that the portal can generate a “bearer token at runtime which is a hashed value (RSA 2056) of a JSON Web Token (JWT) containing these “claims” and the session ID and its lifespan (19 minutes as per LOA 2/PB requirements). The diagram below depicts this process. Figure 15: B2C Technical System to System OIDC Journey In this diagram, the Application uses the Client ID and Secret to authenticate with the B2C Tenant and obtain an Access Token. The Access Token is then used to request User Claims and make API calls. If the Access Token has expired, the Application uses the Refresh Token to request a new Access Token and then uses it to request User Claims and make API calls. The hashed JWT includes: Username App ID SID (session ID) Azure API Management (APIM). APIM is leveraged as the API management system, or the org master and report ingest endpoints (REST). API requests are executed to the APIM perimeter by using a client ID and secret generated by an App Registration in B2C which in turn issues a bearer token for subsequent calls to the endpoints to return payloads such as the RE intuitional data, primarily the basic organization information and the various contacts associated to these institutions from the primary contact to authorized agents that will be interacting with the portal. (RICK + TEAM INPUT REQUIRED) Dynamics 365 Customer Service - Architecture and data model (OOB) The table below lists all the tables that are used in this implementation and includes the CRUD operations including append and append to privileges by security role. The table lists all the column permission rules set by team in the system. This feature secures specific field(s) within a table that provides a more granular level of data governance. For example, only a manager can set a specific value in a column (field) in the case table. The table lists all the relationships and their cascading rules in the system. This is important for data governance as retention and disposition rules that will govern the purging of the records will rely on the cascading rules when using the bulk delete feature whereas cascading rules that configured as “referential” will be purged using Power Automate flow as when a relationship is set to referential, the parent record that’s being deleted will not purge its child records that are referential, instead it will simply remove its reference to it or clear the lookup field value. In certain scenarios this may make sense however the cascading set to parental is key so that for example if an organization is purged, all its child contacts, cases, and related records are also purged automatically without having to configure additional flows or workflows to destroy these records. However, the account table has multiple relationship types to the contact record to track the various types of contacts in 1:N and the system only allows for 1 parental relationship type by table thus both bulk delete and Power Automate flows will be responsible to purge data. Table Name Key(s) Relationship Name Assign Delete Share Re-Parent Delete Merge Table 7: D365 Table / Shema Relationship Types/Cascade Configurations Table Name Create Read Update Delete Append Append To Team Team’s associated security role(s) Table 8: Table CRUD and Append/Append to/Assign privileges by security roles/team Case Management & Subject Convention A key convention in the design of this application is that every center around the case table. This means that each time {ORGANIZATION} generates a reporting cycle, an out of the box Case record is created of type “Reporting Cycle”. This case record has 1 or more associated case records of a particular type (or Subject) such as “Risk Questionnaire”. The Subject field is an OOB field that allows a system administrator/configurator to configure Case types using a tree like structure. The complete Subject tree is provided below (subject to changes over time, thus this version is based on the 2023 available compliance form types. Customizations and extensions Dynamics 365 Customer Service module offers a wide range of customization and configuration features that can be tailored to fit the specific needs of a case management system for financial institutions. With Dynamics 365, users can easily customize the look and feel of the system, including the layout, branding, and navigation. The module also allows for the creation of custom fields and forms, enabling users to capture and track relevant data specific to financial regulations. Additionally, Dynamics 365 provides automated workflows and business process flows that can be customized to streamline case management processes and increase efficiency. These features, combined with a comprehensive reporting and analytics dashboard, enable financial institutions to effectively manage cases and ensure compliance with regulations. Power Pages Sites is another powerful tool that can be utilized to enhance the case management system. Power Pages Sites provides a customizable web portal that allows external users, such as financial institutions, to submit and track the progress of adherence to financial regulations. The portal can be configured to include specific forms and fields for data capture, as well as custom branding to ensure consistency with the financial institution's branding guidelines. Additionally, Power Pages Sites allows for the integration of various third-party applications, such as document management systems or payment gateways, to streamline processes and improve the user experience. With its robust customization and configuration capabilities, Power Pages Sites can provide financial institutions with a powerful tool to effectively manage adherence to financial regulations. This section delves into the details of the various customizations and extensions leveraged for features developed for this implementation. Dynamics 365 workflows Dynamics 365 workflows are automated processes that streamline business processes and enable users to define and automate business logic. Workflows can be used to automate tasks, such as sending email notifications or creating tasks, based on specific conditions or events in the system. They can also be used to enforce business rules and help ensure data consistency. Workflows can be created and customized by users with appropriate permissions using the Workflow Designer in Dynamics 365.\\ Name Description Async/Sync Solution Table 9:Workflows Dynamics 365 actions Dynamics 365 actions are custom methods that can be created and exposed through the system's web API. Actions can be used to encapsulate business logic and provide a simplified interface for external systems to interact with the Dynamics 365 system. Actions can be synchronous or asynchronous and can return data or perform operations on the system. They can be created and customized by developers using the Dynamics 365 SDK. Name Description Async/Sync Solution Table 10: Actions Dynamics 365 plugins & Custom Workflow Steps Plugins Dynamics 365 plugins are custom code that can be executed in response to system events, such as creating or updating records. Plugins can be used to extend the system's functionality, automate business processes, or integrate with external systems. Plugins are written in .NET and can be registered to execute in a synchronous or asynchronous manner. They can be created and customized by developers using the Dynamics 365 SDK. Plugins are “signed” binaries generated by .NET and must be registered to CRM using the Power Platform CLI and DevOps Pipelines (sourced in GIT). The table below lists all custom plugins, their purpose and direct link to repository. Finally, plugins run within a sandbox which secures the platform from unwanted customizations from third party libraries or misconfigurations and unwanted code by forcing the use of a specific first party libraries, signed binaries using developers private key and published via Pipelines. Plugin assemblies are comprised of one of more classes that can be configured as “plugin steps” whose purpose is to run on any CRUD operation and the developer can specify which fields are allowed to be read and interacted with in code and best practice dictates to only use the tables and fields required to make a working configuration. Furthermore, developers are instructed to adhere to professional and verbose logging using the plugin trace logs and this is inspected during code reviews and test automation will calculate the % of plugin trace logs vs lines of codes and the benchmark is 20%. Custom workflow steps These are almost the same as plugins however provide more flexibility to non-developers who prefer to leverage workflows (processes) to call custom steps that are not available. For this implementation, custom workflow step libraries have been imported to help with mundane and repetitive needed features such as querying 1:N relationships for process logic and more robust email automation. No custom step library has been developed in house (yet). Assembly Name Purpose Type Repository Location Owner e.g. Plugin, Custom Workflow Step {ORGANIZATION}, Third Party (provide full name of lib) Table 11: Plugins Power Automate (flows) Power Automate flows (formerly known as Microsoft Flow) are cloud-based workflows that can automate business processes and integrate with other systems and services. Flows can be triggered by events in the system, such as creating or updating records, or by external services, such as receiving an email or a tweet. Flows can be used to perform a wide variety of actions, including sending notifications, creating records in other systems, and generating reports. Flows can be created and customized by users with appropriate permissions using the Power Automate Designer. The table below outlines and describes each Flow developed for this implementation. Flow Name Purpose Trigger Primary Table Table 12: Power Automate Flows Azure B2C & Integration to the Organization Master & Report Ingest APIs B2C has been described in length in previous sections, but to elaborate at our D365 application layer and its “indirect” relationship to the portal, B2C is not only leverage for SSO into the portal but also the OAUTH 2.0 authorization provider of bearer tokens for the backend D365 Power Automate Flows. An app registration with API permissions to both the org master and report ingest APIs are dedicated to each environment (1 per environment) calling the UT1 up to ET1 and eventually prod APIM endpoints. The table below maps the Dataverse environments to each APIM REST service and their respective App Registration whose secret must rotate every 24 months for NP and 6 months for prod (optional – c/b up to the maximum of 24 months). Dataverse Environment API Instance / base endpoint w/o query params or payload App Registration’s CLIENT ID for OAUTH 2.0 token Service for which the flow context executes (API Permissions) Dev UT1 Staging UT1 Test UT1 UAT UT1 (Tbd) Sandbox UT1 (Tbd) Training UT1 (Tbd) Preprod ET1 Prod TBD Table 13: App Registrations for Power Automate Flow REST API Calls to Org Master and Report Ingest Power Automate performs the HTTPS request to B2C to obtain a B2C bearer token to perform the API nightly. System administrators can invoke the refresh of the API calls directly in flow but running it manually as well and a full log of all calls is available directly in the flow interface. Failures will provide a verbose stack trace of the issue. It is important to note that the flow must be associated to the Power Platform Automation service account which is a licensed user as Flows won’t support App Registrations to perform API calls. This account’s credentials are secured in our KeyVaults instance. The following table includes the JSON payload for GET and POST requests (if applicable) and which table and column they map to in the Dataverse environment. This is available in a Swagger API definition endpoints/file below: Endpoints HTTP Verb Payload / Query Param (sender) Payload / Query Param (receiver) - Map Table 14: Org Master & Report Ingest Endpoints, Payload Schema, an Query Param requirements. Mapping repsonses to the Dataverse Schema Figure 16: B2C - Daemon Flow (API's interfacing wth our internal API's) In this diagram, the Daemon first makes an API call to the Application API. The Daemon then authenticates with the B2C Tenant using the Client ID and Secret to obtain an Access Token. The Daemon then makes another API call to the API with Authorization layer using the Access Token to authenticate and obtain the API response. Handling downtime In the event the API(s) are not accessible for which ever reason, our CRM implementation supports data imports using excel and the SDK thus this section describes what developers (or persona responsible to monitory/sync the data is required to do in this scenario. Business Rules Dynamics 365 Business Rules are a feature that enables users to define and enforce custom logic within the system without requiring any code. Business Rules can be used to enforce data validation, automate field calculations, and control the visibility and behavior of form elements based on specific conditions. Business Rules can be configured at both the entity and form level, allowing users to create rules that apply to specific entities or forms within the system. Business Rules are executed in the context of the current user invoking some functionality in Dynamics 365 that triggers one or more of these rules. When a user performs an action, such as creating or updating a record, the system evaluates the associated Business Rules and executes any actions that are defined in those rules. The sequence of execution for Business Rules is determined by the order in which they are defined within the system. Business Rules can be used in conjunction with other Dynamics 365 features, such as workflows, Power Automate flows, and web API calls. Workflows and flows can be configured to trigger based on specific conditions or events and can include actions that invoke Business Rules as part of their execution. Web API calls can also be used to execute Business Rules programmatically. Overall, Dynamics 365 Business Rules provide a flexible and powerful way for users to enforce custom business logic within the system, without requiring any code or development expertise. By combining Business Rules with other Dynamics 365 features, users can create complex, automated processes that help streamline their business operations and improve efficiency. The table below lists all the business rules and what level they’ve configured at. It is important to note that form level business rules execute at run time on the client, whereas entity level configured business rules run on the server thus are more reliable as when using API calls, workflows and other automation we want to ensure that business rules are ran. Form level business rules are useful for immediate feedback when populating forms, however entity level will do the same. The only valid scenario that form level business rules are relevant if there is a client-side rule that needs to be implemented and doesn’t need to be interpreted or governed service / targets a specific form. Table Business Rule Level Table 15: Business rules by Table Azure AAD Groups / Teams & environment integrations In the summer of 2022, the wave 2 general available release of the D365 update included support for full integration of AAD groups to D365 Teams and unlike its previous implementation now supports full CRUD. This means that {ORGANIZATION} can administer RBAC directly in AD which is best practice for access control across the organization. The way its designed in system is that we have an AAD group (M365 type) for each team in CRM and that team is mapped to 1 or more security roles. This means that when a user is onboarded to {ORGANIZATION} to requires access to the compliance case management system, the typical request to IT to have the user added as a member to an AAD Group(s) will automatically replicate to CRM thus giving the user automatic access based on their persona governed by the CRM team’s associated security role. Therefore, no one will have privileged access and user administration privileges in the CRM reducing risk of human error and potential security threats by providing a user with a security role that they should not be allowed to be associated it. Furthermore, the security posture is further optimized by automatically revoking access to CRM if the user leaves {ORGANIZATION} and is removed from AD or the AAD group. If a user is promoted into a new role, they would be added to the AAD group associated with the higher-level CRM team and thus inherit the additional security parameters associated with the role tied to the team. The overall strategy is to leverage 1 RBAC at the organization level rather than app level in terms of access. The table below lists the AAD Groups assigned to its respective CRM team and the security role(s) associated to the team. In certain edge cases, a system administrator may need to troubleshoot an issue in production thus an AAD group for system administrator has been created and mapped to each environment (minus dev which this doesn’t apply) to troubleshoot issues. However since this is a privileged role in the CRM, it is recommended that the group is configured in Azures privileged identity management system (PIM) so that we can control the time and force the user to provide a quick justification for why they need to obtain sys admin in an environment that is not development (e.g. sometimes post deployments, we need to manually activate a workflow, or a model driven app change, this is a rare occurrence and due to a faulty deployment but still needs to be done especially in production to avoid bugs). AAD M365 Group Team/GUID CRM Team Security Role(s) / Field Level Permission Profile(s) Table 16: AAD Group Mapping to CRM Team Figure 17: Azure AD Group/RBAC integration with CRM Environment groups AAD groups are also applied at the environment level. This means that each environment user provisioning is restricted to users who are members of these groups. This also doesn’t mean that they automatically get access, this is done via team AAD group integration however, this is best practice to ensure that the entire directory is not available for security role assignment without using the Organization. The table below lists the security groups associated with each CRM organization. Also included is the security group that has the licensing tied to it – which means each user who requires case management and or customer service licensing (same licensing) will inherit these. Once in that group, they must be added to the environment group and the group(s) for RBAC in the environment(s) they access. Team Name Type Environment Power Pages Site (customer self-service) System Configurations & Integrations This section outlines the site settings configured for each portal. Site settings and settings are the key configurations and integration. It is important to note that all non-production portals are “private” and therefore available only to internal users and protected by Azure AD. The public site is available on the internet and has a Canada.ca DNS CNAME entry configured (described in this section). Setting Name Value Description Table 17: Portal Settings Figure 18: Privatization of Portal Site by Azure AD Group (applicable to non production sites)a Diagnostics (Azure Storage) The Power Apps platform allows for the storage of diagnostic data in an Azure Storage account. This feature enables the collection and analysis of diagnostic data for Power Apps. This data can include app telemetry, usage data, and error logs. The storage account is also leveraged to host a static website of this implementation’s technical and business documentation using the Microsoft DocFX framework, which is the same technical documentation framework leveraged by Microsoft documentation and is part of the leading technical documentation frameworks that is also corporately backed (and open sourced) which aligns with TBS cloud usage guidelines. To implement this feature, the following steps were taken (applies to all environments including production): Creation of an Azure Storage account: An Azure Storage account was created using the Azure portal. Acquisition of connection string: The connection string for the Azure Storage account was obtained. This connection string was in the format of \"DefaultEndpointsProtocol=https;AccountName=<your_storage_account_name>;AccountKey=<your_storage_account_key>;EndpointSuffix=core.windows.net\". Configuration of diagnostic settings: Diagnostic settings were configured for the Power Apps environment by navigating to the Power Page Site, selecting the relevant environment, and choosing \"Diagnostics\" from the left-hand navigation menu. Selection of Azure Storage as destination: In the diagnostic settings, Azure Storage was selected as the destination for the diagnostic data. Provision of connection string: The connection string obtained in step 2 was provided. Selection of data types: The types of data to be collected and stored in the Azure Storage account were selected. Saving of settings: The diagnostic settings were saved. Upon implementation, Power Apps began storing diagnostic data in the Azure Storage account. This data can be analyzed using Azure services such as Azure Monitor, Azure Log Analytics, and Azure Data Explorer. Overall, this integration provides a means to collect and analyze diagnostic data, thereby aiding in the identification and resolution of issues that may arise within the Power Apps environment. For example, if the portal has poor performance or integration between B2C or SharePoint is problematic, examining the diagnostic logs provides a more verbose exception trace. The logs are organized by date, and each date has a dedicated folder with x number of log files to analyze. Diagnostic logs are retained for 30 days and in this implementation, logs are used by developers only to help with troubleshooting, the storage accounts are not being monitored by a SIEM or sent to a log analytics workspace. Finally, the storage account blob containers are set to private (no anonymous usage) and are restricted to VPN traffic and Active Directory user thus inherits the access policies of the organization. The table below lists each storage connection by environment. Storage Connection Storage Account / Subscription Environment SharePoint Online Figure 19: SharePoint document journey/Integration with CRM Exchange Online & notification center Figure 20: Emails & Notifications Architecture and integration with Dynamics 365 Server-side integration The portal integrates with CRM tables using a combination of table permissions to allow developers to configure which tables are allowed to be exposed to the portal. These table permissions are described futher in this document. Once table permissions are configured, the developer uses “Liquid” which is the HTML templating language used to code web templates to expose data from the configured tables to the UI. When a portal is created in a Dataverse environment, the engine creates an “App Registration” record in Azure AD which acts as the primary service account for server-to-server communication between the portal and the integrated Dataverse environment. The table below lists all the App Registration records for each portal generated for each downstream environment up to production. App Registration ID Environment Table 18: App Registration for each Portal Data flow and synchronization Power Pages Sites are external-facing websites built on top of the Dataverse platform, which is a cloud-based data storage and management platform provided by Microsoft. These portals allow users to interact with the data stored in Dataverse without requiring them to have direct access to the underlying database. The data flow between Power Pages Sites and the Dataverse backend follows a specific pattern. Here is a step-by-step breakdown of how the data flows to and from Power Pages Sites and the Dataverse backend: User Interaction: Users interact with the Power Page Site using a web browser or mobile app. They can access various types of data, such as customer records, invoices, or service tickets. Portal Website: The Power Page Site website is hosted on Microsoft's cloud infrastructure, and it acts as a front-end interface for users to interact with the data stored in the Dataverse backend. Authentication: Users are authenticated before they can access the portal. They can either authenticate using their Microsoft account or a custom authentication mechanism provided by the portal. Portal Pages: Users navigate through the portal's web pages to access and interact with the data they require. Each page consists of web components such as forms, lists, and charts. Data Requests: When a user requests data, the portal sends a request to the Dataverse backend. The request includes information about the data the user wants to access or modify. Dataverse: The Dataverse backend processes the data request and retrieves the relevant data from the database. The data is returned to the portal in a JSON format. Data Display: The portal displays the data returned by the Dataverse backend in the web components of the portal page. User Actions: Users can take actions on the displayed data, such as updating customer records or creating new service tickets. When the user performs an action, the portal sends a request to the Dataverse backend to update the database. Dataverse Updates: The Dataverse backend processes the user action request and updates the database with the new data. Confirmation: The portal confirms to the user that the action was completed successfully. Data Sync: In case of offline access, the Power Pages Sites support data synchronization, where changes made to the data in the portal are synced back to the Dataverse backend once the connection is re-established. In summary, the data flow between Power Pages Sites and the Dataverse backend involves users interacting with the portal's web pages, and the portal making requests to the Dataverse backend to retrieve or update data. The Dataverse backend processes these requests and sends the requested data back to the portal for display. Users can take actions on the displayed data, and the portal sends requests to the Dataverse backend to update the database accordingly. The illustration below describes the integration of the portal and the integrated CRM environment. Figure 21: Portal’s integration with Dynamics (OOB) Client-side integration For more dynamic functionality, developers can use JavaScript to perform CRUD operations to Dataverse tables. However, a convention has been applied to our implementation that this will be leveraged for read operations only. To configure a table to be read using the web API (and JavaScript), beyond the table permissions, the following site settings must be configured for each table and fields the developer wants available for JavaScript. In the diagram below, the \"Dataverse Portal\" is represented as node A, the WebAPI configured in site settings is represented as node B, and the Dataverse is represented as node C. The arrow indicates the flow of data between the Dataverse Portal and the WebAPI, which is used to read and write data to the Dataverse. Figure 22: WebAPI Flow Diagram For our initial implementation, this feature is being used for the Notification center. The table below demonstrates the site settings used for exposing the notification center to JavaScript. Table 19: WebAPI Settings for Notification Centre This feature is further described in the next section. Portal notifications Portal notifications are available to any user who receives notifications generated by the notification center that contains protected information rather than receiving the information via email. The email will notify the user to login to their account to view the notification. Figure 23: Notification Centre Feature Screen Captures It is important to note that external users have the freedom to choose when they mark a notification item as read which will automatically update the status reason of the notification item. Another key feature of the notification center is the ability for both internal and external users to communicate via a comment thread. The comment thread is captured also as an activity called “Portal Comment” and provides the flexibility for communicating through the notification center and avoid sharing protected information via email. Figure 24: Portal - Notification Center Comments Figure 25: Internal Portal Comment Response The data model behind the notification center is simple, it includes a templating system, a notification item and a direct link to a contact record which is the user of the portal. Figure 26: Notifications ERD Most portal notifications are sent using processes and or flows (listed in table below) thus are automated. However internal users can create manual notifications using the model driven app. The illustrations below demonstrate the process of creating notification items. Finally notifications are configured as “activity” entities thus are treated as activities in CRM such as emails and tasks and therefore are polymorphic and can be linked to any other activity record, in particular for this implementation, case files, contacts and organizations (Reporting Entities). Figure 27: Creating a manual notification item from model driven app Once saved, an email is sent to the recipient with a link to the portal notification centre advising them to sign in to view the notification details. Figure 28: Example notification item email notification Data Architecture, CRM & Portal Design Conventions The Data Architecture section covers the data model and schema design for the CRM case management system, including entities, fields, and relationships, naming conventions and standards, processes and automations for data management, workflows and business process flows, Power Automate Flows, and business rules and validation. Additionally, it discusses validation rules and custom scripts, the business rules engine, and custom actions. CRM & Portal Design Conventions This section describes the design conventions applied to the portal implementation. By applying conventions developers have a simple reference source to minimize the interpretation of how to implement features. Naming conventions and standards for Portal Artifacts The system is maximizing the usage of the D365 customer service & case management licensing feature which comes with features such as enterprise ISO standard case management features such as SLA timers, alerts, integrations with the organizations primary RBAC system (AD), client insights, email integration and robust monitoring and immutability for auditing of a case management lifecycle. In addition, the {ORGANIZATION} specific requirements not only has been adapted to fit within this feature set, but the build has a series of conventions that must be followed when designing the system when it comes to creating tables, fields, relationships from naming conventions and configurations. The next sections describe these conventions and developers must follow these and the DevOps CI/CD will automatically test against any violations of these conventions to ensure that the system is healthy, maintainable and knowledge transfer is implied: Feature name Description Convention (naming) Convention (configuration) Auto-numbering (Case) Auto-numbering (Related Case Form) Table 20: Portal conventions Dataverse (CRM) Tables, columns, and relationships The data architecture implementation has been standardized to leverage and customize the case management feature of the platform. This means that that the “Case (Incident)” table / feature set is the focal point for every type of compliance form process. Each compliance form type is captured as a Subject which is a case artifact that ships with the customer service module. Furthermore, each subject that is “external facing” such as the “Risk Questionnaire” is linked to a Portal Wizard Form which in turn leverages the out of the box Power Pages web link set / web links & basic forms with a custom framework built around it to meet the WET Canada.ca framework standards, accessibility requirements and overall usability and ease of configuration. A compliance case of type “reporting cycle” is what governs the overall process of inviting reporting entities to submit compliance forms. A reporting cycle has several types including “financial institutions and sector specific” (full list in table below) which allows for dynamic assignment of organizations to a reporting cycle (1 or more) – for example, by choosing sector specific the reporting cycle is automatically populated with all active organizations in that sector, by choosing financial institutions, the case creator can select 1 or more Contacts (external portal users) regardless of sector. Figure 29: Logical ERD - Case Management Process. & Integration with Compliance forms (convention) Solutioning (patches) & Managed vs. Unmanaged A primary solution entitled {ORGANIZATION} is created with all assets from table, fields, custom controls, forms, plugins, flows etc. Developers provision patches off this solution each sprint to perform their duties. They use the UI or CLI with VS Code to make changes to their patches and once tested Development they run the CI pipeline to issue a release to staging to validate a successful deployment of their feature. This pipeline will test best practices and other tests described further in this document. Solutions and patches are “unmanaged” in development and “managed” everywhere else. Managed means that configurators cannot make changes to the system without upgrading (re-deploying) a new version of a solution thus guarding against misalignment between configurations between environments. The diagram below depicts how an unmanaged patch or solution is exported as managed and moved downstream using our pipeline. Figure 30: Unmanaged/Managed Solution Deployments This flow chart represents the process of exporting an unmanaged Dynamics 365 solution patch as a managed solution, and then releasing it through a DevOps release pipeline. The process starts with an unmanaged solution patch, which is then exported as a managed solution using the Dynamics 365 solution export tool. The managed solution is then added to a DevOps release pipeline, which includes several stages such as development, test, and production. When the solution is deployed to each environment, it is tested to ensure that it works correctly and does not cause any issues. Once the solution has been successfully deployed to the development environment, it can be promoted to the test environment for further testing. Finally, when the solution has been thoroughly tested and validated, it can be promoted to the production environment for release to end users. The entire process is managed through the DevOps release pipeline, which ensures that the solution is deployed consistently and reliably across all environments. Best practice dictates that the solutions are deployed not only as managed but the setting to “upgrade” the solution is flagged in the pipeline to ensure that that any components that no longer exist is in the new version but present in the previous version are removed from the system. The pipeline also handles staging the upgrade which allows for the pipeline to deploy reference data or remove data prior to deletion of the previous version. The three types of deployment options for managed solutions are described below: Upgrade This is the default option and upgrades your solution to the latest version and rolls up all previous patches in one step. Any components associated to the previous solution version that are not in the newer solution version will be deleted. This option will ensure that your resulting configuration state is consistent with the importing solution including removal of components that are no longer part of the solution. Stage for Upgrade This option upgrades your solution to the higher version but defers the deletion of the previous version and any related patches until you apply a solution upgrade later. This option should only be selected if you want to have both the old and new solutions installed in the system concurrently so that you can do some data migration before you complete the solution upgrade. Applying the upgrade will delete the old solution and any components that are not included in the new solution. Update This option replaces your solution with this version. Components that are not in the newer solution won't be deleted and will remain in the system. Be aware that the source and destination environment may differ if components were deleted in the source environment. This option has the best performance by typically finishing in less time than the upgrade methods. Web Pages/Page Templates/Web Templates Web pages Web pages in Power Page Sites are the fundamental building blocks of the portal site. They define the content, layout, and functionality of the pages that are displayed to end-users. Each web page is associated with a specific entity or set of entities and can contain a variety of components such as forms, lists, charts, and web files. Page templates Page templates define the layout and structure of web pages in Power Page Sites. They provide a standardized framework that can be used to create consistent and reusable pages across the portal site. Each page template is associated with a specific entity or set of entities and can contain a variety of pre-configured components and placeholders that can be filled with content when the web page is created. Web templates Web templates in Power Page Sites define the overall layout and design of the portal site. They provide a high-level view of the portal, including the navigation menu, header, footer, and other site-wide elements. Web templates can be used to create a consistent and professional-looking portal site that is easy to navigate and use. When a user requests a web page, the following steps are typically taken: The user's request is sent to the portal server, which checks to see if the user is authenticated and authorized to access the requested page. If the user is authorized, the portal server retrieves the appropriate web page and any associated page templates. The page template is applied to the web page, which provides the overall layout and structure for the page. Any placeholders or components in the page template are filled with the appropriate content, such as fields from the entity record or custom code. The completed web page is then rendered and sent back to the user's browser for display. Overall, web pages, page templates, and web templates are critical components of Power Pages Site for this implementation as it enables developers to create and manage our wizard forms and other pages relevant to compliance, it’s basically the foundational of the framework. The table below lists all the web pages, associated page and web templates in this implementation along with its naming conventions. Table 21: Web Pages & Templates Content Snippets The content snippet feature in Power Page Sites allows developers and content creators to create reusable blocks of content that can be easily added to web pages and templates. These content snippets can include text, images, links, and other types of content that are frequently used across multiple pages or sections of a site. One important aspect of content snippets in Power Page Sites is that they can be made editable for static content. This means that content creators can modify the content of a content snippet without having to modify the underlying web page or template. This is especially useful for static content that is used across multiple pages, such as a company address or copyright notice. From a translation perspective, the content snippet feature is extremely important. It allows content creators to create a single version of a piece of content, such as a button label or a heading, and then translate that content into multiple languages using the portal's translation feature. This eliminates the need to duplicate content for each language, which can be time-consuming and error prone. For this implementation, we support both French and English content, thus a content snippet of the same name but different language selection is created to automatically generate the content in the user’s language of choice. This can save time and reduce the risk of errors in the translation process. This tool allows developers and content creators to create reusable blocks of content that can be easily added to web pages and templates. By making content snippets editable for static content, content creators can modify the content of a content snippet without having to modify the underlying web page or template. And from a translation perspective, content snippets can greatly simplify the translation process by allowing translators to focus on translating only the text that needs to be translated, rather than translating an entire web page or template. The table below lists all content snippets used in this implementation along with its naming convention. Table 22: Content Snippets Site Markers The Site Markers feature in Power Pages is a tool that allows users to easily create visual markers on their website pages. These markers can be used to highlight important areas of the page or draw attention to specific elements, such as calls-to-action or anchor (URL) tag reuse (most important). Benefits of using Site Markers include: Increased Engagement: Site markers can help grab the attention of visitors and encourage them to interact with the page. Improved User Experience: By highlighting key elements on the page, site markers can help users quickly find the information they are looking for. Enhanced Branding: Site markers can be customized to match a website's color scheme or design, helping to reinforce brand identity. Better Analytics: Site markers can be used to track user behavior and engagement on specific areas of the page, providing valuable insights for optimization. Use of variable feature to point to site marker liquid object rather than a direct hard coded URL. Therefore, in the event of a URL change, the developer can update the Site Marker webpage / URL and wherever the marker is referenced in the application, it will be updated with the new URL. One of the main benefits of using Site Markers is that they can be leveraged for re-usability and inheritance, which can greatly enhance the developer experience. Instead of hard-coding URLs in anchor tags, developers can reference a Site Marker, which can be updated globally across the website. For example, our footer has a series of Site Markers for Canada.ca and {ORGANIZATION} links that appears at the bottom of every page. Rather than hard coding the URL in every anchor tag on the website, the developer can reference the Site Marker for the footer. If one or more of the footer URL changes or the URL(s) needs to be updated, the developer can simply update the Site Marker, and every anchor tag that references that Site Marker will automatically be updated as well. This not only saves time and effort for developers, but also helps ensure consistency and accuracy across the website. Additionally, if the website is ever redesigned or updated, the Site Markers can be easily updated to reflect the changes, without having to manually update every individual anchor tag on the website. In summary, the re-usability aspect of the Site Markers feature in Power Pages allows developers to leverage inheritance and reference markers rather than hard-coding URLs in anchor tags. This can save time and effort, ensure consistency and accuracy, and enhance the developer experience overall. Overall, Site Markers can be a valuable tool for improving the developer experience by creating URL objects thus levering inheritance and avoid the pitfalls of having to update URL’s manually across multiple pages, increasing engagement, and driving conversions to the compliance portal. In this implementation, we make usage of site markers for inheritance exclusively. The table below lists all the site markers used in this implementation. This includes also the naming convention that needs to be applied to the marker name. Site Marker Name URL Web Template/Web Page Table 23: Site Markers Web Roles Power Page Sites, web roles are used to control access to specific areas and features of a portal site. A web role is a collection of permissions that are assigned to a user or group of users, allowing them to perform specific actions or access specific areas of the site. There are several types of web roles in Power Page Sites, including: Parent web role: This type of web role is typically used to control access to the entire portal site. Users with a parent web role have full access to all areas and features of the site. Global web role: This type of web role is used to control access to specific areas of the site that are common to all portal pages. Users with a global web role have access to all portal pages, but only to the areas and features that are covered by the global web role. Contact web role: This type of web role is used to control access to areas of the site that are specific to a contact record in Dynamics 365. For example, a contact web role might be used to control access to a contact's personal information or account history. Self-web role: This type of web role is used to control access to a user's own data and profile information. Users with a self-web role have access to their own data and profile information, but not to the data and profile information of other users. Web roles are assigned to users or groups of users using security roles in Dynamics 365. Once a user has been assigned a web role, they will be able to access the areas of the site and perform the actions that are covered by that role. In summary, web roles in Power Page Sites are used to control access to specific areas and features of a portal site. There are several types of web roles, including parent, global, contact, and self-roles, each with its own set of permissions and access controls. Web roles are assigned to users or groups of users using security roles in Dynamics 365. Note that out of the box, an administrator web role is available and used by developers to perform development tasks and testing. This role is never assigned to any user who hasn’t been approved. This list is tracked as a JSON object in the test folder in the pipeline and if a user who is not approved is discovered in the delta the release is rejected. The table below lists all web roles in this solution. Table 24: Web Roles Translations Translations in Dynamics 365 and Power Page Sites allow you to create multilingual versions of your application or portal. This means that you can translate your user interface, forms, and other components into different languages to support users who speak different languages. In Dynamics 365, translations are managed using a translation file. This file is essentially a spreadsheet that contains all the labels, messages, and other text that appears in your application. You can export the translation file to Excel, translate the text into your desired languages, and then import the translated file back into Dynamics 365. Power Page Sites also support translations and use a similar process to Dynamics 365. When you create a portal, you can specify the languages that you want to support. You can then export the translation file for your portal and translate the text into your desired languages using Excel. Once you have completed the translations, you can import the translated file back into your portal. In both Dynamics 365 and Power Page Sites, translations can be applied to various components, including forms, views, and fields. You can also use translations to localize your application or portal for different regions or countries, by translating region-specific terms or phrases. To make it easier to manage translations in Power Page Sites, you can use the Content Snippet feature. Content snippets are blocks of text that can be reused throughout your portal, such as for page headers or footer text. When you create a content snippet, you can specify which languages it should be available in. This allows you to create translated versions of your content snippets, which can be used throughout your portal. In summary, translations in Dynamics 365 and Power Page Sites allow you to create multilingual versions of your application or portal. Translations are managed using a translation file, which can be exported, translated, and imported back into your application or portal. Content snippets can be used to manage translations more efficiently in Power Page Sites. Table and column permissions The table and column permission feature of Power Pages site allows you to control access to specific tables and columns in your application. This feature is used to restrict access to certain sensitive data in this implementation to ensure that only certain users can view or modify certain data. The way the table and column permissions in Power Pages site were set up is done in the \"Security\" section of the application's settings. From there, the \"Table and Column Permissions\" option has been configured. Once selected, a list of all the tables in the application is displayed for selection. Developers then selected the tables to set permissions for and choose which web roles should have access to these tables. Similarly, the column-level permissions for each table, which allows to control which web roles can view or modify specific columns within that table has been configured. This is like field level permissions in CRM whereby you can configure security at the field level instead of just at the table level. For example, there are certain fields on the case file that can only be available to certain CRM teams and web roles. By setting up table and column permissions in Power Pages site, we ensure that only authorized users have access to the data they need to perform their job functions, while also protecting sensitive data from unauthorized access. Finally, there are different types of permissions such as parent/child, self, contact and global permission types. For example, the case and form table permissions are set up as “parent” to the “Contact” table permission to ensure that only the currently logged in user can view the cases assigned to them rather than having access to all cases in the system. The same applies to notifications and contact specific data. The table below outlines both the table and column permissions configured in this implementation by web role. Table Permission Name Table Type Parent (if applicable) Web Role Figure 31: Table Permissions Column Permission Name Field Type Web Role Table 25: Column Permissions When and when not to use client-side scripting for portals The convention and rule of thumb for applying the ability to use JavaScript (client-side scripting) on the portal is for optimizing the user experience by providing immediate feedback to the user without having to reload the page or redirect. This is the only use case whereby this feature can be leveraged – we do not allow create, update, and delete operations using client-side scripting. The reason is primarily for security – the way browsers work is that it will download the entire HTML, CSS and JavaScript files on the page and users have access to this code and can interact with it using most browser’s developer tools. This means that if we allowed write operations to the portal via JavaScript we may risk attacks such as a user with an active session, auto creating unlimited amounts of records (however the security perimeter would guard against this) or perhaps try and circumvent validation rules on the forms thus despite this feature being well designed and built with security features that make it difficult to perform nefarious actions, we made the conscious choice of not exposing this feature beyond read operations as there are no risks associated with this. The reason for this is that table and column permissions will persist to the WebAPI (so client-side scripting) and therefore is a user attempts to leverage JavaScript to perform read operations, they will be limited to reading their own data that is governed by the permissions feature. Wizard Forms Due to some of the limitations associated with the “advanced forms” feature of Power Pages Sites, this implementation leverages basic forms with a series of custom web templates that render a wizard form using a combination of web link sets, web links and basic forms. This means that a user will first create a web page that is associated to a basic form of type insert which will serve as a parent to all sections of the wizard. This is because the user will first be presented with either a consent screen (which is a webpage with an insert form) and upon submit is redirected to the first step of the wizard which is another web page associated with a basic form of type “edit” that uses the GUID generated by the insert step and appended to the query string of the browser path. Each step of the wizard is a child of the parent insert form web page. Once this is all configured, the user will then create a “web link set” that will be equivalently named to the parent web page and the links created in the web link set will point to each child web page of the insert form web page in the desired order. Finally, the child pages (wizard steps) must use the wizard form step page template for the wizard to render correctly. The illustration below depicts an example web link set which is a wizard. Table 26: Wizard example (web link set) Figure 32: Parent Page (Wizard Insert Page) Figure 33: Wizard Step (child page) Table 27: Wizard Forms Versioning forms Each year, or periodically, {ORGANIZATION} may issue a new version of a form. The requirement however states that retention of the previous form and its data remains intact. Therefore staff will be required to clone (save and copy) the existing form to change, make the changes, clone (save and copy) the existing portal wizard and configure new web pages (optional) and basic form record to point to the new form. The sequence diagram below depicts this. Furthermore, a step-by-step guide of how to do this is also illustrated (and subject to change below) Basic Form Metadata The Power Pages Basic Form feature is a tool for creating and customizing forms on a website or landing page. With this feature, users can create forms with a variety of field types, including text fields, drop-down menus, and checkboxes, and customize the form's appearance to match the design of their website. The Basic Form feature includes a drag-and-drop interface that makes it easy to add and arrange form fields, and users can preview the form as they build it. The feature also allows for the creation of multi-page forms, where users can split longer forms into smaller sections for easier completion by the user. Additionally, the Power Pages Basic Form feature includes options for configuring form submission settings, such as email notifications and confirmation messages for users. The feature also allows users to set up integrations with third-party tools like email marketing software or customer relationship management (CRM) platforms. Overall, the Power Pages Basic Form feature provides a comprehensive solution for creating and managing forms on a website or landing page, with customizable fields, design options, and integration capabilities. This feature is also pivotable to the wizard form, each form step has a web page that is associated with a basic form. The table below lists all basic forms in the system. The name includes the convention. Basic form name Table Webpage Table 28: Basic Forms Libraries: Plugins and custom workflow extensions (DRY & Namespace) Plugins and custom workflow steps are required for more complex functionality that are either inefficient to create in flows or processes or impossible to implement using these features. This is comprised of C# class libraries that are signed by developers for security and run within a sandbox thus cannot interact with third party libraries or security reasons. In this implementation, the table below lists all the class libraries leveraged in this implementation and the namespace (convention). This implementation uses a third party custom workflow step library that is open source to help with mundane behavior that would take weeks to develop and test and these are listed in this table as well. Class library name Type Managed/Unmanaged Table 29: Class Libraries Email templates The Email Template feature in Dynamics 365 is a tool that enables users to create and customize email templates for various purposes, such as marketing campaigns, customer communication, and sales outreach. The feature allows users to create a template with a pre-defined layout and content, which can then be reused for future emails. The Email Template feature includes a drag-and-drop interface that makes it easy to add images, text, and other elements to the email. Users can also use placeholders to dynamically populate email content with data from Dynamics 365 records, such as the recipient's name or company information. Additionally, the Email Template feature allows users to configure email settings such as sender information, subject lines, and delivery options. Users can also preview the email template before sending it, to ensure that it looks and functions as intended. The Email Template feature also includes capabilities for tracking email engagement and performance, such as open rates and click-through rates. Users can use this data to refine their email content and improve the effectiveness of their outreach efforts. Overall, the Email Template feature in Dynamics 365 provides a comprehensive solution for creating, customizing, and sending emails, with powerful design and tracking capabilities that can help improve engagement and drive results. This feature is used in this implementation for sending notification emails to recipients and invitations to register to the portal. Workflow (processes) Processes are already listed in this document; however the convention is simple: Always use async processes unless the process throws a run time validation Create a process for Create and Update and using the naming convention: Create-Update-{name of table} CRM Form design Forms should always have its parent record anchor in the header, the status reason, and modified on at the minimum. Secondly, forms that are being published to the portal must have 1 column layout. Third, the related menu should never display relationships with the same name. This means you need to edit the form to edit the relationship names on the form itself. PCF Controls and Third-Party Libraries PCF Controls are framework driven controls that are official first party open-source tools that enhance user experience in CRM and now has support for portals as well. In this implementation, PCF controls are unnecessary. However, the following third-party libraries are being used to enhance the developer experience when creating processes: Library Name GIT URL When and when not to use the CLI The Power Platform CLI is a key pillar of the release pipelines however developers can leverage the CLI when cloning the GIT repository and working within their own developer environment. Thus, when using VSCode to develop instead of the interface, use the CLI to publish your changes. Never use the CLI to deploy changes to other environments beyond your own development local clone such as registering plugins and making changes to portals and the solution in dev only. Deployments are restricted to DevOps only thus refrain from deploying to downstream environments using the SDK, only the pipeline. Reference Data Reference data should be generated by the SDK and the schema file must be committed to the Data/Schema folder in the GIT repository. When running the pipeline, make sure to include the full time including the extension of the schema file (xml) in the artifact variable. The pipeline will handle the export and import to target (and generate the artifact for downstream releases). Secondly, we have a reference-data.xml file in the Data/Schema folder that will be used to host every table and field that is used for reference data to move across environments. It is preferable for developers to edit this file. However, for quick testing in staging follow the process described in the previous section. User Interface Design This section covers the user interface design principles and best practices, including usability and user experience guidelines, design patterns and templates, navigation and menu structure, site map and navigation controls, dashboards and reports, forms, views, and dashboards, customization options and considerations, interactive and responsive design, accessibility and localization requirements, and accessibility guidelines and standards. User interface design principles and best practices This section describes and outlines the user interface conventions for this implementation. This includes a combination of both industry best practices and best practices that have been devised for this specific implementation. Model Driven Apps (CRM backend) These apps are accessible by {ORGANIZATION} employees. In the initial phase, we have a convention whereby there are two applications targeted to and based on the personas (team membership & security role assignments) of CRM users. The first application is targeted to transactional work comprised of views, forms, and automation such as flows, web resources, and processes designed to interact with data submitted from financial institutions. The second application is targeted to configurators responsible for configuration and development of features that are inherited by the transactional application and the portal application. There is a third important application entitled “Portal Management” which includes every portal configuration artifact however, the {ORGANIZATION} Administration application is designed to include the same artifacts and exclude features or records that are not being leveraged by this application to simplify the configurators and developers’ experience and target just the artifacts leveraged by this application. Finally, its important to note that developers of the platform can implement a third type of application entitled “Canvas Apps” whose purpose is to create a completely customized user interface / experience whereas Model Driven Apps have a more specific set of controls and tailored/specific user interface artifacts which simplifies development and configuration and provides a “low code” experience. In summary, the interfaces in this system have been built using both Power Pages Sites (website application) and Model Driven Apps but future implementations can include Canvas Apps and perhaps other types as the platform continues to mature over the years. Navigation and menu structure The navigation should be tailored to be as efficient as possible for the employee to know what needs to be actioned next. This means that the site map, which is the term that describes the menu system, should prominently display a dashboard of relevant data tailored to the users’ persona. A series of dashboards have been developed, one for each “security role” in the system, and users can choose which dashboard to set as ‘default’ when they first open the application. Users also can create their own dashboards (personal). The table below lists the dashboards created for each persona and provides a screen capture of how it looks. Users should be able to interact with the dashboard efficiently, which means – if the user is responsible for reviewing newly submitted forms, they should see a table with clickable links to applications who’ve they’ve been assigned to work on. A manager or coordinator dashboard would list records that have yet to be assigned and other views such as forms that have been assigned but have yet to change status or might be approaching or have surpassed the service standards (configured in the SLA feature) for actioning a record. The other menus accessible on the same site map should include the activities in the system (so emails, tasks, cases etc.), list of organizations, lists of contacts who belong to these organizations, lists of cases and views of each type of form implemented in the system. Finally, each user will have both a team and personal view of their queue of work, which, despite this data being available in the dashboard, provides a richer set of data columns and features such as actioning multiple files at ones using Flows and processes and re-assign one or more records to other queues (e.g. for escalations). {ORGANIZATION} application Menu Item (in order) Section Purpose Table 30: {ORGANIZATION} Model Driven App Menu Structure (Convention) {ORGANIZATION} Administrator Application Unlike the {ORGANIZATION} application, the administrator app is designed to lists menu items in order of importance or frequency of the menu item’s artifacts’ purpose. For example, reference data menu items, are designed to create records that the transactional application or portal application will see and utilize in drop down menus such as countries, case types, provinces, notification templates, email templates etc. The full list is provided below. Menu Item (in order) Section Purpose Table 31: {ORGANIZATION} Administrator Model Driven App Menu Structure (Convention) Forms & views After adherence to the site map (menu structure) the artifacts within each menu such as views and forms also have a set of conventions primary aimed to show the user relevant information prominently and provide ease of use when it comes to interacting with the records for which the user is responsible to action. When a user navigates to any menu item, they are immediately presented with a view, unless the menu item is targeted to a dashboard record. User’s can interact with data directly within these views by selecting one or more records and pressing a “flow” or “process” or a button in the “ribbon” which is a utility that lives at the top of each view horizontally that presents the user with various buttons and drop-down menus to export reports, data, import data, open the view directly in excel online to make changes in bulk etc. For more complex work and review of data, the user clicks on a view record and are presented with a form. Thus, it is important that guidelines are followed to ensure that both of these artifacts have a consistent user experience across the system so that users get used to the interface quickly and issue feature requests for future releases based on a consistent and repeatable user experience rather than an experience whereby each menu item presents views in a different manner and when clicking on records, forms are organized differently. Moreover, efficiency is a key factor in these conventions, and this really means that what we present to the user should be organized in a way that makes the interaction with the system as efficient and quickly as possible to not only provide them with a better experience but also resulting in higher efficiencies which translates ultimately down to the client (the financial institutions who will be getting faster response times and more efficient reviews etc.). The list below describes the conventions for forms. Convention Title Purpose Example Table 32: {ORGANIZATION} Model Driven App Form Design Conventions The list below describes the conventions for views. Convention Title Purpose Example Table 33: {ORGANIZATION} Model Driven App Views Design Conventions Customization options and considerations In this implementation, we stress the importance of using out of the box tooling as much as possible. In only very rare cases, should a developer bring in third party libraries to enhance usability unless well supported by the community behind it or is a first party (MS supported) library. By introducing the usage of third party libraries and or controls, the more technical debt is added to the system thus adding more complexity and potential maintenance and run time exception problems in the future due to things such as depreciations not being addressed by the third party library or running into the risk of over-complicating the system just to simplify a feature for the sake of convenience. There are legitimate user experience enhancements that do require the usage of third-party libraries or web resources (custom HTML/JS resources) which are listed below. Also listed below are conventions used when customizations outside the standard OOB tooling provides is implemented. Library Name Purpose Table 34: Third/First Party Library/Controls Usage/Conventions Convention Title Purpose Example Table 35: Conventions when developing Class Libraries and Web Resources Portals (Power Pages Site) The website for this application adheres to WCAG regulations that are facilitated by the implementation of the Canada.ca WET framework implementation in the portal. This CSS/JS framework supports screen readers and other tools used by folks with an impairment in using a traditional keyboard and mouse or trackpad. This framework has been fully implemented on this web site and affects everything from tables shown to users, buttons, forms, links, and navigation. In addition to addressing accessibility, web usability and industry norms are followed so that users who interact with the portal are experiencing a familiar user experience in comparison to other frequently visited web sites with the exception of heavy usage of JavaScript as this will often violate accessibility guidelines. JavaScript is being used in the portal in many areas but our convention is to ensure that for accessibility and security reasons, JavaScript is leveraged for read operations only and rarely used for writing or purging data using the web API. Exceptions to these rules are described in the following sections. Interactive and responsive design Accessibility and localization requirements Accessibility guidelines and standards Multilingual and cultural considerations Integration Architecture The Integration Architecture section covers integration patterns and principles, including API-based integration and connector-based integration, and the APIs and connectors used for integration, such as Dynamics 365 Web API, Microsoft Graph API, and Power Automate Connectors. Additionally, it covers data synchronization and replication, integration scenarios and patterns, data mapping and transformation, and security and authentication requirements for integration, such as OAuth and Azure AD authentication and authorization and permissions management. Integration patterns and principles. API-based integration Connector-based integration APIs and connectors used for integration. Dynamics 365 Web API Microsoft Graph API Power Automate Connectors Data synchronization and replication Integration scenarios and patterns Data mapping and transformation Security and authentication requirements for integration OAuth and Azure AD authentication Authorization and permissions management Security and Access Control The Security and Access Control section covers role-based access control (RBAC) design, user roles and permissions, hierarchical security models, Azure Active Directory integration with Dynamics 365 teams, user and group synchronization, user provisioning and de-provisioning, permissions and privileges management, security roles and profiles, privileges and access levels, authentication and authorization requirements, password policies and security standards, multi-factor authentication, and data protection and compliance considerations, such as data encryption and protection, GDPR, and data privacy regulations. Hierarchical security model (business units / teams) Azure Active Directory integration with Dynamics 365 teams (user provisioning) Azure Active Directory (Azure AD) groups can be integrated with Dynamics 365 teams to enable centralized role-based access control (RBAC) management across Dynamics 365 applications. RBAC is a security model that assigns permissions to users based on their roles within an organization. By using Azure AD groups to manage RBAC in Dynamics 365 teams, organizations can benefit from a centralized and efficient approach to managing user access to Dynamics 365 applications. Contacts (external portal users) a step-by-step explanation of how Azure AD groups integrate with Dynamics 365 teams: Create an Azure AD group: To begin, create an Azure AD group that will be used to manage access to Dynamics 365 teams. This group should include all of the users who require access to Dynamics 365 applications. Assign roles to the Azure AD group: Next, assign one or more roles to the Azure AD group that correspond to the access levels required by users. Add the Azure AD group to Dynamics 365 teams: Once the Azure AD group has been created and roles have been assigned, it can be added to Dynamics 365 teams. This will automatically assign the appropriate roles to all the users in the Azure AD group. Manage RBAC through Azure AD group membership: From this point on, RBAC can be managed centrally by managing Azure AD group membership. If a user requires access to additional Dynamics 365 applications, simply add them to the appropriate Azure AD group and assign the corresponding roles. By leveraging this feature, organizations can benefit from several advantages: Centralized management: By using Azure AD groups to manage RBAC in Dynamics 365 teams, organizations can benefit from centralized management of user access across multiple Dynamics 365 applications. Simplified administration: By managing RBAC through Azure AD group membership, administrators can more easily add or remove users from roles as needed, without having to manage individual user accounts. Increased security: By using Azure AD groups to manage RBAC, organizations can benefit from the security features of Azure AD, including multi-factor authentication, conditional access policies, and more. Increased efficiency: By centralizing RBAC management, organizations can streamline the process of managing user access across multiple Dynamics 365 applications, reducing the time and effort required for administration. In summary, Azure AD groups can be integrated with Dynamics 365 teams to enable centralized RBAC management across multiple Dynamics 365 applications. This approach provides several benefits, including centralized management, simplified administration, increased security, and increased efficiency. In the following section, the implementation of this feature is described in detail for this implementation. The goal is to leverage Active Directory to manage the RBAC of the system rather than relying solely on the application layer thus avoiding pitfalls such as stale users or users who are not assigned to the right team and permissions. User and group synchronization & User provisioning and de-provisioning The table below lists the teams, their associated security roles, and the associated Azure AD group. By adding the user to the Azure AD group, they automatically are added as members to its associated team and inherits the teams’ permissions. This applies to also removing a user from an Azure AD group and adding them to another group. Users who are disabled in AD or purged also have their access revoked from CRM. Team Security Role(s) AAD Group Table 36: Teams & AD Groups Figure 34: D365 Team Integration with AD Group (repetitive) Azure KeyVaults for storing certificates, secrets, and keys. Encryption and decryption Logging and monitoring A log analytics workspace has been provisioned in the nonproduction and production Azure subscriptions hosting the azure workloads that support the Power Platform. In this implementation, the Power Platform logs (Portal and CRM) are sent to the workspace, so are the B2C & SharePoint logs. These workspaces are scanned by the CCCS sensors and logs sent to the GOC SOC. The log analytics workspace locations are listed below. Subscription Name Workspace Name File and data storage Files (unstructured data) is mostly stored in SharePoint. This includes form related attachments and attachments to various case files, accounts, contacts, and forms. Diagnostic logs, which are files as well (JSON format) are stored in Azure Storage accounts located in the Power Platform Azure supporting subscriptions. The Dataverse also stores “web files” which are files such as CSS, JS, and images to support portal asset rendering. Azure Gateway as a proxy perimeter to the portal {ORGANIZATION} has implemented SCED thus a physical connection to the Canadian Azure Data Centres using express route. Therefore, the portal (pre-prod and production) is proxied through this service allowing {ORGANIZATION} to control the perimeter services via a series of assigned IP’s controlled by SSC and governed by the F5 firewall. This is an unconventional but working solution as Power Page Sites provides native support to Azure Front Door as a CDN / Proxy service however {ORGANIZATION} has no plans to implement front door. Network security and perimeter protection Microsoft 365 provides several network security perimeter services to its staff who use the Power Platform, including Power Apps, Power Automate, and Power BI. These services are designed to help ensure that customer data and applications are protected against security threats, both within and outside the Microsoft 365 environment. This is why SAAS is outside of the scope of SCED as GOC operators have no access to the perimeter or infrastructure hosting M365 (so the Power Platform).6 Some of the key network security perimeter services provided by Microsoft 365 include: Azure Active Directory (Azure AD) - This service provides authentication and access management for Power Platform applications, allowing staff to control who can access their data and applications, and what they can do with them. Data Loss Prevention (DLP) - DLP policies can be configured to help prevent data loss or leakage from Power Platform applications, by monitoring and controlling access to sensitive data. By default, a global DLP has been applied to the Power Platform tenant to block all third party and first party connectors to services outside of the Microsoft ecosystem. Conditional Access - This feature allows staff to set policies that control access to Power Platform applications based on a range of factors, including user location, device type, and more. Network security groups - These are firewall rules that can be used to control inbound and outbound network traffic to and from Power Platform applications, helping to prevent unauthorized access and data exfiltration. Azure Private Link - This service allows staff to securely access their Power Platform applications over a private network connection, rather than over the public internet. In addition to these network security perimeter services, Microsoft 365 also provides protection for public Power Pages sites created using Power Apps. Specifically, Power Pages sites are protected by Azure Front Door, which provides security and scalability for web applications. Azure Front Door provides several security features, such as SSL termination, DDoS protection, and bot protection, to protect against common web-based attacks. Overall, Microsoft 365 provides a comprehensive set of network security perimeter services to its staff who use the Power Platform, helping to ensure that customer data and applications are protected against security threats both within and outside the Microsoft 365 environment. Data Loss Prevention The Data Loss Prevention (DLP) feature in the Power Platform allows administrators to control the flow of sensitive data across various Power Platform services, such as Power Apps, Power Automate, and Power BI. With DLP policies, you can restrict the use of sensitive data in the Power Platform, ensuring that only authorized users have access to it. DLP policies can be applied to specific data types, such as credit card numbers, social security numbers, or other custom data types. You can define the actions that should be taken when a user attempts to share or use data that matches the defined policies, such as blocking access, notifying an administrator, or prompting the user to provide a justification for accessing the data. However, it's important to note that the DLP feature has some limitations, especially when targeting out-of-the-box (OOB) flow connectors. OOB flow connectors are pre-built connectors that come with the Power Platform and allow users to connect to various services such as SharePoint, Dynamics 365, or Microsoft Teams. When you apply DLP policies to the Power Platform, it's essential to understand that DLP settings should still be configured tenant-wide in Azure. This means that if you apply DLP policies to specific data types in the Power Platform, it won't necessarily prevent users from sharing or using that data in other Microsoft services outside the Power Platform. Therefore, it's recommended that you configure your DLP policies in Azure to ensure that sensitive data is protected across all Microsoft services, including the Power Platform. In summary, the DLP feature in the Power Platform provides a useful tool for controlling the flow of sensitive data within the Power Platform. However, it's important to understand its limitations when targeting OOB flow connectors, and to configure DLP settings tenant-wide in Azure to ensure complete protection of sensitive data across all Microsoft services. For this implementation, we’ve created a DLP policy exemption to allow D365 to communicate with the organization master API since the Global DLP policy on the tenant prevents usage of the HTTP connector which is what needs to be leverages for perform API calls. The DLP settings are managed by the cloud operation folks and when a new connector is required for a Power Automate Flow such as for example integrating with a third party system such as workday, SAP, or any other app which may have already a pre-built connector in the marketplace, the operator can create a DLP policy for the environment that requires this, while following the usual security measures of adherence to ITSG PBMM security guardrails / controls. Traffic management and load balancing Due to the nature of the system being a SAAS technology – traffic management and load balancing is abstracted by Microsoft 365. Therefore, {ORGANIZATION} does not need to configure these features. If the applications are behaving slowly, it is typically due to mis configurations such as to many synchronous processes running which is a violation of our conventions and easy to identify by viewing the process logs. If the performance issues are difficult to identify, a ticket can be opened with Microsoft to help identify and resolve the issue. Azure KeyVaults for Certificates, Secrets, and Keys An Azure KeyVaults instance has been provisioned for both nonproduction and production Subscriptions that host the Azure Workloads that support the Power Platform. In these KeyVaults is where we store the application App Registration secrets, TLS certificates, private IP addresses and other sensitive information such as DevOps library variable groups. Access to the KeyVaults is constrained to subscription contributors only thus developers do not have access to view this information, they must request temporary access to access a key, secret or certificate if needed for their development work. The table below lists the secret names stored in KeyVaults (not values). KeyVault Artifact Name Purpose Subscription Table 37: Azure KeyVault Artifacts KeyVaults are especially useful to protect our pipeline variable secrets. The diagram below depicts how this connection works, how variable groups connect to KeyVaults to obtain the data required to issue deployments. Figure 35: DevOps Variable Group integration with KeyVaults In this flow chart, the Azure DevOps variable group (represented by the node A) is integrated with a Key Vault (represented by the node B). The flow starts with the variable group, which needs to retrieve secrets from the Key Vault to set the values of its variables. This is done by getting secrets from the Key Vault (represented by the node C), which are then used to set the variable values in the Azure DevOps variable group (represented by the node D). Overall, this flow chart illustrates the basic steps involved in integrating Azure DevOps variable groups with a Key Vault to store and reference their values. Azure DevOps (GIT & CI/CD) Azure DevOps is a powerful platform that can be utilized to automate the deployment of Dynamics 365 solutions, reference data, and portals (PowerApps) using the Power Platform CLI and marketplace helpers. The Power Platform CLI is a command-line interface tool that allows developers to manage and automate the deployment of Power Platform resources, while marketplace helpers are pre-built scripts that automate common deployment scenarios. By integrating these tools into Azure DevOps pipelines, developers can easily deploy and test Dynamics 365 solutions, reference data, and portals with a few simple commands. In addition to simplifying the deployment process, utilizing Azure DevOps pipelines for deployment offers several benefits over manual deployments in Dynamics. Firstly, it allows for greater control and visibility over the deployment process, as developers can easily track changes and errors in the deployment pipeline. This ensures that any issues are identified and resolved quickly, reducing the risk of errors or misconfigurations in downstream environments. Furthermore, by automating the deployment process, developers can save time and reduce the risk of human error, allowing them to focus on other important tasks. With the implementation's strict policy to only allow system administrator privileges to developers in a developer environment, using Azure DevOps pipelines to deploy to a staging environment ensures that only approved changes are promoted to downstream environments. The pipeline outputs errors and misconfigurations from the assets the developer is trying to deploy, ensuring that only valid and functioning changes are deployed to downstream environments. This approach ensures a controlled and consistent deployment process, reducing the risk of errors and minimizing downtime. Once successfully deployed to staging, a release manager can issue the release of the artifacts generated from the successful release to staging (build) to other downstream environments such as QA, UAT, PREPROD, and PROD, ensuring that the deployment process is repeatable and scalable. This section discusses various design paradigms for the application layer, including schema, naming conventions, processes (automations), business rules, Power Automate Flows, and Role-Based Access Control. It also covers the APIs and connectors used for integration. Developer main tools CLI Developer tooling required for this implementation is primarily the CLI extension in VSCode. \\ Ensure the following tooling is installed using these commands. > pac tool list ToolName Installed Version Nuget Status CMT No N/A 9.1.0.80 not yet installed; 'pac tool CMT' will install on first launch PD No N/A 9.1.0.104 not yet installed; 'pac tool PD' will install on first launch PRT Yes 9.1.0.155 9.1.0.155 ok Follow the same procedure to download and launch the CMT and PD tools. If a tool is already installed, the pac tool <toolname> command will simply launch the latest installed version of the tool. You need to periodically update these tools using the following commands > pac tool list ToolName Installed Version Nuget Status CMT No N/A 9.1.0.80 not yet installed; 'pac tool CMT' will install on first launch PD No N/A 9.1.0.104 not yet installed; 'pac tool PD' will install on first launch PRT Yes 9.1.0.155 9.1.0.155 ok > pac solution pack help Help: Package solution components on local filesystem into solution.zip (SolutionPackager) Commands: Usage: pac solution pack --zipfile [--folder] [--packagetype] [--log] [--errorlevel] [--singleComponent] [--allowDelete] [--allowWrite] [--clobber] [--map] [--sourceLoc] [--localize] [--useLcid] [--useUnmanagedFileForMissingManaged] [--disablePluginRemap] [--processCanvasApps] --zipfile The full path to the solution ZIP file (alias: -z) --folder The path to the root folder on the local filesystem. When unpacking/extractins, this will be written to, when packing this will be read from. (alias: -f) --packagetype When unpacking/extracting, use to specify dual Managed and Unmanaged operation. When packing, use to specify Managed or Unmanaged from a previous unpack 'Both'. Can be: 'Unmanaged', 'Managed' or 'Both'; default: 'Unmanaged' (alias: -p) --log The path to the log file. (alias: -l) --errorlevel Minimum logging level for log output [Verbose|Info|Warning|Error|Off]; default: Info (alias: -e) --singleComponent Only perform action on a single component type [WebResource|Plugin|Workflow|None]; default: None. (alias: -sc) --allowDelete Dictates if delete operations may occur; default: false. (alias: -ad) --allowWrite Dictates if write operations may occur; default: false. (alias: -aw) --clobber Enables that files marked read-only can be deleted or overwritten; default: false. (alias: -c) --map The full path to a mapping xml file from which to read component folders to pack. (alias: -m) --sourceLoc Generates a template resource file. Valid only on Extract. Possible Values are auto or an LCID/ISO code of the language you wish to export. When Present, this will extract the string resources from the given locale as a neutral .resx. If auto or just the long or short form of the switch is specified the base locale for the solution will be used. (alias: -src) --localize Extract or merge all string resources into .resx files. (alias: -loc) --useLcid Use LCID's (1033) rather than ISO codes (en-US) for language files. (alias: -lcid) --useUnmanagedFileForMissingManaged Use the same XML source file when packaging for Managed and only Unmanaged XML file is found; applies to AppModuleSiteMap, AppModuleMap, FormXml files (alias: -same) --disablePluginRemap Disabled plug-in fully qualified type name remapping. default: false (alias: -dpm) You can use the Table definition browser to view information for all the tables your Dataverse environment. The Table definition browser is a managed solution you can download here: Microsoft Downloads: MetadataBrowser_3_0_0_5_managed.zip After you download the solution, you must import it to be able to use it. Sign into Power Apps. In the left navigation pane, select Solutions, and then select Import on the command bar. On the Import a solution page, select Browse to locate the solution file (.zip) you downloaded, and select it. Select Next. Information about the solution is displayed. Select Import, and then finish the import process. After you import the solution successfully, locate the app by selecting Apps in the left navigation pane; the app is listed as Metadata Tools. On opening the app, Entities is the default view that lets you view all the tables. Entitles view. You can perform the following actions: View Entity Details: Select a table to view using the Entity Metadata view. Edit Entity: Open the selected form in the default organization, if the table supports this. Text Search: Perform a text search to filter displayed tables using the following table properties: SchemaName, LogicalName, DisplayName, ObjectTypeCode, or MetadataId. Filter Entities: Set simple criteria to view a sub-set of tables. All criteria are evaluated using AND logic. Filter Properties: Filter the properties displayed for any selected table. There are nearly 100 properties in the list. Use this to select just the ones you are interested in. Entity Metadata view Select Entity Metadata to inspect individual tables. Metadata view. You can perform the following actions for a single table: Entity: Select the table from the drop-down list that you want to view. Properties: View all the properties for the table and filter the properties displayed. Edit Entity: Open the selected table edit form in the default organization if the table supports this. Filter Properties: Filter the properties displayed for any selected table. There are nearly 100 properties in the list. Use this to select just the ones you are interested in. Attributes: View the table columns in a master/detail view. With this view you can: Edit Attribute: Open the selected attribute form in the default organization if the attribute supports this. Text Search: Perform a text search to filter displayed columns using the following attribute properties: SchemaName, LogicalName, DisplayName, or MetadataId. Filter Attributes: Filter columns by any attribute property values. Filter Properties: Filter the properties displayed for the selected attribute. Keys: If alternate keys are enabled for a table, you can examine how they are configured. Relationships: View the three types of table relationships: One-To-Many, Many-To-One, and Many-To-Many. With these views you can: Edit Relationship: Open the selected relationship form in the default organization if the relationship supports this. Text Search: Perform a text search to filter displayed relationships using values relevant to the type of relationship. Filter Properties: Filter the relationship by any relationship property value. Privileges: View table privileges. With this view you can: Filter the displayed privilege using the PrivilegeId. Tables are defined by table definitions. By defining or changing the table definitions, you can control the capabilities of a table. To view the table definitions for your environment, use the metadata browser. Download the table definitions browser. More information: Browse table definitions for your environment This topic is about how to work with tables programmatically. To work with tables in Power Apps See Tables in Dataverse. Tables can be created using either the organization service or the Web API. The following information can be applied to both. With the organization service you will use the EntityMetadata class. More information: Create a custom table using code and Retrieve, update, and delete tables With the Web API you will use the EntityMetadata EntityType. More information : Create and update table definitions using the Web API. Table definitions operations How you work with table definitions depends on which service you use. Since the Web API is a RESTful endpoint, it uses a different way to create, retrieve, update, and delete table definitions. Use POST, GET, PUT, and DELETE HTTP verbs to work with table definitions entity types. More information : Create and update table definitions using the Web API. One exception to this is the RetrieveMetadataChanges Function provides a way to compose table definitions query and track changes over time. If working with Organization Service, use RetrieveMetadataChangesRequest class. This class contains the data that is needed to retrieve a collection of table definitions records that satisfy the specified criteria. The RetrieveMetadataChangesResponse returns a timestamp value that can be used with this request at a later time to return information about how table definitions has changed since the last request. Message Web API SDK Assembly CreateEntity Use a POST request to send data to create a table. CreateEntityRequest DeleteEntity Use a DELETE request to delete a table. DeleteEntityRequest RetrieveAllEntities Use GET request to retrieve table data. RetrieveAllEntitiesRequest RetrieveEntity RetrieveEntity Function RetrieveEntityRequest UpdateEntity Use a PUT request to update a table. UpdateEntityRequest RetrieveMetadataChanges Used together with objects in the Microsoft.Xrm.Sdk.Metadata.Query namespace to create a query to efficiently retrieve and detect changes to specific table definitions. More information: Retrieve and detect changes to table definitions. RetrieveMetadataChanges Function RetrieveMetadataChangesRequest Options available when you create a custom table The following lists the options that are available when you create a custom table. You can only set these properties when you create a custom table. Option Description Create as custom activity You can create a table that is an activity by setting the IsActivity property when using the organization service or Web API respectively. For more information, see Custom Activities in Dynamics 365. Table Names There are two types of names, and both must have a customization prefix: LogicalName: Name that is the version of the table name that is set in all lowercase letters. SchemaName: Name that will be used to create the database tables. This name can be mixed case. The casing that you use sets the name of the object generated for programming with strong types or when you use the REST endpoint. Note: If the logical name differs from the schema name, the schema name will override the value that you set for the logical name. When a table is created in the application in the context of a specific solution, the customization prefix used is the one set for the Publisher of the solution. When a table is created programmatically, you can set the customization prefix to a string that is between two and eight characters in length, all alphanumeric characters and it must start with a letter. It cannot start with “mscrm”. The best practice is to use the customization prefix defined by the publisher that the solution is associated with, but this is not a requirement. An underscore character must be included between the customization prefix and the logical or schema name. Ownership Use the OwnershipType property to set this. Use the OwnershipTypes enumeration or OwnershipTypes EnumType to set the type of table ownership. The only valid values for custom tables are OrgOwned or UserOwned. For more information, see Table Ownership. Primary Column With the Organization service, use CreateEntityRequest.PrimaryAttribute property to set this. With the Web API the JSON defining the table must include one StringAttributeMetadata with the IsPrimaryName property set to true. In both cases string column must be formatted as Text. The value of this column is what is shown in a lookup for any related tables. Therefore, the value of the column should represent a name for the record. Enable table capabilities. The following lists table capabilities. You can set these capabilities when you create a table or you can enable them later. Once enabled, these capabilities cannot be disabled. Capability Description Business Process flows Set IsBusinessProcessEnabled to true in order to enable the table for business process flows. Notes To create a relationship with the Annotation table and enable the inclusion of a Notes area in the form. By including Notes, you can also add attachments to records. With the Organization service, use the CreateEntityRequest or UpdateEntityRequest HasNotes property With the Web API set the EntityMetadata.HasNotes property. Activities To create a relationship with the ActivityPointer table so that all the activity type tables can be associated with this table. With the Organization service use the CreateEntityRequest or UpdateEntityRequest HasActivities property. With the Web API, set the EntityMetadata.HasActivities property. Connections To enable creating connection records to associate this table with other connection tables set the IsConnectionsEnabled.Value property value to true. Queues Use the IsValidForQueue property to add support for queues. When you enable this option, you can also set the AutoRouteToOwnerQueue property to automatically move records to the owner’s default queue when a record of this type is created or assigned. E-mail Set the IsActivityParty property so that you can send e-mail to an e-mail address in this type of record. Editable table properties The following lists table properties that you can edit. Unless a managed property disallows these options, you can update them at any time. Property Description Allow Quick Create Use IsQuickCreateEnabled to enable quick create forms for the table. Before you can use quick create forms you must first create and publish a quick create form. Note: Activity tables do not support quick create forms. Access Teams Use AutoCreateAccessTeams to enable the table for access teams. See About collaborating with team templates for more information. Primary Image If a table has an image column you can enable or disable displaying that image in the application using PrimaryImageAttribute. For more information see Image columns Change display text The managed property IsRenameable prevents the display name from being changed in the application. You can still programmatically change the labels by updating the DisplayName and DisplayCollectionName properties. Edit the table Description The managed property IsRenameable prevents the table description from being changed in the application. You can still programmatically change the labels by updating the Description property. Enable for use while offline Use IsAvailableOffline to enable or disable the ability of Dynamics 365 for Microsoft Office Outlook with Offline Access users to take data for this table offline. Enable the Outlook Reading Pane Note: The IsReadingPaneEnabled property is for internal use only. To enable or disable the ability of Office Outlook users to view data for this table, use the Outlook reading pane. You must set this property in the application. Enable Mail Merge Use IsMailMergeEnabled to enable or disable the ability to generate Office Word merged documents that use data from this table. Enable Duplicate Detection Use IsDuplicateDetectionEnabled to enable or disable duplicate detection for the table. For more information, see Detect duplicate data using code Enable SharePoint Integration Use IsDocumentManagementEnabled to enable or disable SharePoint server integration for the table. For more information, see Enable SharePoint document management for specific entities. Enable Dynamics 365 for phones Use IsVisibleInMobile to enable or disable the ability of Dynamics 365 for phones users to see data for this table. Dynamics 365 for tablets Use IsVisibleInMobileClient to enable or disable the ability of Dynamics 365 for tablets users to see data for this table. If the table is available for Dynamics 365 for tablets you can use IsReadOnlyInMobileClient to specify that the data for the record is read-only. Enable Auditing Use IsAuditEnabled to enable or disable auditing for the table. For more information, see Configure table and columns for Auditing. Change areas that display the table You can control where table grids appear in the application Navigation Pane. This is controlled by the SiteMap. Add or Remove Columns As long as the managed property CanCreateAttributes.Value allows for creating columns, you can add columns to the table. For more information, see Column definitions. Add or Remove Views As long as the managed property CanCreateViews.Value allows for creating views, you can use the SavedQuery table to create views for a table. Add or Remove Charts As long as the managed property CanCreateCharts.Value allows for creating charts and the IsEnabledForCharts table property is true, you can use the SavedQueryVisualization table to create charts for a table. For more information, see View data with visualizations (charts). Add or Remove table relationships There are several managed properties that control the types of relationships that you can create for a table. For more information, see Table relationship definitions. Change Icons You can change the icons used for custom tables. For more information, see Change model-driven app custom table icons Can Change Hierarchical Relationship CanChangeHierarchicalRelationship.Value controls whether the hierarchical state of relationships included in your managed solutions can be changed. Messages supported by custom tables. Custom tables support the same base messages as system tables. The set of messages available depends on whether the custom table is user-owned, or organization owned. User-owned tables support sharing, so messages such as GrantAccess, ModifyAccess, and RevokeAccess are available. Best practices and guidance when using Microsoft Dataverse Microsoft Dataverse provides an extensible framework that will allow developers to build highly customized and tailored experiences. While customizing, extending, or integrating with Dataverse, a developer should be aware of the established guidance and best practices. Within this section you will learn about the issues we have identified, their impact, and guidance to resolve those issues. We will explain the background about why things should be done in a certain way and avoid potential problems in the future. This can benefit the usability, supportability, and performance of your environment. The guidance documentation supports the existing information within the Developer and Administration guides. Targeted customization types The documentation targets the following customization types: Custom workflow activities and plug-ins Working with Dataverse data Integrations extending Dataverse. Sections Each guidance article includes most or all the following sections: Title - description of the guidance Category - one or more areas impacted by not following the guidance. Impact potential - the level of risk (high, medium, or low) of affecting the environment by not following the guidance. Symptoms - possible indications that the guidance has not been followed. Guidance - recommendations that may also include examples. Problematic patterns - description or examples of not following the guidance. Additional information - supporting details for a more extensive view. See also - references to learn more about something mentioned in the article. Categories Each guidance article is classified with one or more of the following categories: Usage – improper usage of a particular API, pattern, or configuration Design – design flaws in a customization Performance – customization or pattern that may produce a negative effect on performance in areas such as memory management, CPU utilization, network traffic, or user experience. Security – potential vulnerabilities in a customization that could be exploited in a runtime environment. Upgrade Readiness - customization or pattern that may increase risk of having an unsuccessful version upgrade. Online Migration - customization or pattern that may increase risk of having an unsuccessful online migration. Maintainability – customization that unnecessarily increases the amount of developer effort required to make changes, the frequency of required changes, or the chance of introducing regressions. Supportability – customization or pattern that falls outside the boundaries of published supportability statements, including usage of removed APIs or implementation of forbidden techniques. Release Pipeline The release pipeline for this project is implemented in Azure DevOps. Is comprised of not only automated release pipelines but our repository of requirements and version control using a GIT repository. There are two key pillars of the pipeline, one being the frequently executed from entitled “{ORGANIZATION}-CI” (CI=Continuous integration) and the second being the actual releases to all downstream environments up to production. The CI pipeline gives flexibility to developers to release at a staging environment at will to test their builds and store their artifacts into a release. Come time for a deployment to UAT and beyond, developers can choose any CI release and issue the release to the desired environment where tech leads are responsible for approving. This process is visualized below in both diagrams, one describing the GIT repository process and the second the release pipeline. Figure 36: Release Pipeline - releases (CI) In this diagram, the following pillars are key to each step to the pipeline: Source Environment: The environment from which the Developer-specified solution patch is fetched. Unpack Solution: A step in the pipeline that unpacks the solution or patch. Repack Solution: A step in the pipeline that repacks the solution or patch as a zip file. Reference Data Migration: A step in the pipeline that migrates reference data, if needed. Create Snapshot: A step in the pipeline that creates a manual snapshot of the existing target. Here's a description of the latest pipeline: The Developer specifies the solution patch via an artifact variable. Azure DevOps fetches the solution patch from the Source Environment. Azure DevOps runs the Solution Checker, Power Portal Checker, and Custom Scripts. Azure DevOps unpacks the solution or patch. Azure DevOps stores the results in the GIT Repository associated with the project. Azure DevOps repacks the solution or patch as a zip file and manages it for deployment to the target. Azure DevOps deploys the solution or patch to Production, subject to an Approval Gate. Production reviews the changes and either approves or rejects them. If the Developer has specified that reference data needs to be migrated, Azure DevOps migrates the reference data. Azure DevOps creates a manual snapshot of the existing target. Azure DevOps notifies the Developer of the Approval result. Once a successful build is ready to de deployed to downstream environments up to production, the developer will issue a release directly from the successful pipeline execution. This process is further illustrated below. Figure 37: Deployment to Downstream Environments (CD) Build and Test Automation The process of building solutions for this implementation involves creating a master solution entitled “{ORGANIZATION}” and developers will provision patches off this solution to perform their work. Once they’ve made their development and configuration changes inside their patch, they are required to deploy this to a staging environment using the {ORGANIZATION}-CI pipeline to test their work. If successful, they can issue the release from the pipeline release’s generated artifacts. This applies to both solution patches and portal changes. For class libraries which are C# .net framework based, the need to commit their code to the repository and register their plugin or custom workflow step using the “plugin registration” tool available in the CLI using the following command: pac tool prt This will launch the tool and the developer must authenticate to the tool using their AAD account and register the plugin and their steps (and images when required). Furthermore, plugins must be signed by a developer key to avoid de-compilation by a nefarious actor. The table below lists the plugins developed specifically for this implementation – for now, the only plugin required is to enhance the bilingualism behavior of lookup fields on CRM form and views. Plugin Name Class Library Table 38: Class Libraries (custom) Once a plugin or custom workflow step is registered to the development environment, the developer is responsible to add it to their patch and deploy via the pipeline. Under no circumstances a developer will manually register plugins or workflow steps to any “managed” environment, only in development. Deployment Automation Developers have access to the COMPLIANCE-PATCH pipeline whereby they can release their patches, configuration data and portal configurations to the Staging environment (managed). Patches do not need to be committed by the develop to the repository, the only responsibility for the developer is to provide the logical name (not display name) of the patch or solution being deployed and provide a version number that is increment by a number (4th integer) lower than a patch whose been provisioned later than the one being deployed. For data files being transferred, the developer is responsible to generate the schema file using the configuration migration tool from the SKD and commit the xml file into the Data/Schema directory. Once committed, the developer will be responsible for providing the filename and extensions (always filename.xml) and the pipeline will handle the export from source and import to target. Developers can run up to 4 artefacts containing both solutions and data schema files and these artifacts will run in order. Furthermore, the unmanaged solutions will be installed always as managed in the target environment as per best practice and to avoid future major issues with out of sync environments. Finally, developers also can deploy their latest portal changes by setting “Yes” in the “Deploy Portal (Yes/No)” variable (which is set to know by default. The Comments parameter is leveraged for populating a git commit message as the pipeline will commit all artifacts to the repo and if no comment is found a default one is set which concatenates the personas name and time stamp. The pipeline will also automatically produce a backup of the target environment before deployment to recover for disaster recovery. This backup is stored in the same Azure Storage account that stores the portal diagnostics and has a 30 day retention policy. A pipeline is also available to developers to restore the backup if the deployment has caused an issue that is irreparable. The table and figure below outline the pipelines and their variables. Pipeline Name Variable Permitted Executor Role Example deployment run configuration (TODO -> screen cap of same pipeline (once I can access network) Figure 38: CI Pipeline Variables The figure below illustrates the automation of our deployment process. This aligns with Microsoft’s recommendations with the exception of our specific environment implementation which includes more emphasis of testing for conventions, mis-configurations, an the environment backup snapshot we trigger as the first state of the pipeline for disaster recovery (which is stored and retained in our Azure Storage account as per Microsoft recommendation – this lowers our capacity costs). Figure 39: ALM Illustration Build tools Azure DevOps is the exclusive version control (GIT) and deployment tool for our build. However, the pipeline developed for this uses the Power Platform Build Tools first party plugin in DevOps in conjunction with the Power Platform Command Line Interface (CLI) for more flexibility. When writing a CLI script in a pipeline step, we use PowerShell instead of Bash, as we use a Windows Server (latest) image to host our agent. The CLI support also takes precedence over UNIX based OS support unfortunately (for now) therefore we do have some limitations around path length and some of the GIT capabilities you get with a Linux agent (in particular path lengths). In a later phase of this project, we plan on moving our ALM to a Linux based agent as it’s a well supported operating system for the web and pipelines. However, since we are using the CLI this is not really an issue in this implementation as the OS is abstracted from developer and the pipeline ALM (CICD) is working flawlessly. Its important to note that the DevOps project is linked to a subscription has we are running parallel jobs (up to 2 at time) – instead of incremental stages. The free tier of DevOps gives us 5 free basic licensed users and 1 free hosted agent pool agent job. The CLI is comprised of the following commands, when developers clone or pull the current repository version of our code base for this implementation, they are recommended to use VSCode with the Power Platform Build Tools extensions installed in VSCode to run the commands to grab the latest version of the code, download, unpack, test, and re-back their patches. They also use the CLI to invoke tools such ss the plugin registration tool (for C# class libraries to publish plugins and custom workflow steps), and the configuration migration utility (rarely as they need to simply update the existing schema (XML file) in the repository to include to new tables and for fields that are considered as reference data or dependent data for a process they are building (e.g. a business rule may depend on the existence of a country record to exist thus this record must live in target otherwise wont activate. All this work happens within their own branch, and they are required to merge their work into our main development branch, resolve merge conflicts (if any) then issue a pull-request with appropriate comments and link to work items and approve and auto complete themselves (only for development branch). Once done, they run the {ORGANIZATION}-PATCH pipeline and provide the variable values of what they are deploying, and this will run the entire pipeline tasks including all tests and provide logging. This pipeline only deploys to staging, which is an environment that developers only have access to test their deployments and create artifacts of their release for an actual release to downstream environments up to production. These releases are managed by the release manager and there is a separation of duty whereby the releaser cannot approve his / her own release, this must be completed by someone else on the team. Releases have a retention policy of 30 days (the same applies to release artifacts stored in DevOps). Note, and this must be emphasized, our pipeline should be able to build an environment from scratch and provide a simple tool for developers to avoid making common mistakes and create security vulnerabilities. {ORGANIZATION} must ensure to closely monitor the group of folks who have system administrator access in any other environment than development. Under no circumstances will system administrative privileges will be granted to managed environment (anything other than dev) as deployments can also be done manually (like any other custom framework or COTS) and therefore we’ve created the group ftnc_compliance_dev_administrators who belong to the system administrator team in development via the AAD Group integration with CRM team (explained in this document). The log analytics workspace will log security role assignments and if this scenario takes place, a high incident will be alerted to the appropriate folks to take action. There are rare scenarios whereby system administrator privileges are required to issue quick fixes or diagnose an issue post deploy but in these scenarios, these folks must PIM themselves in the ftnc_compliance-{env}-administrators group (4 hour limit) Below is another more elaborate illustration of this process. Repositoris (GIT) A GIT repository is used to source control our CRM solutions, reference data schema files (e.g., languages, genders, countries, teams, orgs, and individuals. Below illustrates the repo structure and the YAML and PS1 script files to support the pipeline (CI/CD). It’s important note that for security reasons, the .gitignore file exclude environment variables stores in text and JSON files. This means that the pipeline(s) operating. Agent which is hosted in our Azure subscription rather than using the default hosted agent. The agent specifications are provided in this table: Agent Name Operating System Specifications (capacity/threads) Table 39: Custom Agents/Hosted Pools The repository folder structure is the convention we are applying for any Power Platform project although some may not require canvas and portal apps. Branching Strategy / Code Review Process For continuous integration, developers have the freedom to deploy their pipeline artifacts (if it passes our solution and portal checker tasks in the pipeline. If it fails, developers are alerted via email with an anchor tag to view the verbose log of the failure reason and then act (e.g., make the necessary fixes/add missing dependencies etc. and re-deploy). This is the extent to which developers can deploy and test using staging. One developer successfully deploys to staging he / she can appeal to the project manager or release to other downstream environments provided the tech lead reviews and approves the release (mandatory 1 approval). Branches will be assigned to each developer and will be cloned from Development. Even though we are using the same environment / code base, we will be leveraging patches for this mapping and each developer will have his or her own developer portal type development to avoid / minimize conflicts. (SECOND SCENARIO BE AUTHORED) Table 40: Branch mapping strategy (Patching) Service Connections Service Connections in Azure DevOps provide a secure and efficient way to manage connections to external resources, including Dataverse environments. By creating a Service Connection for Dataverse, you can easily reference and manage configurations across different Dataverse environments in your pipelines and other DevOps resources. To use Service Connections for Dataverse, you will need to create a new connection and enter the required connection details, such as the URL of your Dataverse environment and your user credentials. Once you have created the Service Connection, you can use it in your pipelines to deploy and manage configurations across multiple Dataverse environments, such as plugins, workflows, and data synchronization. Using Service Connections in this way helps to ensure that your pipeline executions are secure and efficient, by managing the authentication and authorization details required to access your Dataverse environments in a centralized and reusable way. This approach also makes it easier to manage changes to your Dataverse environments over time, by allowing you to deploy and manage configurations consistently across multiple environments. The table below lists each service connection used by the release pipeline and their type. Service Connection Name Type {ORGANIZATION}-DEV Dataverse {ORGANIZATION}-STAGING Dataverse {ORGANIZATION}-QA Dataverse {ORGANIZATION}-UAT Dataverse {ORGANIZATION}-PREPROD Dataverse {ORGANIZATION}-TRAINING Dataverse {ORGANIZATION}-SANDBOX Dataverse Azure AD Service Account Connection for Power Automate Flows The {ORGANIZATION}-SYSTEM App Registration (App ID: ) is leveraged as a service account to authorize Flows to communicate with its Dataverse environment to perform automation. First-Party Microsoft Plugin to Use CLI Commands in the YAML File for the Pipeline Design To simplify the developer experience when building and optimizing the pipeline, the Power Platform Build Tools library from the DevOps Marketplace has been installed in the 139fc DevOps organization and being use for the Compliance Case Management Project’s pipeline automation scripts. Agent Pools & Self Hosted Agents (future state) In this current implementation, the Azure Pipelines built in Agent is leveraged to host the infrastructure required to run the pipelines. However, {ORGANIZATION} is implementing a self hosted agent which is further described below: The self-hosted agent acts as an intermediary between the Developer and the CICD tool, performing builds and deployments as necessary. Benefits: Greater control and customization over the build and deployment process. Increased flexibility in terms of where and how builds and deployments are performed. Ability to use custom hardware and software configurations to meet specific needs. Potential issues: Requires additional hardware and infrastructure to maintain. May require additional setup and configuration time. Can introduce additional security concerns if not properly secured. In general, self-hosted agents can be a great choice for teams that need greater control and flexibility over their build and deployment processes. However, they require additional resources and maintenance, so teams should carefully weigh the benefits and potential issues before deciding to use them. Table 41: Self Hosted Agents (Detailed) Technical Documentation Process Microsoft DocFx has been implemented to host our build book, solution architecture documents, training guides (for various personas) in our private Storage Account (blob). Developers are responsible for updating our documentation each major release and thus must be part of their pull-request post staging (releases) otherwise the PR will be rejected. The pipeline will automatically update the DocFx static site hosted in Azure Storage. Since this storage account is private only and also has a requirement for users to authorize to Active Directory, {ORGANIZATION}’s RBAC (Access Control Policies) are inherited. The figure below illustrates the example home page of the documentation site. Its important to note that DocFx is the framework used by Microsoft for their documentation and is based on Markdown. It also provides the ability to export all documents to PDF for extended stakeholders. For now, any {ORGANIZATION} employee can access the documentation site only once they are on a {ORGANIZATION} device and authenticated to VPN and the CA. The benefit here is that this enterprise and free open-sourced corporate backed framework for technical documentation makes it extremely simply for staff to search anything and navigate and since its baked into our SDLC process, this documentation, unlike word or other types of static documents has less chance or updated. It also helps with on boarding users by providing them with material out of the gate to ingest. This will further refined by video generated content hosted in this site as well and tutorials (future phase).sssssssssss Figure 40: Document Static Site (Azure Storage) - DocFx Cost Management Billing Policies Capacity Licensing Monitoring Exchange Online This section describes the synchronization of shared mailboxes to send alerts and the Contact-Us mailbox for support linked to a queue in Dynamics. For nonproduction environments, cloud native share mailboxes using the 139gc.onmicrosoft.com domain are being leveraged for development and configuration whereas production will have mailboxes configured in a cloud domain dedicated to the application. This implementation does not require or rely upon “hybrid exchange” as we are not synchronizing any mailboxes that originate from on premises, instead, a new cloud native domain will be configured in Exchange online (which supports multiple domains) to send alerts to recipients and provide a help centre mailbox that is synched to Dynamics and exclusively managed in Dynamics. This means that internal users will be able to review emails from the help centre queue which has an associated help mailbox and promote these to a case or associate to existing case in the system. All email messages synchronized in Dynamics are created as “email message” records” and are of type “activity” thus can be associated manually or automatically to any other activity record such as cases, organizations, and contacts. Once an email is received and associated, it is removed from the queue and responses are automatically tracked in the same thread in the associated record such as a case, thus eliminating the need for users to automatically re-associate a response to the same record. This implementation also does not allow for synchronization of personal employee mailboxes (at this time) and this is another reason why hybrid exchange is not a dependency. Synchronization of Shared Mailboxes to Send Alerts The following mailboxes have been provisioned in M365 Exchange. The table below lists these mailboxes and their associated environments. It’s important to note that an email can only be synched to one environment at a time and that only a G.A or Exchange Administrator can approve a mailbox or uncheck the setting that prevents application layer system administrators to synchronize these mailboxes. Mailbox Environment Environment Map This final section includes a table that lists all environments and their integrated technologies including SharePoint Subsites, Storage container addresses, B2C App Registrations, Portal URL, Security Group and synched mailboxes. D365 URL Portal URL Private / Public Portal Dataverse Environment Security Group SharePoint Site/Subsite URL DevOps GIT Repository Branch B2C API App Registration ID B2C SSO Registration ID Storage Container Address Azure DevOps SPN Synched Shared Mailboxes Table 42: Environment Map Azure Subscriptions The Case Compliance System is primarily implemented in the Power Platform, the SAAS CRM technology that ships with Microsoft 365. However, there is a series of supporting services required to secure the platform, monitor it, and enhance the overall management of the entirety of the system. For this implementation, {ORGANIZATION} has two subscriptions, one dedicated to non-production and one for production. The tables below outline the services, resource groups and tags used for each service implemented in each subscription. Every resource’s implementation detail listed in this table is described in this document and hyperlinks are set on the resource name for ease of navigation. Resource Type Purpose Resource Group Tags IAM Table 43: Azure Subscription Resource List (Non-Production) Resource Type Purpose Resource Group Tags IAM Table 44: Azure Subscription Resource List (Production) Backup, Restore, and RTO/RPO This section describes the backup, restore, RTP and RPO features of the platform and its implementation for this solution. RTO and RPO RTO and RPO are terms used to describe the amount of time a system can be down and the amount of data that can be lost without causing significant harm to the business. For Dynamics 365 online, the RTO (Recovery Time Objective) is the maximum acceptable amount of time that it can take to restore the system after a disruption or outage. Microsoft guarantees an RTO of 4 hours for Dynamics 365 online. This means that Microsoft will work to restore the service within 4 hours of a disruption or outage, and staff can expect the service to be back up and running within that time frame. The RPO (Recovery Point Objective) for Dynamics 365 online is the maximum amount of data that can be lost during an outage or disruption. Microsoft guarantees an RPO of 12 hours for Dynamics 365 online. This means that in the event of an outage or disruption, staff can expect to lose no more than 12 hours of data. It is worth noting that RTO and RPO guarantees are subject to the terms of the Microsoft Service Level Agreement (SLA), which outlines the specific terms and conditions of the agreement between Microsoft and the customer. The SLA also outlines the remedies available to the customer in the event that Microsoft fails to meet these guarantees. Backup and Restore The backup and restore feature in Dataverse allow {ORGANIZATION} to create backups of your Dataverse environments and restore them in case of data loss or corruption. Backups can be taken manually or scheduled to run automatically. The backups can be stored in Azure Storage and can be accessed at any time. Restoring a backup overwrites the current environment with the data and configuration from the backup. Backup and Restore in Sandbox Environment In a Sandbox environment, you can create a backup manually by going to the Power Platform Admin Center and selecting the environment. From there, you can click on the \"Backup & Restore\" tab and select \"New Backup\". You can also schedule backups to run automatically on a daily or weekly basis. Once a backup is created, it will be stored in Azure Storage. To restore a backup in a Sandbox environment, you can go to the \"Backup & Restore\" tab in the Power Platform Admin Center and select \"Restore Backup\". You will then be prompted to select the backup you want to restore. Restoring a backup will overwrite the current environment with the data and configuration from the backup. Backup and Restore in Production Environment In a Production environment, you can create a backup manually by going to the Power Platform Admin Center and selecting the environment. From there, you can click on the \"Backup & Restore\" tab and select \"New Backup\". However, creating a backup manually is not recommended in a Production environment, as it can cause performance issues. It is recommended to schedule backups to run automatically on a weekly or monthly basis. To restore a backup in a Production environment, you can go to the \"Backup & Restore\" tab in the Power Platform Admin Center and select \"Restore Backup\". However, restoring a backup in a Production environment is a more complex process and requires additional steps. It is recommended to seek assistance from Microsoft support via submitting a ticket in the Power Platform Admin Centre. Finally, when restoring an environment, the environment is automatically set to “administrative mode” meaning that only system administrators can access the system to review that the restore was successfully completed. If successful, administrative mode can be turned off in the admin centre and other users can now access the environment. Implementing Backup and Restore in Release Pipeline using CLI You can use the Common Data Service (CDS) CLI to create and manage backups and restores as part of a release pipeline. The CDS CLI provides a set of commands that allow you to automate the process of creating backups and restoring them. Creating the backup using the CLI, uses the following command (step in pipeline for disaster recovery): cds backup create --environmentName <environmentName> --containerName <containerName> --description <description> --zip This command creates a backup of the specified environment and stores it in the specified container in Azure Storage. To restore a backup using the CLI, you can use the following command: cds backup restore --environmentName <environmentName> --containerName <containerName> --backupName <backupName> This command restores the specified backup to the specified environment. These commands have been implemented into our release pipeline as part of your deployment process. This help ensures that our environments are always backed up and can be restored quickly in case of data loss or corruption. Business Requirement Implementation Summary & Testing TO DO -> inc. user journey diagram, HL features, state machine Business requirements are tracked in Azure DevOps in the same project that hosts this implementation’s GIT repository, release pipelines and artifacts. This is beneficial for tagging work items such as tasks and user stories to specific releases when running the pipeline thus allowing QA and UAT (testers) to scope what to test for each release. Furthermore, a subset of users is assigned the basic+test licenses to make use of the testing tools of DevOps. We make use of both manual and automated testing, both described below. Azure Test Plans: This is a testing solution for manual and exploratory testing. It allows users to create test plans, test suites, and test cases, and to execute tests against different configurations of their application. Azure Test Plans also integrates with Azure Pipelines, allowing users to run automated tests as part of their build and release pipelines. Azure Test Automation: Azure Test Automation also integrates with Azure Pipelines, allowing users to run automated tests as part of their build and release pipelines. For this implementation, integration testing is being automated by running a series of PowerShell scripts that validate the YAML configurations of tested portal artifacts such as Table Permissions, Column Permissions, Site Settings, Web Templates, Wizard Form Configurations and Global settings. This means that once a feature has been tested “manually” and validated by testers, the developer will store the YAML version of each of the portal tested artifacts in the Validated Portal Test folder in the GIT repository and will compare these against every subsequent release of these same artifacts. If a change has been made to a tested file, a log is generated in the pipeline artifact for the staging pipeline only, therefore will allow the release. However, it is the developer’s responsibility to either update the new version of the YAML file based on new requirements or repair the misconfiguration and re-issue a new release to staging that is clean. For any other environment, the pipeline will block the release if the YAML files do not match the tested artifacts. The diagram below depicts this process. For CRM (backend testing), there are several tools being used and described below: Power Apps Test Studio is an automated testing tool for Model-driven apps that developers use to create and run UI test cases and helps them verify that your app's features and functionalities are working as expected. The tool offers a simple, intuitive user interface that makes it easy to create and manage test cases, and it integrates seamlessly with the Power Apps platform. Some of the key benefits of using Power Apps Test Studio for automated testing of Model-driven apps include. It’s important to note that building a model-driven-app, unlike the portal requires very little coding thus the platform provides UX testing tools to test the user, and these are described below and used by developers: Efficient test case creation and execution: With Power Apps Test Studio, you can quickly create test cases using a visual recorder, or by manually creating test steps. You can also run your test cases in parallel, which helps to save time and improve efficiency. Improved test coverage: The tool allows you to test a wide range of scenarios and use cases, which helps to ensure that your app is thoroughly tested and that all features and functionalities are working as expected. Integration with Power Apps platform: Power Apps Test Studio integrates seamlessly with the Power Apps platform, which makes it easy to create and manage test cases and to collaborate with other team members. Easy to maintain and update: The tool makes it easy to maintain and update your test cases, even as your app evolves, and new features are added. Overall, Power Apps Test Studio is a powerful and flexible automated testing tool that is specifically designed for Model-driven apps in Dynamics 365. It can help you to improve the quality and reliability of your app and ensure that it meets the needs and expectations of your users. Moq is the automated testing tool used for C# class libraries that are needed for plugins and custom workflow steps for more complex functionality such as the bilingual plugin utility. Moq is a popular mocking framework for C# that can be used to create mock objects and simulate dependencies in your code. We also use XUNIT which integrates with Moq. Moq is important as it will create a “Moq” organization context thus allowing to create CRUD operation in our assertion functions without the need to write to a Dataverse environment. When developers check in their code and solutions both testing tools are run as tasks in the pipeline and must succeed in the build for the release to be issued. When it comes to tying in with a release pipeline, all these testing tools can be integrated with Azure Pipelines, which is the continuous integration and continuous delivery (CI/CD) service offered by Azure DevOps. This means that users can include their tests as part of their build and release pipelines, ensuring that their applications are thoroughly tested before they are deployed to production. For example, users can create a build pipeline that builds and packages their application and includes automated tests using Azure Test Automation. They can then create a release pipeline that deploys the application to a test environment and includes manual and exploratory tests using Azure Test Plans. Finally, they can create a separate release pipeline that deploys the application to production, after all tests have passed. Overall, the testing tools available in Azure DevOps provide Basic+test licensed users with a comprehensive suite of solutions for testing their applications, from manual and exploratory testing to automated and load testing. These tools can be easily integrated with Azure Pipelines, allowing users to include their tests as part of their build and release pipelines, and to ensure that their applications are thoroughly tested before they are deployed to production. The same applies to the automated test suite using Moq and the Power Apps Studio Testing Tool. Case Management & State Behavior This CRM implementation is heavily geared towards administering and automating the process surrounding the compliance of {ORGANIZATION} policy imposed on the Canadian financial industry. Thus, the case management feature is a key pillar in addressing a large portion of the requirements surrounding the processes that govern the compliance policies/processes. A case is already always provisioned for each form that gets published to the website and cases can also be published for internal purposes as well. For the first phase, external facing cases are at the forefront of this application, and the first compliance form being implemented is the “risk questionnaire”. The process is visualized in the diagram below to help the reader understand the overall “happy path” of the process and this applies (or will apply) to most if not all external facing forms (that are triggered and managed by cases) in the system. Figure 41: Initial engagement (Triggering the risk questionnaire process) {ORGANIZATION} also offers support (interactively) through this application by means of auto associating email correspondence to existing contacts and or related cases in the system. In a later phase, the omnichannel module will be activated which allows RE’s to communicate directly with staff using chat like functionality on the web site. For now however, interactive support equates to email and a live comment feed in the “notification” centre whereby RE’s can comment a notification received and staff has the ability to answer these comments in real time. The figure below shows the email integration and notification centre implementation for support cases. Figure 42: Interactive support (email) The notification center feature is described earlier in this document and can be found here."
  },
  "_site/README.html": {
    "href": "_site/README.html",
    "title": "Introduction | GOC Theme Documentation",
    "keywords": "Introduction Our goal is to provide our customers with the most up to date and relevant documentation surrounding the various products and services we support. Despite our strong focus to using PAAS and SAAS services, which are already well documented online, it is often a challenge to find the most relevant information based on the specific implementation details of what our company has deployed to your environment(s). The Compliance Case Management System is being implemeted in Microsoft's 365 suite of products, in partular the Power Platform. The image below depicts where Power Platform Lives within the Myriad of Azure and M365 services at a high level. Architecture To accomplish a seamless and automatable source controlled documentation static generator, Implementation leverages Microsoft's Open Source Documentation system called DocFx. The architecture for Microsoft's DocFX Documentation Platform can be found here: docfx Design Spec | DocFX website (dotnet.github.io) Building To get started quickly, follow the guide in the DocFx documentation. Getting Started with DocFX | DocFX website (dotnet.github.io) To convert a word doc use ALL WORD DOCS TO CONVERT MUST ALSO BE COMMITTED TO THE REPOSITORY UNDER THE WORDDOCS FOLDER IN THE documentation FOLDER: pandoc documentation/Solution\\ Design\\ \\&\\ Architecture/docfxSDD-v2.6.docx --from=docx --to=markdown_strict+yaml_metadata_block --output=documentation/Solution\\ Design\\ \\&\\ Architecture/SDD-v2.6.md --wrap=none --metadata title=\"Solution Design\" Next, you must also extract all media references with the following command (example, but make sure htat ) pandoc --extract-media ../SDD/images ocumentation/Solution\\ Design\\ \\&\\ Architecture/docfxSDD-v2.6.docx -o --output=documentation/Solution\\ Design\\ \\&\\ Architecture/SDD-v2.6.md --wrap=none --metadata title=\"Solution Design\" Once you are familiar with the solution, you are ready to clone Implementation's docfx instance. Simply Git Clone into a local folder, and ensure that you have the required dependencies for DocFx on your machine/server. Once the solution is cloned locally, execute the following command: docfx docfx.json --serve Building (PDF) To get started quickly, follow the guide in the DocFx documentation. Create/Downloadable PDF Files | DocFX website (dotnet.github.io) To build PDF files, first install wkhtmltopdf by downloading the latest binary from the official site or install using chocolatey: choco install wkhtmltopdf. {TBD: MUST FIND UNIX VERSION OR MACOS EQUIV AS I DONT WANT THE PDF CONVERSION STUCK USING WINDOWS} Make sure the wkhtmltopdf command is added to PATH environment variable and is available in the terminal. PDF Config Add a pdf section in docfx.json: { \"pdf\": { \"content\": [{ \"files\": [ \"**/*.{md,yml}\" ] }], \"wkhtmltopdf\": { \"additionalArguments\": \"--enable-local-file-access\" }, } } Most of the config options are the same as build config. The wkhtmltopdf config contains additional details to control wkhtmltopdf behavior: filePath: Path to wkhtmltopdf.exe. additionalArguments: Additional command line arguments passed to wkhtmltopdf. Usually needs --enable-local-file-access to allow access to local files. Running docfx command againt the above configuration produces a PDF file for every TOC included in the content property. The PDF files are placed under the _site_pdf folder based on the TOC name. See this sample on an example PDF config. Add Cover Page A cover page is the first PDF page before the TOC page. To add a cover page, add a cover.md file alongside toc.yml. The content of cover.md will be rendered as the PDF cover page. NOTE on PDF Dont use DocFX's PDF generator as all it does is create one huge PDF which is obviously not what we want. So please make sure to review the scripts in the .pipelines folder (PS1). These scripts will loop through the documentation folder content sub folder's .md files and run the following pandoc command: pdfgen.ps1 -> this creates the pdf's but will also create the PDFs in the _site/documentation subfolders with the HTML files so that the web server can reference the pdf files in the anchor tag as ./ #Note that this script is ran in the loop so make sure you example the actual PS function. Also you need to have this executable installed in your local environment or in your DevOps agent - so this needs to be refactored at somepoint so that we can reference this exe - perhaps in our GIT repo pandoc .\\ALM.md -o output.pdf --pdf-engine=\"C:\\Users\\Fred\\AppData\\Local\\Programs\\MiKTeX\\miktex\\bin\\x64\\pdflatex.exe\" addpdflink.ps1 -> creates the anchor in the md file. Make sure that once you run this, you run the docfx docfx.json command again to regenerate the content and re-deploy the site. # Read the content of the Markdown file $content = Get-Content -Path $file.FullName # Find the first heading in the Markdown file $heading = $content | Select-String -Pattern \"^#\\s+(.*)\" | ForEach-Object { $_.Matches.Groups[1].Value } if ($heading -ne $null) { # Generate the link to the PDF file $pdfLink = \"[Download PDF](./$($file.BaseName).pdf)\" # Add the link below the first heading $newContent = $content -replace \"^#\\s+$heading\", \"# $heading`r`n$pdfLink\" # Write the updated content back to the file $newContent | Set-Content -Path $file.FullName } This is the original configuration in the DocFX.json file which I've removed but kept here for reference."
  }
}